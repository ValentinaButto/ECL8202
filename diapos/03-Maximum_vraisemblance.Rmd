---
title: "Maximum de vraisemblance"
date: "ECL8202 - Hiver 2020"
output: 
  xaringan::moon_reader:
    css: ["default", "metropolis", "metropolis-fonts", "styles-xar8202.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE,
                      fig.dim = c(8, 5))
library(tidyverse)
library(cowplot)
theme_set(
  theme_cowplot(font_size = 16) +
    theme(panel.background = element_rect(fill = "#fafafa"),
          plot.background = element_rect(fill = "#fafafa"))
)
```


# Introduction

- Maximum de vraisemblance (*maximum likelihood*): méthode générale pour estimer les paramètres d'un modèle statistique.

--

- Ex.: Modèle de régression où la réponse $y$ est séparée en une portion expliquée par différents prédicteurs et une portion non-expliquée, représentée par une distribution statistique.

--

- Le modèle contient différents paramètres inconnus, à être estimés à partir des observations.

--

- Selon le maximum de vraisemblance, les meilleurs estimés des paramètres sont ceux qui maximisent la probabilité des valeurs observées de la variable.

---

# Introduction

Plusieurs méthodes vues dans le cours préalable (ECL7102) sont basées sur un calcul de la vraisemblance:

- Sélection de modèles avec l'AIC.

- Estimation des paramètres d'un modèle linéaire généralisé.

- Estimation des paramètres d'un modèle linéaire mixte (maximum de vraisemblance restreint).

---

# Contenu du cours

- Principe du maximum de vraisemblance

- Application du maximum de vraisemblance dans R

- Test du rapport de vraisemblance

- Calcul des intervalles de confiance d'un paramètre

- Estimation de plusieurs paramètres: profil de vraisemblance et approximation quadratique

---

class: inverse, center, middle

# Principe du maximum de vraisemblance

---

# Exemple

- Estimer le taux de germination d'un lot de semences en faisant germer $n = 20$ semences dans des conditions similaires.

- Réponse $y$: nombre de semences ayant germé avec succès.

--

- $y$ suit une distribution binomiale, où $p$ est la probabilité de germination dans la population:

$$f(y \vert p) = {n \choose y} p^y (1-p)^{n-y}$$

--

- $f(y \vert p)$: Distribution de $y$, conditionnelle à une valeur donnée de $p$.

---

# Distribution conditionnelle de $y$

Exemple: Distribution de $y$ si $p = 0.2$.

```{r}
ggplot(NULL, aes(x = 0:20)) +
    labs(x = "y", y = "f(y|p=0.2)") +
    stat_function(fun = dbinom, n = 21, args = list(size = 20, prob = 0.2),
                  geom = "bar", color = "black", fill = "white") +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0))
```

---

# Distribution conditionnelle de $y$

Exemple: Distribution de $y$ si $p = 0.2$.

```{r}
ggplot(NULL, aes(x = 0:20)) +
    labs(x = "y", y = "f(y|p=0.2)") +
    stat_function(fun = dbinom, n = 21, args = list(size = 20, prob = 0.2),
                  geom = "bar", color = "black", fill = "white") +
    geom_segment(aes(x = 0, xend = 6, y = dbinom(6, 20, 0.2),
                     yend = dbinom(6, 20, 0.2)), 
                 color = "#b3452c", linetype = "dashed", size = 1) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0))
```

---

# Fonction de vraisemblance

- Vue comme une fonction de $p$ plutôt que de $y$, la probabilité d'avoir observé les données $y$ pour une valeur donnée du paramètre $p$ est la vraisemblance $L$ de $p$.

--

- Ex.: $L(p)$ pour $y = 6$ et $n = 20$.

```{r}
ggplot(NULL, aes(x = 0:1)) +
    labs(x = "p", y = "L(p)") +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = "density") +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))
```

---

# Fonction de vraisemblance

- Vu comme une fonction de $p$ plutôt que de $y$, la probabilité d'avoir observé les données $y$ pour un valeur donnée du paramètre $p$ est la vraisemblance $L$ de $p$.

- Ex.: $L(p)$ pour $y = 6$ et $n = 20$.

```{r}
ggplot(NULL, aes(x = 0:1)) +
    labs(x = "p", y = "L(p)") +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = "density") +
    geom_segment(aes(x = 0, xend = 0.2, y = dbinom(6, 20, 0.2),
                     yend = dbinom(6, 20, 0.2)), 
                 color = "#b3452c", linetype = "dashed", size = 1) +
    geom_segment(aes(x = 0.2, xend = 0.2, y = 0, yend = dbinom(6, 20, 0.2)), 
                 color = "#b3452c", linetype = "dashed", size = 1) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))
```

---

# Fonction de vraisemblance

- $y = (y_1, y_2, ..., y_n)$ est le vecteur d'observations et $\theta = (\theta_1, ..., \theta_m)$ le vecteur des paramètres du modèle statistique pour $y$.

--

- La vraisemblance pour un vecteur $\theta$ donné est la probabilité conjointe des observations $y$, conditionnellement à ces valeurs de $\theta$.

$$L(\theta) = p(y | \theta)$$

--

- Attention: La fonction de vraisemblance *n'est pas* une distribution de probabilité pour $\theta$.

---

# Maximum de vraisemblance

Selon le principe du maximum de vraisemblance, le meilleur estimé des paramètres du modèle selon nos observations $y$ est le vecteur de valeurs $\theta$ qui maximise la valeur de $L(\theta)$.

---

# Exemple: Distribution binomiale

.pull-left[

- L'estimé du maximum de vraisemblance pour $p$ correspond à:

$$\hat{p} = \frac{y}{n}$$

- Dans notre exemple, avec $y = 6$ et $n = 20$, $\hat{p} = 0.3$.

]

.pull-right[

```{r}
ggplot(NULL, aes(x = 0:1)) +
    labs(x = "p", y = "L(p)") +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = "density") +
    geom_segment(aes(x = 0, xend = 0.3, y = dbinom(6, 20, 0.3),
                     yend = dbinom(6, 20, 0.3)), 
                 color = "#b3452c", linetype = "dashed", size = 1) +
    geom_segment(aes(x = 0.3, xend = 0.3, y = 0, yend = dbinom(6, 20, 0.3)), 
                 color = "#b3452c", linetype = "dashed", size = 1) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))
```

]

---

# Exemple: Modèle linéaire

$$y \sim N(\beta_0 + \beta_1 x, \sigma)$$

--

- Probabilité d'une observation de $y$:


$$f(y \vert \beta_0, \beta_1, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{y - \beta_0 - \beta_1 x}{\sigma} \right)^2}$$

--

- Pour $n$ observations indépendantes:

$$f(y_1, ..., y_n \vert \beta_0, \beta_1, \sigma) = \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{y_i - \beta_0 - \beta_1 x_i}{\sigma} \right)^2}$$

---

# Log-vraisemblance

$$L(\beta_0, \beta_1, \sigma) = \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{y_i - \beta_0 - \beta_1 x_i}{\sigma} \right)^2}$$

--

- En pratique, plus facile de travailler avec $l = \log L$.

- Les paramètres maximisant $l$ maximisent aussi $L$.

---

# Log-vraisemblance

$$L(\beta_0, \beta_1, \sigma) = \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{y_i - \beta_0 - \beta_1 x_i}{\sigma} \right)^2}$$

$$l(\beta_0, \beta_1, \sigma) = \sum_{i=1}^n \left( \log \left( \frac{1}{\sigma \sqrt{2 \pi}} \right) - \frac{1}{2} \left( \frac{y_i - \beta_0 - \beta_1 x_i}{\sigma} \right)^2 \right)$$

--

$$l(\beta_0, \beta_1, \sigma) = n \log \left( \frac{1}{\sigma \sqrt{2 \pi}} \right) - \frac{1}{2 \sigma^2}  \sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i \right)^2$$

---

# Modèle linéaire

$$l(\beta_0, \beta_1, \sigma) = n \log \left( \frac{1}{\sigma \sqrt{2 \pi}} \right) - \frac{1}{2 \sigma^2}  \sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i \right)^2$$

- Pour un modèle linéaire, les estimés des $\beta$ selon la méthode des moindre carrés concordent avec ceux du maximum de vraisemblance.

---

# Estimé de la variance

$$l(\beta_0, \beta_1, \sigma) = n \log \left( \frac{1}{\sigma \sqrt{2 \pi}} \right) - \frac{1}{2 \sigma^2}  \sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i \right)^2$$

- La valeur de $\sigma^2$ maximisant $l$ est égale à:

$$\hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i \right)^2$$

--

- On sait que cet estimé est biaisé.

--

- En général, les estimés du maximum de vraisemblance ont un biais négligeable si $n$ est assez grand.

---

class: inverse, center, middle

# Application du maximum de vraisemblance dans R

---

# Trouver le maximum de vraisemblance

- Excepté pour quelques modèles simples, il est difficile de déterminer le maximum d'une fonction de vraisemblance analytiquement.

--

- Algorithmes numériques permettent de trouver le maximum d'une fonction par itération, jusqu'à l'obtention d'une précision voulue.

--

- Dans R, `optim` est une fonction générale pour obtenir un minimum ou maximum.

- Le package *bbmle* offre des fonctions spécialisées au problème du calcul du maximum de vraisemblance.

---

# Exemple

- Richesse spécifique des plantes de 30 îles de l'archipel des Galapagos.

```{r}
galap <- read.csv("../donnees/galapagos.csv")
head(galap)
```

--

- Modélisons le nombre d'espèces (*Species*) en fonction de la superficie de l'île (*Area*) et de la distance vers l'île la plus proche (*Nearest*).

---

# Distribution binomiale négative

- Distribution appropriée pour les données de comptage dont la variance est supérieure à celle prévue par la distribution de Poisson.

--

.pull-left[

**Poisson**

$$y \sim \textrm{Pois}(\lambda)$$

- La moyenne et la variance de $y$ sont toutes deux égales à $\lambda$.

]

--

.pull-right[

**Binomiale négative**

$$y \sim \textrm{NB}(\mu, \theta)$$

- $y$ a une moyenne $\mu$ et une variance $\mu + \frac{\mu^2}{\theta}$

- Pour $\theta$ élevé, on s'approche du modèle de Poisson.

]

---

# Modèle binomial négatif

$$y \sim \textrm{NB}(\mu, \theta)$$

--

- Comme pour la régression de Poisson, c'est le logarithme de la moyenne de $y$ qui dépend linéairement des prédicteurs.

$$\log\mu = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...$$

--

$$\mu = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...}$$

--

- Pour notre modèle du nombre d'espèces, nous appliquerons aussi une transformation logarithmique aux prédicteurs (superficie et distance à l'île la plus près).

---

# Fonction de vraisemblance

- Nous créons une fonction dans R avec comme arguments les quatre paramètres à ajuster: $\beta_0$, $\beta_{Area}$, $\beta_{Near}$ et $\theta$.

```{r, echo = TRUE}
nll_galap <- function(b_0, b_area, b_near, theta) {
    mu_sp <- exp(b_0 + b_area * log(galap$Area) + b_near * log(galap$Nearest))
    -sum(dnbinom(galap$Species, mu = mu_sp, size = theta, log = TRUE))
}
```

- Puisque les outils d'optimisation dans R recherchent un minimum plutôt qu'un maximum, la fonction calculera $-l = - \log L$.

---

# Fonction de vraisemblance

```{r, echo = TRUE}
nll_galap <- function(b_0, b_area, b_near, theta) {
    mu_sp <- exp(b_0 + b_area * log(galap$Area) + b_near * log(galap$Nearest)) #<<
    -sum(dnbinom(galap$Species, mu = mu_sp, size = theta, log = TRUE))
}
```

- `mu_sp`est la moyenne prédite du nombre d'espèces (vecteur de 30 valeurs)

---

# Fonction de vraisemblance

```{r, echo = TRUE}
nll_galap <- function(b_0, b_area, b_near, theta) {
    mu_sp <- exp(b_0 + b_area * log(galap$Area) + b_near * log(galap$Nearest)) 
    -sum(dnbinom(galap$Species, mu = mu_sp, size = theta, log = TRUE)) #<<
}
```


- `mu_sp`est la moyenne prédite du nombre d'espèces (vecteur de 30 valeurs)

- `dnbinom` donne un vecteur avec le logarithme de la probabilité (`log = TRUE`) pour chaque observation.

---

# Trouver le maximum

- La fonction `mle2` du package *bbmle* détermine le maximum de vraisemblance pour un modèle donné.

- La fonction requiert au minimum deux arguments: la fonction $-l$ à minimiser, puis une liste de valeurs initiales pour chaque paramètre.

```{r, warning = FALSE, echo  = TRUE}
library(bbmle)

mle_galap <- mle2(nll_galap, 
        start = list(b_0 = 0, b_area = 0, b_near = 0, theta = 1))
```

---

# Trouver le maximum

```{r, warning = FALSE, echo = TRUE}
mle_galap
```

---

# Valeur de la vraisemblance

- La log-vraisemblance obtenue dans cet exemple (-137.98) correspond à une valeur infime de $L$.

```{r, echo = TRUE}
exp(-137.98)
```

--

- $L$ est la probabilité d'avoir observé *exactement* ces données. Elle diminue avec plus de données, car il y a plus de combinaisons de valeurs possible.

--

- On n'interprète pas la valeur absolue de $L$ (ou $l$), mais plutôt sa valeur relative pour différentes valeurs des paramètres en fonction du même jeu de données.

---

# Modèle binomial négatif

- Pour notre exemple, le package *MASS* contient une fonction `glm.nb` qui permet d'estimer directement les paramètres d'une régression binomiale négative.

--

```{r, message = FALSE, warning = FALSE, echo = TRUE}
library(MASS)
glm.nb(Species ~ log(Area) + log(Nearest), galap)
```

---

# Quelle méthode utiliser?

- Il est préférable d'utiliser une fonction spécialisée si elle existe.

--

- Certains modèles non-linéaires peuvent être transformés en modèle linéaire.

- Exemple: loi de puissance entre le nombre d'espèces $S$ et la superficie $A$.

$$S = cA^z$$

--

$$\log(S) = \log(c) + z \log(A)$$

--

- Dans d'autres cas, le modèle présumé ne peut pas être ramené à une format standard, donc il faut écrire sa propre fonction de vraisemblance.

---

# Exemple: Courbe de dispersion

- Estimer la dispersion d'une espèce à partir de graines recueillies dans des pièges.

--

- $y_i$: Nombre de graines dans un piège $i$

$$y_i \sim \textrm{NB}(\mu_i, \theta)$$

--

- Courbe de dispersion $f(r_{ij})$: probabilité qu'une graine d'un arbre $j$ tombe dans un piège $i$ situé à distance $r_{ij}$.

--

$$y_i \sim \textrm{NB}(\sum_j b\times f(r_{ij}), \theta)$$

---

# Exemple: Indice de compétition

- Indice de compétition (CI): compétition exercée par chaque voisin $j$ d'un arbre $i$ augmente avec le diamètre du voisin $D$ et diminue avec la distance $r$ entre les deux.

$$CI_i = \sum_j \frac{D_j^{\delta}}{r_{ij}^{\gamma}}$$

--

- On souhaite estimer $\gamma$ et $\delta$.

--

- Intégration du *CI* dans un modèle prédisant la croissance $y$:

$$y_i = \beta_0 + ... + \beta_{CI} \sum_j \frac{D_j^{\delta}}{r_{ij}^{\gamma}}$$

---

# Limites du maximum de vraisemblance

- Les propriétés avantageuses du maximum de vraisemblance, notamment l'absence de biais, sont valides seulement dans la limite où $n$ (nombre d'observations) est grand.

--

- Le maximum de vraisemblance s'applique à tous les problèmes d'estimation de paramètres de modèles statistiques, mais il peut exister des meilleures méthodes pour un problème donné, surtout si $n$ est faible.

--

- Pour des fonctions de vraisemblance complexes, il peut y avoir plusieurs maximums locaux. Dans ce cas, il n'est pas garanti que les algorithmes trouvent le maximum global.

---

class: inverse, center, middle

# Test du rapport de vraisemblance

---

# Exemple

Vraisemblance de la probabilité $p$ de germination des semences si 6 semences ont germé sur 20.

.pull-left[

```{r}
ggplot(NULL, aes(x = 0:1)) +
    labs(x = "p", y = "L(p)") +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = "density") +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))
```

]

--

.pull-right[

- Ici, $\hat{p} = 0.3$ selon le maximum de vraisemblance.

- Si le fournisseur affirme que $p = 0.5$, est-ce que cette valeur est compatible avec nos résultats?

]

---

# Exemple

- Vraisemblance selon l'hypothèse nulle $p_0 = 0.5$, comparée à celle de l'estimé $\hat{p}$.

```{r, echo = TRUE}
l_0 <- dbinom(6, 20, prob = 0.5)
l_0
```

```{r, echo = TRUE}
l_max <- dbinom(6, 20, prob = 0.3)
l_max
```

---

# Rapport de vraisemblance

- Le test du rapport de vraisemblance (*likelihood-ratio test*) utilise la statistique:

$$- 2 \log \left( \frac{L(\theta_0)}{L(\hat{\theta})} \right)$$

--

- Exprimée en fonction de la log-vraisemblance:

$$- 2 \left( l(\theta_0) - l(\hat{\theta}) \right)$$

--

- Si l'hypothèse nulle est vraie et l'échantillon est grand, cette statistique suit approximativement une distribution $\chi^2$ avec 1 degré de liberté.

---

# Exemple

- Rapport de vraisemblance

```{r, echo = TRUE}
rv <- -2*log(l_0 / l_max)
rv
```

--

- Valeur $p$ sous l'approximation du $\chi^2$

```{r, echo = TRUE}
1 - pchisq(rv, df = 1)
```

--

- *Note*: Le test du rapport de vraisemblance ne s'applique pas si l'hypothèse nulle est à la limite des valeurs possibles du paramètre (ex.: $p = 0$ ou $p = 1$ pour une distribution binomiale).

---

# Comparaison entre modèles

- Comparaison de deux modèles nichés:

M1: $y = \beta_0 + \beta_1 x_1 + \epsilon$

M2: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon$ 

--

- M1 est équivalent à M2 avec $\beta_2 = \beta_3 = 0$.

--

- Si M1 est le vrai modèle, la statistique du rapport de vraisemblance comparant les modèles ajustés M1 et M2 suit une distribution $\chi^2$ avec 2 degrés de liberté (nombre de paramètres additionnels de M2).

$$- 2 \left( l_{M1} - l_{M2} \right) \sim \chi^2(2)$$

---

# Comparaison avec l'AIC

Critère d'information d'Akaike (AIC):

$$AIC = - 2 \log L + 2K =  -2l + 2K$$

- $K$: nombre de paramètres du modèle.

--

- Contrairement au test du rapport de vraisemblance, l'AIC permet de comparer plus de deux modèles à la fois, nichés ou non.

--

*Question différente*

- AIC: Quel modèle prédirait le mieux la réponse pour un nouvel échantillon?

- Test du rapport de vraisemblance: Est-ce probable que le modèle le plus simple soit correct, considérant l'écart entre sa vraisemblance et celle du modèle complexe?

---

class: inverse, center, middle

# Calcul des intervalles de confiance

---

# Test d'hypothèse et intervalle

- Si on ne peut pas rejeter l'hypothèse nulle $\theta = \theta_0$ avec un seuil de signification $\alpha$, alors $\theta_0$ fait partie de l'intervalle de confiance à $100(1-\alpha)\%$ pour $\theta$.

--

- Pour un test du rapport de vraisemblance, la valeur maximale de la statistique qui n'est pas rejetée avec $\alpha = 0.05$ correspond au 95e centile de la distribution du $\chi^2$.

$$- 2 \left( l(\theta_0) - l(\hat{\theta}) \right) = \chi^2_{0.95}(1)$$

--

- Donc les limites de l'intervalle à 95% sont les valeurs de $\theta_0$ où:

$$l(\theta_0) = l(\hat{\theta}) - \frac{\chi^2_{0.95}(1)}{2}$$

---

# Exemple

Germination des semences avec un modèle binomial: $y = 6$ et $n = 20$, donc $\hat{p} = 0.3$.

--

- Limite de $L$ pour l'intervalle à 95%:

```{r, echo = TRUE}
exp(dbinom(6, 20, 0.3, log = TRUE) - qchisq(0.95, df = 1)/2)
```

---

# Exemple

Germination des semences avec un modèle binomial: $y = 6$ et $n = 20$, donc $\hat{p} = 0.3$.

```{r}
ggplot(NULL, aes(x = 0:1)) +
    labs(x = "p", y = "L(p)") +
    stat_function(geom = "area", fill = "#d3492a", n = 1000,
        fun = function(x) ifelse(x > 0.132 & x < 0.516,
                                 dbinom(6, 20, prob = x), NA)) +
    stat_function(fun = function(x) dbinom(6, 20, prob = x), 
                  geom = "density") +
    geom_hline(yintercept = 0.0279, linetype = "dashed") +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))
```

---

# Exemple

Échantillon plus grand avec le même taux observé: $y = 15$, $n = 50$.

```{r}
ggplot(NULL, aes(x = 0:1)) +
    labs(x = "p", y = "L(p)") +
    stat_function(geom = "area", fill = "#d3492a", n = 1000,
        fun = function(x) ifelse(x > 0.185 & x < 0.435,
                                 dbinom(15, 50, prob = x), NA)) +
    stat_function(fun = function(x) dbinom(15, 50, prob = x), 
                  geom = "density") +
    geom_hline(yintercept = 0.0179, linetype = "dashed") +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(breaks = seq(0, 0.12, 0.03), 
                       limits = c(0, 0.13), expand = c(0, 0))
```

---

# Modèle avec plusieurs paramètres

- Test du rapport de vraisemblance pour un paramètre $\theta$:

$$- 2 \left( l(\theta_{0}) - l(\hat{\theta}) \right)$$

--

- Quelle valeur choisir pour les autres paramètres si on fixe $\theta = \theta_0$?

--

- On ne peut pas fixer les autres paramètres à leur valeur au maximum de vraisemblance si les estimés sont corrélés.

---

# Corrélation entre les estimés

Le meilleur estimé de la pente change si on fixe l'ordonnée à l'origine à 0.

```{r, echo = FALSE}
set.seed(82)
rand_df <- data.frame(x = seq(0, 5, 0.2), y = 5 + 2*seq(0, 5, 0.2) + rnorm(26, 0, 5))
ggplot(rand_df, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    geom_smooth(method = "lm", formula = y ~ x - 1, se = FALSE, linetype = "dashed")
```

---

# Vraisemblance profilée

- Pour construire la courbe de $l(\theta_0)$, on fixe successivement le paramètre à différentes valeurs, puis on ré-estime le maximum de vraisemblance pour les autres paramètres.

- La fonction résultante est la vraisemblance profilée (*profile likelihood*) de $\theta$.

- Dans le package *bbmle*, elle est obtenue avec la fonction `profile` appliquée au résultat de `mle2`.

---

# Vraisemblance profilée

Le graphique montre la courbe $\sqrt{- 2 \left( l(\theta_{0}) - l(\hat{\theta}) \right)}$.

```{r, warning = FALSE, echo = TRUE}
galap_pro <- profile(mle_galap)
plot(galap_pro)
```

---

# Intervalle de confiance profilés

```{r, echo = TRUE}
confint(galap_pro, level = 0.95)
```

---

# Approximation quadratique

- Vraisemblance profilée prend beaucoup de temps pour un modèle complexe.

--

- Méthode plus approximative, mais beaucoup plus rapide: supposer que la log-vraisemblance suit une forme quadratique.

- Pour un paramètre, la forme quadratique est une parabole centrée sur le maximum de $l$.

--

- L'approximation est meilleure lorsque la taille de l'échantillon augmente.

---

# Approximation quadratique

Modèle binomial avec $y = 6$, $n = 20$

```{r}
ggplot(NULL, aes(x = 0:1)) +
  labs(x = "p", y = "l(p)") +
  stat_function(fun = function(x) dbinom(6, 20, prob = x, log = TRUE), geom = "line") +
  stat_function(fun = function(x) -1.652-(x-0.3)^2/0.021, geom = "line", linetype = "dashed") +
  coord_cartesian(ylim = c(-4.5, -1.5))
```

---

# Approximation quadratique

Modèle binomial avec $y = 15$, $n = 50$

```{r}
ggplot(NULL, aes(x = 0:1)) +
  labs(x = "p", y = "l(p)") +
  stat_function(fun = function(x) dbinom(15, 50, prob = x, log = TRUE), geom = "line") +
  stat_function(fun = function(x) -2.1-(x-0.3)^2/0.0084, geom = "line", linetype = "dashed") +
  coord_cartesian(ylim = c(-5, -2))
```

---

# Approximation quadratique

Pour un paramètre:

$$- 2 \left( l(\theta_{0}) - l(\hat{\theta}) \right) = a (\theta_{0} - \hat{\theta})^2$$

--

- $a$ mesure la courbure de la parabole. Plus la courbure (ou dérivée seconde) est prononcée, plus l'estimé de $\theta$ est précis.

--

- Si l'approximation quadratique est bonne:

$$\frac{\textrm{d}^2(-l)}{\textrm{d}\theta^2} = \frac{1}{\sigma_{\hat{\theta}}^2}$$

- $\sigma_{\hat{\theta}}$ est l'erreur-type de $\hat{\theta}$.


---

# Approximation quadratique

- Pour $m$ paramètres, la courbure en $m$ dimensions est représentée par une matrice $m \times m$ des dérivées partielles secondes de $-l$ (matrice d'information de Fisher).

- En inversant cette matrice, on obtient les variances et covariances des estimés.

--

- Dans le package *bbmle*, `method = "quad"` de la fonction `confint`.

```{r}
confint(mle_galap, level = 0.95, method = "quad")
```

---

# Résumé

- Pour un modèle statistique, la vraisemblance est une fonction qui associe à chaque valeur des paramètres la probabilité des données observées, conditionnelle à cette paramétrisation.

--

- Selon le principe du maximum de vraisemblance, le meilleur estimé des paramètres est celui qui maximise la vraisemblance.

--

- Afin de déterminer le maximum de vraisemblance pour un modèle personnalisé dans R, il faut créer une fonction qui calcule la log-vraisemblance en fonction des paramètres, puis faire appel à un algorithme d'optimisation pour trouver le maximum.

---

# Résumé

- Le test du rapport de vraisemblance permet de tester une hypothèse sur la valeur d'un paramètre estimé au moyen du maximum de vraisemblance, d'obtenir un intervalle de confiance pour ce paramètre, ou de comparer deux modèles nichés.

--

- Pour estimer l'incertitude d'un estimé dans un modèle avec plusieurs paramètres ajustables, nous pouvons soit calculer la vraisemblance profilée pour ce paramètre, soit avoir recours à l'approximation quadratique.

