<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Maximum likelihood</title>

<script src="libs/header-attrs-2.23/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">



<h1 class="title toc-ignore">Maximum likelihood</h1>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Maximum Likelihood is a general method for estimating the parameters
of a statistical model. The parameters of a statistical model define the
structure of the model and are estimated from the observed (or
available) data during the model fitting process.</p>
<p>The model’s structure is the specific functional form assumed for the
data distribution. The functional form of a model is the mathematical
expression representing how variables are combined to describe the
underlying relationship between them. Thus, the functional form depends
on the nature of the involved variables (count, continuous data,
categories, etc.) and their relationships. The goal of a parametric
model is to fit a distribution to your data that can generalize the
relationship between your variables to all cases similar to your case
study.</p>
<p>Suppose we have a series of observations of a random variable <span
class="math inline">\(y\)</span> and a potential statistical model for
this variable. This model may include the dependence of <span
class="math inline">\(y\)</span> on other predictor variables, as well
as a statistical distribution for the unexplained portion of the
variation in <span class="math inline">\(y\)</span>. In general, such a
model contains various unknown parameters that need to be adjusted to
the observed data.</p>
<p>According to Maximum Likelihood, the best estimates of a model’s
parameters are those that maximize the probability of the observed
values of the variable. This method can be applied regardless of the
mathematical form of the model, allowing us to choose models most
compatible with our understanding of natural processes, without being
limited by models already implemented in statistical software (The
Bayesian methods we will see later in the course also have this
versatility).</p>
<p>If the general Maximum Likelihood method has not been presented in
the preceding course (ECL7102), some of the methods covered in this
course were based on this principle:</p>
<ul>
<li><p>Model selection using AIC is based on the likelihood
function.</p></li>
<li><p>Parameter estimation for generalized linear models is done by
maximizing likelihood.</p></li>
<li><p>Parameter estimation for mixed-effects linear models uses a
modified version of Maximum Likelihood (Restricted Maximum Likelihood or
REML).</p></li>
</ul>
<div id="course-content" class="section level2">
<h2>Course Content</h2>
<ul>
<li><p>Principle of Maximum Likelihood</p></li>
<li><p>Application of Maximum Likelihood in R</p></li>
<li><p>Likelihood Ratio Test</p></li>
<li><p>Calculation of Confidence Intervals</p></li>
</ul>
<p>Estimation of Multiple Parameters: Profile Likelihood and Linear
Approximation - Estimation de plusieurs paramètres: vraisemblance
profilée et approximation linéaire</p>
</div>
</div>
<div id="principle-of-maximum-likelihood" class="section level1">
<h1>Principle of Maximum Likelihood</h1>
<div id="likelihood-function" class="section level2">
<h2>Likelihood Function</h2>
<p>Likelihood is a measure of the consistency between the observed data
and the possible values of a model’s parameters. Likelihood is thus a
measure of the probability that the observed data will occur given the
estimated parameters.</p>
<p>Suppose we want to estimate the germination rate of a batch of seeds
by germinating 20 of these seeds under the same conditions. If the
variable <span class="math inline">\(y\)</span> represents the number of
seeds that successfully germinated for one realization of the
experiment, then <span class="math inline">\(y\)</span> follows a
binomial distribution:</p>
<p><span class="math display">\[f(y \vert p) = {n \choose y} p^y
(1-p)^{n-y} \]</span></p>
<p>where the number of trials <span class="math inline">\(n =
20\)</span>, <span class="math inline">\(p\)</span> is the germination
probability for the population, and <span class="math inline">\({n
\choose y}\)</span> represents the number of ways to choose <span
class="math inline">\(y\)</span> individuals out of <span
class="math inline">\(n\)</span>. We write <span
class="math inline">\(f(y \vert p)\)</span> to specify that this
distribution of <span class="math inline">\(y\)</span> is
<em>conditional</em> on a certain value of <span
class="math inline">\(p\)</span>.</p>
<p><em>Note</em>: In the binomial distribution, Y represents the number
of successes in a fixed number of independent trials, each with a
success probability P. If we want to examine the distribution of Y under
the assumption that P has a specific value, we can express this as a
distribution conditional on the value of P that we have established.</p>
<p>For example, here is the distribution of <span
class="math inline">\(y\)</span> if <span class="math inline">\(p =
0.2\)</span>. The probability of obtaining <span class="math inline">\(y
= 6\)</span> in this case is approximately 0.11 (dotted line on the
graph).</p>
<pre class="r"><code>ggplot(data.frame(x = 0:20), aes(x)) +
    labs(x = &quot;y&quot;, y = &quot;f(y|p=0.2)&quot;) +
    stat_function(fun = dbinom, n = 21, args = list(size = 20, prob = 0.2),
                  geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;white&quot;) +
    geom_segment(aes(x = 0, xend = 6, y = dbinom(6, 20, 0.2),
                     yend = dbinom(6, 20, 0.2)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0))</code></pre>
<pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>If we have observed <span class="math inline">\(y = 6\)</span>, but
we do not know the value of <span class="math inline">\(p\)</span>, the
same equation allows us to calculate the probability of obtaining this
<span class="math inline">\(y\)</span> for each possible value of <span
class="math inline">\(p\)</span>. Viewed as a function of <span
class="math inline">\(p\)</span>, rather than <span
class="math inline">\(y\)</span>, this same equation corresponds to the
likelihood function (denoted <span class="math inline">\(L\)</span>) of
<span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[L(p) = f(y \vert p) = {n \choose y} p^y
(1-p)^{n-y}\]</span></p>
<p>Here is the form of <span class="math inline">\(L(p)\)</span> for
<span class="math inline">\(y = 6\)</span> and <span
class="math inline">\(n = 20\)</span>:</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = &quot;density&quot;) +
    geom_segment(aes(x = 0, xend = 0.2, y = dbinom(6, 20, 0.2),
                     yend = dbinom(6, 20, 0.2)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    geom_segment(aes(x = 0.2, xend = 0.2, y = 0, yend = dbinom(6, 20, 0.2)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The likelihood of <span class="math inline">\(p = 0.2\)</span> for
this observation of <span class="math inline">\(y\)</span> is therefore
also 0.11. Note that <span class="math inline">\(f(y \vert p)\)</span>
was a discrete distribution, but since <span
class="math inline">\(p\)</span> is a continuous parameter, the
likelihood <span class="math inline">\(L(p)\)</span> is defined for all
real values between 0 and 1.</p>
<p>In a more general context, let’s assume that <span
class="math inline">\(y = (y_1, y_2, ..., y_n)\)</span> is a vector of
observations, and <span class="math inline">\(\theta = (\theta_1, ...,
\theta_m)\)</span> is a vector of adjustable parameters of the proposed
model to explain these observations. In this case, the likelihood of a
specific vector of values for <span
class="math inline">\(\theta\)</span> corresponds to the joint
probability of the observations of <span
class="math inline">\(y\)</span>, conditioned on these values of <span
class="math inline">\(\theta\)</span>. We will see a specific example of
calculating <span class="math inline">\(L\)</span> for a multi-parameter
model (normal distribution) in the next section.</p>
<p><span class="math display">\[L(\theta) = p(y | \theta)\]</span>
<em>Note</em>: Even though the value of <span
class="math inline">\(L(\theta)\)</span> for a given <span
class="math inline">\(\theta\)</span> corresponds to a probability, the
likelihood function is not a probability distribution because, in the
theory presented here, <span class="math inline">\(\theta\)</span> is
not a random variable. Also, the integral of a likelihood function (area
under the curve of <span class="math inline">\(L(\theta)\)</span>
vs. <span class="math inline">\(\theta\)</span>) is not always equal to
1, unlike that of a probability density.</p>
</div>
<div id="maximum-likelihood" class="section level2">
<h2>Maximum Likelihood</h2>
<p>The maximum likelihood method is an parameter estimation approach
aiming to find values that maximize the likelihood of observed data
based on specific parameters of a statistical model. In other words, it
seeks to minimize the differences between the observed or available data
and the modeled data using a certain combination of parameters.</p>
<p>According to the maximum likelihood principle, the best estimate of
the model parameters based on our observations <span
class="math inline">\(y\)</span> is the vector of values <span
class="math inline">\(\theta\)</span> that maximizes the value of <span
class="math inline">\(L(\theta)\)</span>.</p>
<div id="example-binomial-distribution" class="section level3">
<h3>Example: Binomial Distribution</h3>
<p>Here is a binomial distribution: <span class="math display">\[y \sim
Binomial(n,p)\]</span> It is a distribution defined by two parameters,
n, the number of trials, and p, the probability of success. The maximum
likelihood method helps us calculate the success proportion in the
population <span class="math inline">\(\hat{p}\)</span> based on the
observed or available data, representing our sample.</p>
<p>The estimate of <span class="math inline">\(\hat{p}\)</span>
according to maximum likelihood is given by:</p>
<p><span class="math display">\[\hat{p} = \frac{y}{n}\]</span>
<strong>Note</strong>: The calculation demonstration is presented in
Bolker’s referenced book chapter.</p>
<p>The proportion of success in the sample <span
class="math inline">\(\hat{p}\)</span> is the best estimate of the
probability of success <span class="math inline">\(p\)</span> in the
population.</p>
<p>If <span class="math inline">\(\hat{p}\)</span> represents a point
estimate of the proportion of success in the population, it is possible
to construct a confidence interval around <span
class="math inline">\(\hat{p}\)</span> in which we can be confident that
the true value of p lies. The confidence interval is given by the
function <span class="math inline">\(L(p)\)</span>. This function
depends on <span class="math inline">\(\hat{p}\)</span>, the critical
value associated with the chosen confidence level <span
class="math inline">\(z\)</span>, and the sample size <span
class="math inline">\(n\)</span>.</p>
<p>The value of p that directly maximizes the likelihood function is
obtained through optimization algorithms; however, if you simulate a
sample from the following binomial distribution data, you can detect
this value in the graph.</p>
<p>So, we start by simulating a binomial distribution, and we need two
parameters for this: 1) <span class="math inline">\(y\)</span>: the
number of successes we want to evaluate. 2) <span
class="math inline">\(n\)</span>: the sample size for different success
probability values.</p>
<p>Using the stat_function function from the ggplot2 package, it is
possible to plot a probability density function for the binomial
distribution with the chosen parameters. The probability density
function is the function that assigns a relative probability to each
possible value of a continuous random variable.</p>
<p>Our binomial function can be defined in advance. If <span
class="math inline">\(y = 6\)</span> and <span class="math inline">\(n =
20\)</span>, then:</p>
<pre class="r"><code>binom_density &lt;- function(x) dbinom(6, size = 20, prob = x)</code></pre>
<p>The argument prob = x specifies that we want to evaluate the
probability for different values of the success probability represented
by the variable x.</p>
<p>By visualizing the graph obtained with ggplot, we can observe that
the maximum of <span class="math inline">\(L(p)\)</span> is achieved for
<span class="math inline">\(p = 0.3\)</span>.</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(fun = binom_density,
                  geom = &quot;density&quot;) +
    geom_segment(aes(x = 0, xend = 0.3, y = dbinom(6, 20, 0.3),
                     yend = dbinom(6, 20, 0.3)), 
                color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
   geom_segment(aes(x = 0.3, xend = 0.3, y = 0, yend = dbinom(6, 20, 0.3)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="exemple-modèle-linéaire-et-log-vraisemblance"
class="section level3">
<h3>Exemple: Modèle linéaire et Log-vraisemblance</h3>
<p>In the simple linear regression model, the response variable <span
class="math inline">\(y\)</span> follows a normal distribution, with a
mean linearly dependent on the predictor <span
class="math inline">\(x\)</span> and a constant standard deviation <span
class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[y \sim N(\beta_0 + \beta_1 x,
\sigma)\]</span></p>
<p>This model has three parameters to estimate: 1) the intercept <span
class="math inline">\(\beta_0\)</span>, 2) the slope <span
class="math inline">\(\beta_1\)</span>, and 3) the error term <span
class="math inline">\(\sigma\)</span>. The error term is the difference
between the observed value of <span class="math inline">\(y\)</span> and
the value predicted by the model. In other words, it represents the
variance not explained by the independent variables.</p>
<p>Similar to the binomial distribution, it is possible to calculate the
probability density of an observation of <span
class="math inline">\(y\)</span> with a suitable formula. In this case,
we are not estimating the proportion of success in the sample (<span
class="math inline">\(\hat{p}\)</span>) as before, but rather the
probability density of the random variable <span
class="math inline">\(y\)</span>, given by the formula:</p>
<p><span class="math display">\[f(y \vert \beta_0, \beta_1, \sigma) =
\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{y - \beta_0 -
\beta_1 x}{\sigma} \right)^2}\]</span></p>
<p>The likelihood function for the entire dataset is the product of
probability densities for all observations. If we have <span
class="math inline">\(n\)</span> independent observations of <span
class="math inline">\(y\)</span> (each with the value of the predictor
<span class="math inline">\(x\)</span>), their joint probability density
is given by the product (denoted <span
class="math inline">\(\Pi\)</span>) of individual probability densities.
As a function of the parameters, the following equation gives the joint
likelihood of <span class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span>, and <span
class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[L(\beta_0, \beta_1, \sigma) = f(y_1,
..., y_n \vert \beta_0, \beta_1, \sigma) = \prod_{i=1}^n \frac{1}{\sigma
\sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{y_i - \beta_0 - \beta_1
x_i}{\sigma} \right)^2}\]</span> The goal is to find the values of <span
class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span>, and <span
class="math inline">\(\sigma\)</span> that maximize this likelihood
function. However, it is often easier to calculate the log-likelihood,
denoted as <span class="math inline">\(l = \log L\)</span>. Since the
logarithm is a monotone function – meaning if <span
class="math inline">\(L\)</span> increases, <span
class="math inline">\(\log L\)</span> also increases – the parameter
values that maximize <span class="math inline">\(l\)</span> will also
maximize <span class="math inline">\(L\)</span>.</p>
<p>As the logarithm transforms a product into a sum, after grouping
constant terms, the log-likelihood for the linear regression problem
above corresponds to:</p>
<p><span class="math display">\[l(\beta_0, \beta_1, \sigma) = n \log
\left( \frac{1}{\sigma \sqrt{2 \pi}} \right) - \frac{1}{2
\sigma^2}  \sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i
\right)^2\]</span> From this point, the process to estimate the
parameters at maximum likelihood involves calculating the partial
derivatives of the log-likelihood with respect to each parameter. By
setting these derivatives to zero and solving, we find the parameter
estimates. Checking for concavity ensures that these estimates are
global maxima. Finally, estimating uncertainty is done by calculating
the variance of the estimates, providing an assessment of their
precision.</p>
<p>If we want to see how this could work in R, we can start by
generating an independent variable <span
class="math inline">\(x\)</span> from a normal distribution, then a
dependent variable <span class="math inline">\(y\)</span> that uses a
linear model to simulate a linear relationship between <span
class="math inline">\(y\)</span> and <span
class="math inline">\(x\)</span>. For this, <span
class="math inline">\(y\)</span> is given by two coefficients (intercept
and slope) plus the error term, which follows a normal distribution.</p>
<pre class="r"><code>set.seed(42)

x &lt;- rnorm(100)

y &lt;- 3 + 2 * x + rnorm(100, mean = 0, sd = 1) #intercept =3 ; pente = 2; erreur = distribution normale

datareg&lt;-data.frame(x,y)
plot(x,y)</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Next, we create a function called log_likelihood that takes two
arguments: parameters (a vector containing the model parameters) and
data (the observed data).</p>
<p>Definition of the log-likelihood function:</p>
<pre class="r"><code>x &lt;- rnorm(100)
y &lt;- 3 + 2 * x + rnorm(100, mean = 0, sd = 1)

datareg &lt;- data.frame(x, y)

norm_nll &lt;- function(y_o, x_c, sigma) {
  mu &lt;- y_o + x_c * datareg$x
  -sum(dnorm(datareg$y, mu, sigma, log = TRUE))
}

# Estimate parameters by maximizing the log-likelihood with mle2 from the bbmle package

mle_norm &lt;- mle(norm_nll, start = list(y_o = 1, x_c = 0, sigma = 1))
mle_norm</code></pre>
<pre><code>## 
## Call:
## mle(minuslogl = norm_nll, start = list(y_o = 1, x_c = 0, sigma = 1))
## 
## Coefficients:
##       y_o       x_c     sigma 
## 3.0324929 1.9604096 0.8708721</code></pre>
<pre class="r"><code>lm(y ~ x, datareg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = datareg)
## 
## Coefficients:
## (Intercept)            x  
##       3.033        1.960</code></pre>
<p>Here, having equivalent parameter estimates in the case of Ordinary
Least Squares (OLS) and Maximum Likelihood Estimation (MLE) depends on
the assumption in linear regression that the model errors are assumed to
follow a normal distribution.</p>
</div>
</div>
</div>
<div id="maximum-likelihood-application-in-r" class="section level1">
<h1>Maximum Likelihood Application in R</h1>
<div id="example-galapagos-islands-plants" class="section level2">
<h2>Example: Galapagos Islands Plants</h2>
<p>The file <a href="../donnees/galapagos.csv">galapagos.csv</a>
contains a dataset on the species richness of plants on 30 islands in
the Galapagos Archipelago (<em>Source</em>: Johnson, M.P. et Raven, P.H.
1973. Species number and endemism: The Galapagos Archipelago revisited.
<em>Science</em> 179: 893–895.)</p>
<pre class="r"><code>galap &lt;- read.csv(&quot;../donnees/galapagos.csv&quot;)
str(galap)</code></pre>
<pre><code>## &#39;data.frame&#39;:    30 obs. of  8 variables:
##  $ Name     : chr  &quot;Baltra&quot; &quot;Bartolome&quot; &quot;Caldwell&quot; &quot;Champion&quot; ...
##  $ Species  : int  58 31 3 25 2 18 24 10 8 2 ...
##  $ Endemics : int  23 21 3 9 1 11 0 7 4 2 ...
##  $ Area     : num  25.09 1.24 0.21 0.1 0.05 ...
##  $ Elevation: int  346 109 114 46 77 119 93 168 71 112 ...
##  $ Nearest  : num  0.6 0.6 2.8 1.9 1.9 8 6 34.1 0.4 2.6 ...
##  $ Scruz    : num  0.6 26.3 58.7 47.4 1.9 ...
##  $ Adjacent : num  1.84 572.33 0.78 0.18 903.82 ...</code></pre>
<p>We will model these data with a negative binomial distribution. This
distribution is suitable for representing count data with variance
greater than that predicted by the Poisson distribution.</p>
<p>If a variable <span class="math inline">\(y\)</span> follows a
Poisson distribution, then its mean and variance are both given by the
same parameter <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[y \sim
\textrm{Pois}(\lambda)\]</span></p>
<p>The negative binomial distribution has two parameters, providing
greater flexibility than the Poisson distribution because it allows for
variation in the number of trials needed to achieve a specified number
of successes.</p>
<p>In a negative binomial distribution, the variance is generally
greater than the mean, allowing for greater dispersion..</p>
<p><span class="math display">\[y \sim \textrm{NB}(\mu,
\theta)\]</span></p>
<p>In this model, <span class="math inline">\(y\)</span> has a mean of
<span class="math inline">\(\mu\)</span> and a variance of <span
class="math inline">\(\theta\)</span>.</p>
<p><span class="math inline">\(\mu\)</span> represents the mean of the
distribution, i.e., the average number of trials needed to achieve the
specified number of successes.</p>
<p><span class="math inline">\(\theta\)</span> is the dispersion
parameter that influences the variability of the distribution and
determines, along with the mean, the success probability <span
class="math inline">\(p\)</span>, according to the formula:</p>
<p><span class="math display">\[p= \frac{\theta}{\mu
+\theta}\]</span></p>
<p>Adjusting <span class="math inline">\(\theta\)</span> allows for
better modeling of situations where data variability is significant,
which may be the case in modeling count data.</p>
<p>The parameter <span class="math inline">\(\theta\)</span> is always
positive. A small value of <span class="math inline">\(\theta\)</span>
represents a more variable distribution, while if <span
class="math inline">\(\theta\)</span> is very high, the second term is
negligible, and the distribution tends toward that of Poisson.</p>
<p>Similar to Poisson regression, the negative binomial model uses a
logarithmic link to connect <span class="math inline">\(\mu\)</span> to
a linear function of predictors.</p>
<p><span class="math display">\[\log\mu = \beta_0 + \beta_1 x_1 +
\beta_2 x_2 + ...\]</span></p>
<p>For this example, we will fit the model of the number of species
(Species) based on the island’s area (Area, in km<span
class="math inline">\(^2\)</span>) and the distance to the nearest
island (Nearest, in km). We also take the logarithm of each
predictor.</p>
</div>
<div id="use-of-the-bbmle-package-bbmle" class="section level2">
<h2>Use of the bbmle package <em>bbmle</em></h2>
<p>Most models do not allow for analytical derivation of the maximum
likelihood position. In such cases, we resort to optimization algorithms
that numerically estimate the maximum value of the (log-)likelihood
function and the value of each parameter corresponding to this
maximum.</p>
<p>In R, the optim function is a general tool for determining the
minimum or maximum of a given function. However, there are also
functions specialized for maximum likelihood estimation problems. In
this course, we will use the mle2 function from the bbmle package.</p>
<p>First, we need to write a function that calculates the negative
log-likelihood for our problem. By convention, optimization algorithms
require a function to minimize. Therefore, instead of maximizing the
log-likelihood, we minimize its negation.</p>
<pre class="r"><code>nll_galap &lt;- function(b_0, b_area, b_near, theta) {
    mu_sp &lt;- exp(b_0 + b_area * log(galap$Area) + b_near * log(galap$Nearest))
    -sum(dnbinom(galap$Species, mu = mu_sp, size = theta, log = TRUE))
}</code></pre>
<p>The nll_galap function above accepts four parameters corresponding to
the three coefficients of the linear predictor needed to calculate <span
class="math inline">\(\mu\)</span> and the <span
class="math inline">\(\theta\)</span> parameter of the negative binomial
distribution.</p>
<p>The first line of the function calculates the linear predictor and
takes its exponential to obtain the average number of species mu_sp.
Reminder: In R, most mathematical operations are performed element-wise
on vectors. Thus, mu_sp contains 30 values, the first calculated from
the predictor values for island 1, the second for the values of island
2, and so on.</p>
<p>The second line calculates the log-likelihood of each observation
according to the binomial model with dnbinom (also element-wise), then
sums them up and takes the negation.</p>
<p>Note that we specify log = TRUE in dnbinom to calculate the logarithm
of the likelihood. As seen before, the log-likelihood of a set of
observations is equal to the sum of their individual log-likelihoods
when the observations are independent.</p>
<p>Finally, we load the bbmle package and call the mle2 function. The
first argument of this function is our function calculating the negation
of the log-likelihood. We also need to specify for the start argument a
list of initial values for each parameter that the algorithm will use to
start the search for the maximum.</p>
<p>The exact choice of initial values is not critical in most cases, but
it is recommended to provide plausible values (not too extreme) for the
parameters. Therefore, we choose a null value for each coefficient but a
positive value for <span class="math inline">\(\theta\)</span>, which
must be greater than zero.</p>
<pre class="r"><code>library(bbmle)

mle_galap &lt;- mle2(nll_galap, start = list(b_0 = 0, b_area = 0, b_near = 0, theta = 1))
mle_galap</code></pre>
<pre><code>## 
## Call:
## mle2(minuslogl = nll_galap, start = list(b_0 = 0, b_area = 0, 
##     b_near = 0, theta = 1))
## 
## Coefficients:
##        b_0     b_area     b_near      theta 
##  3.3352151  0.3544290 -0.1042696  2.7144722 
## 
## Log-likelihood: -137.98</code></pre>
<p>The execution of the function produces several warnings (## Warning
in dnbinom(galap$Species, mu = mu_sp, size = theta, log = TRUE):##
Production of NaN) in R, which are not shown here. These warnings likely
result from cases where the algorithm attempts to assign a negative
value to theta and generates an error. In such cases, it simply tries a
new value.</p>
</div>
<div id="interpretation-of-likelihood" class="section level2">
<h2>Interpretation of likelihood</h2>
<p>Note that the maximum of the log-likelihood in the above result is
equal to -137.98, which corresponds to a very small likelihood
value:</p>
<pre class="r"><code>exp(-137.98)</code></pre>
<pre><code>## [1] 1.191372e-60</code></pre>
<p>Likelihood corresponds to the probability of obtaining exactly the
values that appear in the dataset according to the model. Considering
the numerous possible values for an observation of the variable and the
fact that these possibilities multiply for each subsequent observation,
it is not surprising that this probability is very low, especially for a
large sample.</p>
<p>The absolute value of likelihood is not easily interpretable.
Instead, its relative value allows for comparing the fit of different
parameter values applied to the same observed data.</p>
<p>Nevertheless, working with numbers extremely close to zero can be
challenging; that’s one of the reasons why the logarithm of likelihood
is used in practice.</p>
</div>
<div id="when-to-use-maximum-likelihood" class="section level2">
<h2>When to use Maximum Likelihood?</h2>
<p>For our example, we could have used the glm.nb function from the MASS
package, specifically designed to estimate parameters for a negative
binomial regression. By fitting our model with this function, we can
verify that the results align with the application of mle2.</p>
<pre class="r"><code>library(MASS)
glm.nb(Species ~ log(Area) + log(Nearest), galap)</code></pre>
<pre><code>## 
## Call:  glm.nb(formula = Species ~ log(Area) + log(Nearest), data = galap, 
##     init.theta = 2.714482206, link = log)
## 
## Coefficients:
##  (Intercept)     log(Area)  log(Nearest)  
##       3.3352        0.3544       -0.1043  
## 
## Degrees of Freedom: 29 Total (i.e. Null);  27 Residual
## Null Deviance:       138.7 
## Residual Deviance: 32.7  AIC: 284</code></pre>
<p>The functions available in R and various packages already cover a
good number of common models, including linear models, generalized
linear models, mixed models, and others. Moreover, several non-linear
models can be linearized with an appropriate transformation. For
example, a power-law relationship between the number of species <span
class="math inline">\(S\)</span> and habitat area <span
class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[S = cA^z\]</span></p>
<p>This can be transformed into a linear relationship by taking the
logarithm on each side:</p>
<p><span class="math display">\[\log(S) = \log(c) + z
\log(A)\]</span></p>
<p>When a specialized function is available to estimate the parameters
of a model, it is simpler to use it rather than coding the model oneself
and applying maximum likelihood.</p>
<p>However, there are cases where the assumed model for the data does
not fit into a standard format. Here are some examples in forest
ecology.</p>
<p><strong>Ajustement d’une courbe de dispersion</strong> (ex.: Clark et
al. 1999)</p>
<p>Une façon d’estimer la capacité de dispersion d’une espèce de plantes
est d’échantillonner les graines tombant dans des pièges placés à
différentes distances de plantes mères. En particulier, on s’intéresse à
estimer la courbe de dispersion <span
class="math inline">\(f(r)\)</span> qui correspond à la probabilité
qu’une graine tombe à une distance <span
class="math inline">\(r\)</span> de son point d’origine.</p>
<p>Supposons que <span class="math inline">\(y\)</span> représente le
nombre de graines dans un des pièges et peut être représenté par une
distribution binomiale négative.</p>
<p><span class="math display">\[y_i \sim \textrm{NB}(\mu_i,
\theta)\]</span></p>
<p>Le nombre de graines moyen dans le piège <span
class="math inline">\(i\)</span>, <span
class="math inline">\(\mu_i\)</span>, correspond à la somme des
contributions de chaque plante mère <span
class="math inline">\(j\)</span> située à proximité; cette contribution
est égale au nombre de graines produites par une plante mère (<span
class="math inline">\(b\)</span>, que nous supposons fixe) multiplié par
la courbe de dispersion évaluée pour la distance <span
class="math inline">\(r_{ij}\)</span> entre le piège <span
class="math inline">\(i\)</span> et la plante <span
class="math inline">\(j\)</span>.</p>
<p><span class="math display">\[y_i \sim \textrm{NB}(\sum_j b\times
f(r_{ij}), \theta)\]</span></p>
<p>Puisque <span class="math inline">\(f\)</span> est une fonction
non-linéaire avec ses propres paramètres à ajuster, puis que la moyenne
de <span class="math inline">\(y\)</span> contient la somme de valeurs
de <span class="math inline">\(f\)</span> évaluées à différentes
distances, il est nécessaire de créer sa propre fonction de
vraisemblance et la maximiser avec un outil comme <code>mle2</code>.</p>
<p><strong>Fitting a dispersion curve</strong> (ex.: Canham et
al. 2004)</p>
<p>The growth of trees in a forest can be reduced by competition from
neighboring trees. If we assume that the competition exerted on tree
<span class="math inline">\(i\)</span> by neighbor <span
class="math inline">\(j\)</span> increases with the diameter <span
class="math inline">\(D_j\)</span> of that neighbor and decreases with
the distance <span class="math inline">\(r_{ij}\)</span> between the two
trees, we can define a competition index (CI) that sums the effects of
each neighbor on <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[CI_i = \sum_j
\frac{D_j^{\delta}}{r_{ij}^{\gamma}}\]</span></p>
<p>We want to estimate the powers <span
class="math inline">\(\delta\)</span> and <span
class="math inline">\(\gamma\)</span> appearing in the index from the
data. Suppose we have a linear model for the growth <span
class="math inline">\(y_i\)</span> of tree $i,” where we add a term
dependent on this index:</p>
<p><span class="math display">\[y_i = \beta_0 + ... + \beta_{CI} \sum_j
\frac{D_j^{\delta}}{r_{ij}^{\gamma}}\]</span></p>
<p>There is no way to simplify this last term, so the maximum likelihood
can be useful for estimating the coefficients (all <span
class="math inline">\(\beta\)</span>, <span
class="math inline">\(\gamma\)</span>, and <span
class="math inline">\(\delta\)</span>) of this now non-linear model.</p>
</div>
<div id="limits-of-maximum-likelihood" class="section level2">
<h2>Limits of Maximum Likelihood</h2>
<p>Most advantageous properties of maximum likelihood estimates,
including lack of bias, hold true in the limit where the sample size is
large. What constitutes a sufficiently large sample depends on the model
and particularly the number of parameters to estimate.</p>
<p>In practice, maximum likelihood is obtained through a numerical
algorithm searching for the maximum through an iterative process. A
complex likelihood function could have multiple local maxima (points
where the function is maximized concerning values close to the
parameters), in which case it is not guaranteed that the algorithm finds
the global maximum (the one with the highest likelihood).</p>
</div>
</div>
<div id="likelihood-ratio-test" class="section level1">
<h1>Likelihood Ratio Test</h1>
<div id="testing-a-parameter-value" class="section level2">
<h2>Testing a Parameter Value</h2>
<p>It is possible to use the likelihood function to test a hypothesis
about the value of a parameter.</p>
<p>For example, consider the likelihood function calculated at the
beginning of the course to estimate the germination probability of a
batch of seeds if 6 seeds germinated out of 20 trials.</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = &quot;density&quot;) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>In this case, the maximum likelihood estimate is <span
class="math inline">\(\hat{p} = 0.3\)</span>. Suppose the seed provider
claims that their germination rate is 50%. Is the result of the
experiment compatible with this value?</p>
<p>The likelihood corresponding to the null hypothesis (<span
class="math inline">\(p_0 = 0.5\)</span>) is approximately <span
class="math inline">\(L(p_0) = 0.037\)</span>, compared to a maximum of
<span class="math inline">\(L(\hat{p}) = 0.192\)</span>.</p>
<pre class="r"><code>l_0 &lt;- dbinom(6, 20, prob = 0.5)
l_max &lt;- dbinom(6, 20, prob = 0.3)
c(l_0, l_max)</code></pre>
<pre><code>## [1] 0.03696442 0.19163898</code></pre>
<p>The ratio between these two values of <span
class="math inline">\(L\)</span> is used to define a statistic for the
likelihood ratio test. This statistic corresponds to -2 times the
logarithm of the ratio between the likelihood of the parameter under the
null hypothesis and the maximum likelihood estimate.</p>
<p><span class="math display">\[- 2 \log \left(
\frac{L(\theta_0)}{L(\hat{\theta})} \right)\]</span></p>
<p>Equivalently, the ratio can be replaced by the difference of
log-likelihoods:</p>
<p><span class="math display">\[- 2 \left( l(\theta_0) - l(\hat{\theta})
\right)\]</span></p>
<p>The factor of -2 was chosen so that, if the null hypothesis is true
and the sample size is large enough, the distribution of this statistic
approaches the <span class="math inline">\(\chi^2\)</span> distribution
with 1 degree of freedom. In our example, the likelihood ratio statistic
is equal to 3.29.</p>
<pre class="r"><code>rv &lt;- -2*log(l_0 / l_max)
rv</code></pre>
<pre><code>## [1] 3.291315</code></pre>
<p>The probability of obtaining a ratio greater than or equal to this,
if the null hypothesis <span class="math inline">\(p = 0.5\)</span> is
true, can be approximated using the cumulative distribution of the <span
class="math inline">\(\chi^2\)</span>.</p>
<pre class="r"><code>1 - pchisq(rv, df = 1)</code></pre>
<pre><code>## [1] 0.06964722</code></pre>
<p><em>Note</em>: The likelihood-ratio test does not apply if the null
hypothesis is at the limit of possible values for a parameter. For
example, for the parameter <span class="math inline">\(p\)</span> of a
binomial distribution, we cannot use this test for the null hypothesis
<span class="math inline">\(p_0 = 0\)</span> or <span
class="math inline">\(p_0 = 1\)</span>.</p>
</div>
<div id="model-comparison" class="section level2">
<h2>Model Comparison</h2>
<p>The likelihood-ratio test is also used to compare two models. In this
case, the models must be nested, meaning that the simpler model contains
a subset of the parameters of the more complex model. For example,
consider a linear regression model with 1 predictor (M1) and a second
model with 3 predictors (M2).</p>
<ul>
<li>M1: <span class="math inline">\(y = \beta_0 + \beta_1 x_1 +
\epsilon\)</span></li>
<li>M2: <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2
x_2 + \beta_3 x_3 + \epsilon\)</span></li>
</ul>
<p>In this case, M1 can be seen as a version of M2 where <span
class="math inline">\(\beta_2\)</span> and <span
class="math inline">\(\beta_3\)</span> are set to 0. If M1 is the true
model for the data, the likelihood-ratio statistic between the two
models approximately follows a <span
class="math inline">\(\chi^2\)</span> distribution, with degrees of
freedom equal to the difference in the number of estimated parameters
between the two models (here, 2).</p>
<p><span class="math display">\[- 2 \left( l_{M1} - l_{M2} \right) \sim
\chi^2(2)\]</span></p>
<p>In the course ECL7102, we studied model comparison using the Akaike
Information Criterion (AIC):</p>
<p><span class="math display">\[AIC = - 2 \log L + 2K =  -2l +
2K\]</span></p>
<p>In this formula, <span class="math inline">\(K\)</span> is the number
of adjustable parameters in the model. We also saw a correction to the
AIC (AICc) for “small” samples (when <span
class="math inline">\(N/K\)</span> &lt; 30, where <span
class="math inline">\(N\)</span> is the sample size).</p>
<p>The AIC has a broader scope than the likelihood-ratio test since it
allows the comparison of more than two models, whether nested or not.
When both methods apply, their goals are different:</p>
<ul>
<li><p>AIC aims to identify the model that best predicts the response
for a new sample from the same population;</p></li>
<li><p>The likelihood-ratio test indicates whether the observed
difference between the fit of the simplest model and the most complex
model is compatible with the hypothesis that the simplest model is
correct.</p></li>
</ul>
</div>
</div>
<div id="confidence-interval-calculation" class="section level1">
<h1>Confidence Interval Calculation</h1>
<p>If <span class="math inline">\(\hat{\theta}\)</span> is the maximum
likelihood estimate for a parameter <span
class="math inline">\(\theta\)</span>, we can obtain a confidence
interval for this parameter using the relationship between hypothesis
testing and confidence interval:</p>
<blockquote>
<p>If we cannot reject the null hypothesis <span
class="math inline">\(\theta = \theta_0\)</span> at a significance level
<span class="math inline">\(\alpha\)</span>, then <span
class="math inline">\(\theta_0\)</span> is part of the <span
class="math inline">\(100(1-\alpha)%\)</span> confidence interval for
<span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<p>For example, the 95% confidence interval bounds are the values of
<span class="math inline">\(\theta\)</span> where the likelihood-ratio
statistic is equal to the 95th percentile of the <span
class="math inline">\(\chi^2\)</span> distribution; it is the maximum
value of the statistic that is not rejected at a significance level
<span class="math inline">\(\alpha = 0.05\)</span>.</p>
<p><span class="math display">\[- 2 \left( l(\theta_0) - l(\hat{\theta})
\right) = \chi^2_{0.95}(1)\]</span></p>
<p><em>Reminder</em>: The <span class="math inline">\(\chi^2\)</span>
test is one-sided, as only high values of the statistic indicate a
significant deviation from the null hypothesis.</p>
<p>By isolating <span class="math inline">\(\theta_0\)</span> in the
equation, we get:</p>
<p><span class="math display">\[l(\theta_0) = l(\hat{\theta}) -
\frac{\chi^2_{0.95}(1)}{2}\]</span></p>
<p>Thus, we need to determine the values of <span
class="math inline">\(\theta\)</span> for which the log-likelihood is
approximately 1.92 lower than the maximum.</p>
<pre class="r"><code>qchisq(0.95, df = 1) / 2</code></pre>
<pre><code>## [1] 1.920729</code></pre>
<div id="exemple" class="section level2">
<h2>Exemple</h2>
<p>For our seed germination example (<span class="math inline">\(\hat{p}
= 0.3\)</span>), the 95% confidence interval bounds correspond to <span
class="math inline">\(L = 0.0281\)</span>.</p>
<pre class="r"><code>exp(dbinom(6, 20, 0.3, log = TRUE) - qchisq(0.95, df = 1)/2)</code></pre>
<pre><code>## [1] 0.02807512</code></pre>
<p>This threshold is represented by the dashed line on the graph below
and corresponds to an approximate interval of (0.132, 0.516) for <span
class="math inline">\(p\)</span>.</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(geom = &quot;area&quot;, fill = &quot;#d3492a&quot;, n = 1000,
        fun = function(x) ifelse(x &gt; 0.132 &amp; x &lt; 0.516,
                                 dbinom(6, 20, prob = x), NA)) +
    stat_function(fun = function(x) dbinom(6, 20, prob = x), 
                  geom = &quot;density&quot;) +
    geom_hline(yintercept = 0.0279, linetype = &quot;dashed&quot;) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>For an experiment with the same estimate of <span
class="math inline">\(\hat{p}\)</span> but a larger sample size <span
class="math inline">\((n = 50, y = 15)\)</span>, the limit of <span
class="math inline">\(L\)</span> for the 95% interval is 0.0179.</p>
<pre class="r"><code>exp(dbinom(15, 50, 0.3, log = TRUE) - qchisq(0.95, df = 1)/2)</code></pre>
<pre><code>## [1] 0.01792382</code></pre>
<p>As seen below, the likelihood function and, consequently, the
confidence interval are narrower.</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(geom = &quot;area&quot;, fill = &quot;#d3492a&quot;, n = 1000,
        fun = function(x) ifelse(x &gt; 0.185 &amp; x &lt; 0.435,
                                 dbinom(15, 50, prob = x), NA)) +
    stat_function(fun = function(x) dbinom(15, 50, prob = x), 
                  geom = &quot;density&quot;) +
    geom_hline(yintercept = 0.0179, linetype = &quot;dashed&quot;) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(breaks = seq(0, 0.12, 0.03), 
                       limits = c(0, 0.13), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="profile-likelihood" class="section level2">
<h2>Profile Likelihood</h2>
<p>If <span class="math inline">\(m\)</span> parameters are estimated
simultaneously, the likelihood function is not a curve but rather an
<span class="math inline">\(m\)</span>-dimensional surface. When
calculating the likelihood ratio <span class="math inline">\(-2 \left(
l(\theta_{0}) - l(\hat{\theta}) \right)\)</span> for different values of
<span class="math inline">\(\theta_{0}\)</span> for one of the
parameters, it is necessary to choose the values for the other <span
class="math inline">\(m - 1\)</span> parameters. A simple solution would
be to fix all other parameters at their maximum likelihood estimated
values, but this assumes that these estimates are independent. In
general, if <span class="math inline">\(\theta_0\)</span> is fixed to a
value other than <span class="math inline">\(\hat{\theta}\)</span>, the
estimate maximizing the likelihood may change.</p>
<p>For example, in the illustrated linear regression model below, the
best estimate of the slope changes if we fix the intercept at 0 (dashed
line).</p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>To construct the curve of <span
class="math inline">\(l(\theta_0)\)</span> for different values of the
parameter, it is necessary, for each fixed value of <span
class="math inline">\(\theta_0\)</span>, to find the maximum likelihood
for the remaining parameters. The resulting curve is called the profile
likelihood.</p>
<p>The profile function from the bbmle package evaluates the profile
likelihood for each parameter based on the result of mle2. Here are the
results obtained for the earlier fitted model (negative binomial
regression for the number of plant species on the Galapagos
Islands).</p>
<pre class="r"><code>galap_pro &lt;- profile(mle_galap)
plot(galap_pro)</code></pre>
<p><img src="03-Maximum_likelihood_modif_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>For each parameter, the graph shows the square root of the likelihood
ratio <span class="math inline">\(\sqrt{- 2 \left( l(\theta_{0}) -
l(\hat{\theta}) \right)}\)</span> for the profile likelihood. The square
root transformation allows for a quick assessment of whether the profile
likelihood is approximately quadratic (see the next section), which
would result in a symmetric “V” shape after transformation.</p>
<p>Various confidence intervals are overlaid on the graph; these
intervals can also be directly obtained using the confint function.</p>
<pre class="r"><code>confint(galap_pro, level = 0.95)</code></pre>
<pre><code>##             2.5 %     97.5 %
## b_0     3.0259619 3.66809720
## b_area  0.2837173 0.42822254
## b_near -0.2600032 0.05105544
## theta   1.5113578 4.69693757</code></pre>
</div>
<div id="quadratic-approximation" class="section level2">
<h2>Quadratic Approximation</h2>
<p>Since calculating the profile likelihood of a parameter requires
repeatedly adjusting the other model parameters, this method can be
time-consuming for a complex model.</p>
<p>A more approximate but much faster method is to assume that the
log-likelihood follows a quadratic form. With a single parameter, this
quadratic form is a parabola centered at the maximum likelihood: <span
class="math inline">\(- 2 \left( l(\theta_{0}) - l(\hat{\theta}) \right)
= a (\theta_{0} - \hat{\theta})^2\)</span>. Here, the coefficient <span
class="math inline">\(a\)</span> measures the curvature of the parabola.
As seen in the binomial example above, the more pronounced this
curvature, the more precise the parameter estimate.</p>
<p>In fact, if the quadratic approximation is good, the variance of
<span class="math inline">\(\hat{\theta}\)</span> (and thus the square
of its standard error) is the inverse of the second derivative of <span
class="math inline">\(-l\)</span>, which measures the curvature at the
maximum.</p>
<p><span
class="math display">\[\frac{\textrm{d}^2(-l)}{\textrm{d}\theta^2} =
\frac{1}{\sigma_{\hat{\theta}}^2}\]</span></p>
<p>With <span class="math inline">\(m\)</span> parameters, the curvature
in <span class="math inline">\(m\)</span> dimensions around the maximum
is represented by an <span class="math inline">\(m \times m\)</span>
matrix of second partial derivatives of <span
class="math inline">\(-l\)</span>, called the Fisher information matrix.
By inverting this matrix, we obtain the variances and covariances of the
estimates. Assuming that the quadratic approximation is correct, these
variances and covariances are sufficient to obtain the desired
confidence intervals for each parameter.</p>
<p>In the bbmle package, confidence intervals under the quadratic
approximation can be calculated by specifying method = “quad” in the
confint function:</p>
<pre class="r"><code>confint(mle_galap, level = 0.95, method = &quot;quad&quot;)</code></pre>
<pre><code>##             2.5 %    97.5 %
## b_0     3.0246480 3.6457823
## b_area  0.2847479 0.4241100
## b_near -0.2536734 0.0451341
## theta   1.1781122 4.2508322</code></pre>
<p>Here, it is observed that the estimates are close to those of the
profile likelihood, except for <span
class="math inline">\(\theta\)</span>. Inspecting the profiles obtained
earlier reveals that the profile for <span
class="math inline">\(\theta\)</span> adheres less to the quadratic
form.</p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>For a statistical model, likelihood is a function that associates
each parameter value with the probability of the observed data,
conditioned on that parameterization. According to the maximum
likelihood principle, the best estimates of the parameters are those
that maximize the likelihood.</p></li>
<li><p>To determine the maximum likelihood for a custom model in R, you
need to create a function that calculates the log-likelihood based on
the parameters and then use an optimization algorithm to find the
maximum.</p></li>
<li><p>The likelihood ratio test allows testing a hypothesis about the
value of a parameter estimated using maximum likelihood, obtaining a
confidence interval for this parameter, or comparing two nested
models.</p></li>
<li><p>To estimate the uncertainty of an estimate in a model with
multiple adjustable parameters, we can either calculate the profile
likelihood for that parameter or use the quadratic
approximation.</p></li>
</ul>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>-The course content is adapted from the ECL8202 - Complex Data
Analysis course given in 2020 by Philippe Marchand, formerly a professor
at IRF-UQAT. You can add this information to the reference.Marchand,
Philippe (2020). ECL8202 - Complex Data Analysis, available on GitHub at
<a href="https://github.com/pmarchand1/ECL8202"
class="uri">https://github.com/pmarchand1/ECL8202</a></p>
<ul>
<li><p>Bolker, B.M. (2008) Ecological models and data in R. Princeton
University Press, Princeton, New Jersey. (Chapitre 6 sur le maximum de
vraisemblance)</p></li>
<li><p>Canham, C.D., LePage, P.T. et Coates, K.D. (2004) A neighborhood
analysis of canopy tree competition: effects of shading versus crowding.
Canadian Journal of Forest Research 34: 778–787.</p></li>
<li><p>Clark, J.S., Silman, M., Kern, R., Macklin, E. et
HilleRisLambers, J. (1999) Seed dispersal near and far: Patterns across
temperate and tropical forests. Ecology 80: 1475–1494.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
