<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Hierarchical Bayesian models, part 2</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Hierarchical Bayesian models, part 2</h1>

</div>


<div id="contents" class="section level1">
<h1>Contents</h1>
<ul>
<li><p>Review: Model comparison and selection</p></li>
<li><p>Bayesian approach to model comparison</p></li>
<li><p>Comparing models with <em>loo</em> and <em>brms</em></p></li>
</ul>
</div>
<div id="model-comparison-and-selection" class="section level1">
<h1>Model comparison and selection</h1>
<p>Suppose we have different statistical models that aim to explain the same data. Models could include different predictors, different distributions of the response, etc. How to determine which model best represents the phenomenon studied?</p>
<p>Most model comparison methods seek to optimize the model’s ability to <strong>predict new observations</strong> of the phenomenon. In other words, it is not enough to assess whether the model approaches the data used to fit it. A more complex model, with more adjustable parameters, will always be closer to this data.</p>
<p>In general, an overly simple model has a large systematic error (bias or underfitting), because it omits significant effects on the response variable; an overly complex model has a large random error (variance or overfitting), because it tends to represent “accidental” associations of a particular sample that do not generalize to the population. The ideal compromise between these two types of error, which minimizes the total error, depends on the amount of data, since a large sample decreases the variance associated with the estimation of many parameters in a complex model.</p>
<p><img src="../images/biais_variance_en.png" /></p>
<p>With large data sets, it is possible to set aside part of the data (often ~20 to 30%) to create a validation set, while the rest of the data form the training set. In this case, each of the candidate models is fit on the training data and the predictive performance of the fitted models is evaluated on the validation set.</p>
<div id="cross-validation" class="section level2">
<h2>Cross-validation</h2>
<p>Setting aside part of the data for validation is not practical if the sample size is small. With relatively little data, each point is important for accurately estimating the parameters of the model; also, the smaller the validation set, the more likely it is to be non-representative of the population.</p>
<p>Cross-validation provides a way to assess predictive performance on new observations without having to set aside a validation set. This method consists in randomly dividing the observations into groups and measuring the quality of the prediction of the observations of a group according to a model fitted to the rest of the observations.</p>
<p>For example, if each group has only one observation (leave-one-out cross-validation), we can evaluate the prediction of each value of the response <span class="math inline">\(y_i\)</span> for a model fitted without observation <span class="math inline">\(i\)</span>. However, this method requires refitting the model <span class="math inline">\(n\)</span> times, where <span class="math inline">\(n\)</span> is the number of observations.</p>
<p>If the number of observations is large, it may be more practical to divide the observations into <span class="math inline">\(k\)</span> groups (k-fold cross-validation), for example <span class="math inline">\(k\)</span> = 10, and to fit each of the models to be compared <span class="math inline">\(k\)</span> times, leaving a different <span class="math inline">\(1/k\)</span> fraction of the observations aside each time.</p>
</div>
<div id="akaike-information-criterion" class="section level2">
<h2>Akaike information criterion</h2>
<p>Since cross-validation methods are costly in terms of calculation, it is useful to be able to approximate the prediction error that would be obtained in cross-validation without having to refit the model several times.</p>
<p>For models fitted by the maximum likelihood method, the Akaike information criterion (AIC) offers a measure of fit based on information theory, which tends to produce the same result as leave-one-out cross-validation if the sample size is large enough. The AIC is calculated as follows:</p>
<p><span class="math display">\[ AIC = -2 \log L + 2 K \]</span></p>
<p>where <span class="math inline">\(L\)</span> is the likelihood function at its maximum and <span class="math inline">\(K\)</span> is the number of parameters estimated by the model. A small AIC value represents a better predictive power of the model. The first term in the equation represents the fit to the observed data, while the second term penalizes more complex models.</p>
<p>The AIC is defined to the nearest additive constant, so its absolute value gives no information. Rather, it is the difference of AIC between the candidate models which is interpretable. This difference is defined with respect to the minimum value of the AIC among the compared models: <span class="math inline">\(\Delta AIC = AIC - \min AIC\)</span>. The best model has a <span class="math inline">\(\Delta AIC = 0\)</span>.</p>
<p>The expression:</p>
<p><span class="math display">\[ e^{-\frac{\Delta AIC}{2} } \]</span></p>
<p>corresponds to the evidence ratio of each model vs. the one with the minimum AIC. For example, <span class="math inline">\(\Delta AIC = 2\)</span> corresponds to a ratio of ~0.37 (~3 times less likely), while <span class="math inline">\(\Delta AIC = 10\)</span> corresponds to a ratio of ~0.0067 (~150 times less likely).</p>
</div>
<div id="multi-model-predictions" class="section level2">
<h2>Multi-model predictions</h2>
<p>With <span class="math inline">\(m\)</span> candidate models, we can use the evidence ratios described above to define the Akaike weight <span class="math inline">\(w\)</span> for each model:</p>
<p><span class="math display">\[w_i = \frac{e^{\frac{-\Delta AIC_i}{2}}}{\sum_{j=1}^{m} e^{\frac{-\Delta AIC_j}{2}}}\]</span></p>
<p>The denominator normalizes each ratio by their sum, so that the sum of the weights <span class="math inline">\(w_i\)</span> equals 1.</p>
<p>If several models are plausible and have a significant Akaike weight, then it is possible to average their predictions for a new observation of the response (this prediction is noted <span class="math inline">\(\tilde{y}\)</span>), by weighting the prediction <span class="math inline">\(\tilde{y_j}\)</span> of each candidate model by its weight <span class="math inline">\(w_j\)</span>.</p>
<p><span class="math display">\[\tilde{y} = \sum_{j = 1}^m w_j \tilde{y_j}\]</span></p>
<p>Multi-model predictions are often more accurate than those obtained by considering only the best model, because they take into account the uncertainty about the form of the model.</p>
</div>
</div>
<div id="bayesian-approach-for-model-comparison" class="section level1">
<h1>Bayesian approach for model comparison</h1>
<div id="predictive-density" class="section level2">
<h2>Predictive density</h2>
<p>For a model estimated by maximum likelihood, the predictions of new observations are obtained by fixing the parameters of the model at their estimated value. The likelihood of this new observation is therefore <span class="math inline">\(p(\tilde {y} | \hat{\ theta})\)</span>, where <span class="math inline">\(\hat{\theta}\)</span> are the maximum likelihood parameter estimates.</p>
<p>In a Bayesian approach, the predictions of new observations are obtained by averaging the predictions over the posterior distribution of the parameters. The predictive density of <span class="math inline">\(\tilde{y}\)</span> conditional on the model fitted to observations <span class="math inline">\(y\)</span>, denoted <span class="math inline">\(p(\tilde{y} | y)\)</span>, is equal to the mean of the likelihood <span class="math inline">\(p(\tilde{y} | \theta)\)</span> for the joint posterior distribution of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math inline">\(p(\tilde{y} | y) = \int p(\tilde{y} | \theta) p(\theta | y) \text{d}\theta\)</span> .</p>
<p>In practice, if a Monte-Carlo method generates <span class="math inline">\(S\)</span> parameter vectors <span class="math inline">\(\theta_{(1)}, ..., \theta_{(S)}\)</span> that approximate the joint posterior distribution, we calculate <span class="math inline">\(p(\tilde{y} | y)\)</span> by averaging the predictions of each vector:</p>
<p><span class="math inline">\(p(\tilde{y} | y) = \frac{1}{S} \sum_{j = 1}^S p(\tilde{y} | \theta_{(j)})\)</span> .</p>
<p>As with likelihood, it is easier to work with the logarithm of the predictive density.</p>
</div>
<div id="cross-validation-1" class="section level2">
<h2>Cross-validation</h2>
<p>To determine the model that maximizes the predictive density of new observations, as defined above, we can use cross-validation. However, since fitting hierarchical Bayesian models sometimes requires considerable computing time, it is not practical in these cases to repeat the estimation of the model a large number of times, leaving aside some of the data. We therefore most often use criteria which approximate the predictive performance of a cross-validation.</p>
<p>If the AIC approximates the cross-validation error for the models fit by maximum likelihood with a fairly large number of observations, this criterion does not apply well to Bayesian models. On the one hand, the parameter values maximizing the likelihood are not directly found when fitting Bayesian models. In addition, it is difficult to define a number of parameters <span class="math inline">\(K\)</span> because of the hierarchical structure and the constraints imposed by the prior distributions of the parameters, which result in those parameters not being completely “free”.</p>
</div>
<div id="selection-criteria-for-hierarchical-bayesian-models" class="section level2">
<h2>Selection criteria for hierarchical Bayesian models</h2>
<div id="dic" class="section level3">
<h3>DIC</h3>
<p>The Deviance Information Criterion (DIC), based on AIC, was one of the first criteria developed for the comparison of Bayesian models:</p>
<p><span class="math display">\[DIC = -2 \log p(y | \bar{\theta}) + 2 p_D\]</span></p>
<p>where <span class="math inline">\(\bar{\theta}\)</span> is the mean of the posterior distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p_D\)</span> is the effective number of parameters, which can be calculated in several ways.</p>
<p>Like the AIC, the DIC represents well the relative predictive performance of models on new data, if the sample size is large enough. However, this is not a Bayesian prediction because it is based on a single estimate of each parameter (its mean value) rather than on the entire posterior distribution.</p>
</div>
<div id="waic" class="section level3">
<h3>WAIC</h3>
<p>The Watanabe-Akaike information criterion (WAIC) is similar to the DIC, but the first term is based on the joint predictive density of the observations <span class="math inline">\(y_1, ..., y_n\)</span>.</p>
<p><span class="math display">\[WAIC = -2 \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{j = 1}^S p(y_i | \theta_{(j)}) \right) + 2 p_W\]</span> ,</p>
<p>where the penalty <span class="math inline">\(p_W\)</span> is the sum of variances of the log predictive density at each point:</p>
<p><span class="math display">\[p_W = \sum_{i=1}^n \text{Var}_j \left(\log p(y_i | \theta_{(j)}) \right)\]</span></p>
<p>Here, Var<span class="math inline">\(_j\)</span> denotes the variance of the expression in parentheses over the set of iterations <span class="math inline">\(j\)</span>.</p>
<p>The WAIC of a <em>brms</em> model can be calculated with the <code>waic</code> function.</p>
</div>
<div id="psis-loo" class="section level3">
<h3>PSIS-LOO</h3>
<p>A method recently developed by Vehtari et al. (2017) consists in estimating the predictive density at each point which would be obtained by leave-one-out cross-validation, that is to say by predicting <span class="math inline">\(y_i\)</span> from the model fitted to the data excluding <span class="math inline">\(i\)</span>, <span class="math inline">\(y_{-i}\)</span>.</p>
<p><span class="math inline">\(p(y_i | y_{-i}) = \int p(y_i | \theta) p(\theta | y_{-i}) \text{d}\theta\)</span> .</p>
<p>The PSIS-LOO method (PSIS = Patero smoothed importance sampling, LOO = leave-one-out) aims to estimate this quantity without performing cross-validation. Summarily, this approximation is obtained by averaging the <span class="math inline">\(p(y | \theta_{(j)})\)</span>, but with a special weighting of the <span class="math inline">\(\theta_{(j)}\)</span> (importance sampling). This weighting is then adjusted so that the extreme weights follow a theoretical model (Pareto distribution).</p>
<p>This method is implemented in the R package <em>loo</em> and can be called from the <code>loo</code> function applied to the result of a model in <em>brms</em>.</p>
<p>As we will see in the example below, the PSIS-LOO method produces its own diagnosis. The choice of the weights for each value <span class="math inline">\(y_i\)</span> is based on a parameter of the Pareto distribution <span class="math inline">\(k\)</span> and when <span class="math inline">\(k &gt; 0.7\)</span>, the approximation of <span class="math inline">\(p(y_i | y_{- i})\)</span> is potentially unstable. If this problem occurs for some observations, it is possible to refit the model by excluding these observations only in order to directly calculate <span class="math inline">\(p(y_i | y_{-i})\)</span>.</p>
<p>The result of this method is the estimated logarithm of the predictive density <span class="math inline">\(elpd_{loo}\)</span>, in other words the sum of <span class="math inline">\(\log p(y_i | y_{- i})\)</span>. An information criterion (LOOIC) similar to DIC and WAIC can be obtained by multiplying <span class="math inline">\(elpd_{loo}\)</span> by -2.</p>
</div>
<div id="comparison-of-methods" class="section level3">
<h3>Comparison of methods</h3>
<p>The PSIS-LOO method is a little more precise than the WAIC, especially for small samples, but the WAIC is generally faster to calculate.</p>
<p>Since they are based on Bayesian predictive density rather than on a single mean estimate of each parameter, these two methods (WAIC and PSIS-LOO) are currently preferred over DIC. However, both assume that the individual observations <span class="math inline">\(y_i\)</span> are independent of each other, conditional on the value of the parameters. Typically, this assumption is not respected if the model directly includes a correlation between different values of the response (e.g. temporal or spatial correlation).</p>
</div>
</div>
<div id="multi-model-predictions-1" class="section level2">
<h2>Multi-model predictions</h2>
<p>In the previous section, we saw that a multi-model prediction for a new observation <span class="math inline">\(\tilde{y}\)</span> is calculated by the weighted average of the predictions of the different models.</p>
<p><span class="math display">\[\tilde{y} = \sum_{j = 1}^m w_j \tilde{y_j}\]</span></p>
<p>As for the AIC, we can define weights according to the differences in IC between two models and this for different Bayesian criteria (e.g. WAIC, LOOIC).</p>
<p>However, it is not always optimal to combine the models in proportion to the evidence ratios. For example, the two best models can produce redundant predictions, while the third and fourth best models can help correct some of the worse models’ poor predictions.</p>
<p><strong>Model stacking</strong> consists in searching for the weights <span class="math inline">\(w_j\)</span> which minimize the multi-model prediction error given by the weighted average (Yao et al. 2018). This calculation can be done directly from the results of the PSIS-LOO method, as we will see in the example in the next section.</p>
</div>
</div>
<div id="comparison-of-models-with-loo-and-brms" class="section level1">
<h1>Comparison of models with <em>loo</em> and <em>brms</em></h1>
<p>The <a href="../donnees/rikz.csv">rikz.csv</a> dataset contains data on the richness of the benthic microfauna (<em>Richness</em>) for 45 sites spread over 5 beaches (<em>Beach</em>) in the Netherlands, depending on the vertical position of the site (<em>NAP</em>) and an exposure index measured at the beach level (<em>Exposure</em>).</p>
<pre class="r"><code>rikz &lt;- read.csv(&quot;../donnees/rikz.csv&quot;)
rikz$Exposure &lt;- as.factor(rikz$Exposure)
head(rikz)</code></pre>
<pre><code>##   Sample Richness Exposure    NAP Beach
## 1      1       11       10  0.045     1
## 2      2       10       10 -1.036     1
## 3      3       13       10 -1.336     1
## 4      4       11       10  0.616     1
## 5      5       10       10 -0.684     1
## 6      6        8        8  1.190     2</code></pre>
<p>Last week, we fit with <em>brms</em> a Poisson regression model for specific richness as a function of <em>NAP</em> and <em>Exposure</em>, with a random effect of the <em>Beach</em> on the intercept.</p>
<pre class="r"><code>library(brms)

rikz_prior &lt;- c(set_prior(&quot;normal(0, 1)&quot;, class = &quot;b&quot;),
                set_prior(&quot;normal(2, 1)&quot;, class = &quot;Intercept&quot;),
                set_prior(&quot;normal(0, 0.5)&quot;, class = &quot;sd&quot;))

mod1 &lt;- brm(Richness ~ NAP + Exposure + (1 | Beach), data = rikz, 
            family = poisson, prior = rikz_prior,
            control = list(adapt_delta = 0.99))</code></pre>
<p>We now consider a different version of the model where the effect of <em>NAP</em> also varies randomly between beaches.</p>
<pre class="r"><code>mod2 &lt;- brm(Richness ~ NAP + Exposure + (1 + NAP | Beach), data = rikz, 
            family = poisson, prior = rikz_prior,
            control = list(adapt_delta = 0.99))</code></pre>
<p>Here are the fixed effects and the standard deviation of the random effects estimated for the two models. In model 2, the uncertainty on <code>b_NAP</code> has increased and the random effect of the range on this coefficient has a standard deviation of 0.34 with a credibility interval of 0.06 to 0.70, comparable to the random effect of the beach on the intercept.</p>
<pre class="r"><code>posterior_summary(mod1, pars = &quot;b|sd&quot;)</code></pre>
<pre><code>##                       Estimate  Est.Error        Q2.5      Q97.5
## b_Intercept          2.3768063 0.29928962  1.65052195  2.9026726
## b_NAP               -0.5022979 0.07147339 -0.64245708 -0.3635854
## b_Exposure10        -0.4591405 0.32879974 -1.04058741  0.2932071
## b_Exposure11        -1.1593086 0.34394732 -1.76553502 -0.3930065
## sd_Beach__Intercept  0.2474170 0.13614456  0.02605894  0.5622590</code></pre>
<pre class="r"><code>posterior_summary(mod2, pars = &quot;b|sd&quot;)</code></pre>
<pre><code>##                       Estimate Est.Error        Q2.5      Q97.5
## b_Intercept          2.3730140 0.3342694  1.59135757  2.9490364
## b_NAP               -0.5770183 0.1542767 -0.89430239 -0.2779925
## b_Exposure10        -0.3921654 0.3734278 -1.07957393  0.3969519
## b_Exposure11        -1.1529676 0.3797490 -1.81743688 -0.2800094
## sd_Beach__Intercept  0.3036332 0.1558014  0.03965657  0.6790722
## sd_Beach__NAP        0.3494459 0.1563327  0.07779468  0.7054053</code></pre>
<div id="looic-computation" class="section level2">
<h2>LOOIC computation</h2>
<p>The <code>loo</code> function in <em>brms</em> compares different models according to the PSIS-LOO criterion.</p>
<pre class="r"><code>loo1 &lt;- loo(mod1, mod2, compare = TRUE)</code></pre>
<pre><code>## Warning: Found 1 observations with a pareto_k &gt; 0.7 in model &#39;mod1&#39;. It is
## recommended to set &#39;moment_match = TRUE&#39; in order to perform moment matching for
## problematic observations.</code></pre>
<pre><code>## Warning: Found 3 observations with a pareto_k &gt; 0.7 in model &#39;mod2&#39;. It is
## recommended to set &#39;moment_match = TRUE&#39; in order to perform moment matching for
## problematic observations.</code></pre>
<pre class="r"><code>loo1</code></pre>
<pre><code>## Output of model &#39;mod1&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -105.6  9.5
## p_loo        10.2  3.5
## looic       211.1 19.0
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     42    93.3%   917       
##  (0.5, 0.7]   (ok)        2     4.4%   127       
##    (0.7, 1]   (bad)       1     2.2%   157       
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## See help(&#39;pareto-k-diagnostic&#39;) for details.
## 
## Output of model &#39;mod2&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -101.9  7.2
## p_loo        13.5  3.7
## looic       203.9 14.5
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     33    73.3%   796       
##  (0.5, 0.7]   (ok)        9    20.0%   255       
##    (0.7, 1]   (bad)       3     6.7%   37        
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## See help(&#39;pareto-k-diagnostic&#39;) for details.
## 
## Model comparisons:
##      elpd_diff se_diff
## mod2  0.0       0.0   
## mod1 -3.6       3.4</code></pre>
<p>The result indicates that model 1 has a smaller log predictive density compared to model 2 (difference of 3.6), but the standard error of this difference (column 2) is 3.4. So we cannot be certain that model 2 is the best.</p>
<p>In addition, R warns us that for 4 observations (1 of model 1, 3 of model 2), the PSIS-LOO estimate is unstable with a $ k&gt; 0.7 $ in the Pareto distribution. This warning means that for these observations, the weights used for the approximation of the predictive density of cross validation have too many extreme values to estimate their variance. As suggested by the message, we re-evaluate the LOOIC with the argument <code>reloo = TRUE</code>, which will refit the model by omitting each of the problematic observations, to calculate the predictive density of cross validation directly.</p>
<pre class="r"><code>loo_corr &lt;- loo(mod1, mod2, compare = TRUE, reloo = TRUE)</code></pre>
<pre><code>## 1 problematic observation(s) found.
## The model will be refit 1 times.</code></pre>
<pre><code>## 
## Fitting model 1 out of 1 (leaving out observation 10)</code></pre>
<pre><code>## Start sampling</code></pre>
<pre><code>## 3 problematic observation(s) found.
## The model will be refit 3 times.</code></pre>
<pre><code>## 
## Fitting model 1 out of 3 (leaving out observation 10)</code></pre>
<pre><code>## 
## Fitting model 2 out of 3 (leaving out observation 22)</code></pre>
<pre><code>## 
## Fitting model 3 out of 3 (leaving out observation 38)</code></pre>
<pre><code>## Start sampling
## Start sampling
## Start sampling</code></pre>
<pre class="r"><code>loo_corr</code></pre>
<pre><code>## Output of model &#39;mod1&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -105.8  9.5
## p_loo        10.5  3.6
## looic       211.6 19.1
## ------
## Monte Carlo SE of elpd_loo is 0.1.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     43    95.6%   157       
##  (0.5, 0.7]   (ok)        2     4.4%   127       
##    (0.7, 1]   (bad)       0     0.0%   &lt;NA&gt;      
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## 
## All Pareto k estimates are ok (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.
## 
## Output of model &#39;mod2&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -103.1  7.7
## p_loo        14.7  4.3
## looic       206.3 15.5
## ------
## Monte Carlo SE of elpd_loo is 0.3.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     36    80.0%   37        
##  (0.5, 0.7]   (ok)        9    20.0%   255       
##    (0.7, 1]   (bad)       0     0.0%   &lt;NA&gt;      
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## 
## All Pareto k estimates are ok (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.
## 
## Model comparisons:
##      elpd_diff se_diff
## mod2  0.0       0.0   
## mod1 -2.7       3.0</code></pre>
<p>Here, the LOOIC value changed lightly compared with the previous case. As a comparison, WAIC produces a larger difference between the two models.</p>
<pre class="r"><code>waic(mod1, mod2, compare = TRUE)</code></pre>
<pre><code>## Warning: 
## 6 (13.3%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre><code>## Warning: 
## 9 (20.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre><code>## Output of model &#39;mod1&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -105.2  9.5
## p_waic         9.9  3.6
## waic         210.5 19.0
## 
## 6 (13.3%) p_waic estimates greater than 0.4. We recommend trying loo instead. 
## 
## Output of model &#39;mod2&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -100.5  6.9
## p_waic        12.0  3.3
## waic         201.0 13.9
## 
## 9 (20.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. 
## 
## Model comparisons:
##      elpd_diff se_diff
## mod2  0.0       0.0   
## mod1 -4.7       3.6</code></pre>
</div>
<div id="comparison-with-glmm" class="section level2">
<h2>Comparison with GLMM</h2>
<p>When we fitted these models with GLMMs in course 5, the AIC was lower for model 1, with a random effect on the intercept only. Why does the Bayesian method give a different result?</p>
<ul>
<li><p>First, the use of prior distributions constrains the values of the parameters so that a more complex model shows less overfitting.</p></li>
<li><p>Then, the AIC and the Bayesian criteria are based on different predictions. Suppose that two models differ by the inclusion or omission of a parameter <span class="math inline">\(\theta\)</span>. The AIC compares the predictions when this parameter is omitted, which implies for example <span class="math inline">\(\theta = 0\)</span>, with the predictions at the estimated maximum likelihood value $ $. In contrast, the Bayesian predictions of the model including <span class="math inline">\(\theta\)</span> are an average on the posterior distribution of <span class="math inline">\(\theta\)</span>, which will include values close to 0 if the posterior probability of that case is not negligible.</p></li>
</ul>
<p>For these two reasons, the maximum likelihood and Bayesian approach predictions differ except in very specific cases, e.g. a linear model where the prior distribution is of little importance and the posterior distribution for all parameters is symmetrical.</p>
</div>
<div id="model-stacking" class="section level2">
<h2>Model stacking</h2>
<p>The result of <code>loo</code> contains an element for each of the compared models. Each of these elements contains a <code>pointwise</code> matrix which presents among other statistics the estimated value of the log predictive density <span class="math inline">\(\log p(y_i | y_{- i})\)</span> for each point <span class="math inline">\(i\)</span> (<code>elpd_loo</code>) and the standard error of this estimate (<code>mcse_elpd_loo</code>).</p>
<pre class="r"><code>head (loo1$loos$mod1$pointwise)</code></pre>
<pre><code>##       elpd_loo mcse_elpd_loo      p_loo    looic influence_pareto_k
## [1,] -3.035422   0.010108673 0.21491808 6.070843          0.2736016
## [2,] -2.631103   0.009949967 0.17020507 5.262206          0.3098142
## [3,] -2.552925   0.008736143 0.12135383 5.105851          0.2586973
## [4,] -4.569708   0.021133985 0.65050744 9.139416          0.3687250
## [5,] -2.204221   0.003305800 0.02572645 4.408443          0.3002091
## [6,] -2.226006   0.005473939 0.06371198 4.452012          0.2461702</code></pre>
<p>If we wanted to combine the predictions of these two models, the <code>stacking_weights</code> function of the <em>loo</em> package allows us to determine the weights for the optimal superposition of the two models. This function requires a matrix with one column per model, corresponding to the <code>elpd_loo</code> column of the<code>pointwise</code> matrix mentioned above.</p>
<pre class="r"><code>library(loo)
stacking_weights(cbind(loo1$loos$mod1$pointwise[,1], loo1$loos$mod2$pointwise[,1]))</code></pre>
<pre><code>## Method: stacking
## ------
##        weight
## model1 0.037 
## model2 0.963</code></pre>
<pre class="r"><code>stacking_weights(cbind(loo_corr$loos$mod1$pointwise[,1], loo_corr$loos$mod2$pointwise[,1]))</code></pre>
<pre><code>## Method: stacking
## ------
##        weight
## model1 0.127 
## model2 0.873</code></pre>
<p>For the PSIS-LOO estimate with correction of problematic values, we see that a much greater weight is given to model 2. As mentioned above, since the predictions are based on the whole posterior distribution of <code>sd_Beach__NAP</code>, this includes cases where the standard deviation of this random effect approaches 0 and we therefore approach model 1.</p>
</div>
</div>
<div id="application-predator-prey-model" class="section level1">
<h1>Application: predator-prey model</h1>
<p>Rosenbaum et al. (2019) model the population dynamics of a predator-prey-resource system in a controlled environment (chemostat). The predator is a rotifer species (microscopic animal) and the prey is a unicellular alga whose growth depends on the nitrogen concentration (limiting resource).</p>
<div id="theoretical-model" class="section level2">
<h2>Theoretical model</h2>
<p>The authors assume that the nitrogen concentration <span class="math inline">\(S\)</span>, algae density <span class="math inline">\(A\)</span> and rotifer density <span class="math inline">\(R\)</span> vary over time according to the following equations:</p>
<p><span class="math display">\[\frac{dS}{dt} = \delta S^* - \frac{1}{c_A} \frac{f_A S}{h_A + S} A - \delta S\]</span></p>
<p>This equation indicates that the rate of change in nitrogen concentration depends on (1) nitrogen influx <span class="math inline">\(\delta S^*\)</span> (<span class="math inline">\(\delta\)</span> is the chemostat flow rate and <span class="math inline">\(S^*\)</span> is the concentration in the incoming solution); (2) nitrogen consumption by the algae (<span class="math inline">\(c_A\)</span> is the conversion factor, <span class="math inline">\(f_A\)</span> is the maximum growth rate and <span class="math inline">\(h_A\)</span> is the half-saturation point) and (3) the output flow <span class="math inline">\(\delta S\)</span>.</p>
<p><span class="math display">\[\frac{dA}{dt} = \frac{f_A S}{h_A + S} A - \frac{1}{c_R} \frac{f_R A}{h_R + A} R - \delta A\]</span></p>
<p>This equation indicates that the rate of change in algae density depends on (1) nitrogen consumption by the algae, (2) algae consumption by the rotifers (conversion factor <span class="math inline">\(c_R\)</span>, maximum growth rate <span class="math inline">\(f_R\)</span> and half-saturation point <span class="math inline">\(h_R\)</span>) and (3) output flow <span class="math inline">\(\delta A\)</span>.</p>
<p><span class="math display">\[\frac{dR}{dt} = \frac{f_R A}{h_R + A} R - \delta R\]</span></p>
<p>Finally, the rate of change in rotifer density depends on their algae consumption and output flow.</p>
</div>
<div id="data-and-statistical-model" class="section level2">
<h2>Data and statistical model</h2>
<p>The authors measured the daily concentration of algae and rotifers for about 20 days in 18 replicates of this experiment. In some replicates, the time series showed stable concentrations, while others showed cyclic dynamics.</p>
<p>For this experiment, the parameters <span class="math inline">\(\delta\)</span>, <span class="math inline">\(S^*\)</span>, <span class="math inline">\(h_A\)</span> and <span class="math inline">\(h_R\)</span> are known, whereas the maximum growth rates of the two organisms (<span class="math inline">\(f_A\)</span> and <span class="math inline">\(f_R\)</span>) and the conversion factors (<span class="math inline">\(c_A\)</span> and <span class="math inline">\(c_R\)</span>) must be estimated from the data. The following is a summary of the Stan model implementing this model:</p>
<ul>
<li><p>The logarithms of the parameters <span class="math inline">\(f_A\)</span>, <span class="math inline">\(f_R\)</span>, <span class="math inline">\(c_A\)</span> and <span class="math inline">\(c_R\)</span> vary randomly between the replicates, according to a normal distribution where the mean and standard deviation are parameters to be estimated.</p></li>
<li><p>The concentrations of each element of the system at the beginning of the experiment (<span class="math inline">\(S_0\)</span>, <span class="math inline">\(A_0\)</span>, and <span class="math inline">\(R_0\)</span>) are parameters to be estimated.</p></li>
<li><p>For a given value of the parameters, the above series of three equations is solved numerically to obtain the time series of <span class="math inline">\(S\)</span>, <span class="math inline">\(A\)</span> and <span class="math inline">\(R\)</span>. (Stan allows solving systems of differential equations in a model.)</p></li>
<li><p>The observations of <span class="math inline">\(A\)</span> and <span class="math inline">\(R\)</span> follow a log-normal distribution around their true value, with a standard deviation to be estimated.</p></li>
</ul>
<p>For all parameters to be estimated, the authors assigned prior distributions with weak constraints (<em>weakly informative prior</em>) based on past experiences with this type of system.</p>
</div>
<div id="simulations" class="section level2">
<h2>Simulations</h2>
<p>The authors carried out simulations from the full model, with parameters drawn from their prior distribution, in order to ensure that the model fitted to the results of these simulations is capable of recovering the true values of the parameters <span class="math inline">\(f_A\)</span>, <span class="math inline">\(f_R\)</span>, <span class="math inline">\(c_A\)</span> and <span class="math inline">\(c_R\)</span> (in other words, that the parameters are <em>identifiable</em>).</p>
<p>Figure 1 of this study, reproduced below, shows the posterior distribution of the estimation error of the parameters for different simulations. This test allowed the authors to determine that when the populations of the two species are stable rather than cyclical (top green results), three of the four parameters are difficult to estimate precisely.</p>
<p><img src="../images/Rosenbaum2019_Fig1.jpg" /> <em>Source: Rosenbaum et al. 2019, Fig.1</em></p>
</div>
<div id="fitting-the-model" class="section level2">
<h2>Fitting the model</h2>
<p>From the model fitted to the observations, the authors produced hindcast estimates of the <span class="math inline">\(S\)</span>, <span class="math inline">\(A\)</span>, and <span class="math inline">\(R\)</span> concentrations on each day for each of the replicates (Figure 3, reproduced below, with the points corresponding to the observed values of <span class="math inline">\(A\)</span> and <span class="math inline">\(R\)</span>). Since the nitrogen concentration <span class="math inline">\(S\)</span> (top row in blue) was not measured, the accuracy of its estimation depends on the accuracy of the model parameters, and is therefore less accurate when populations are stable rather than cyclic (top row).</p>
<p><img src="../images/Rosenbaum2019_Fig3.jpg" /> <em>Source: Rosenbaum et al. 2019, Fig.3.</em></p>
<p>By comparing the dynamics estimated by the model to the observation points, we can see that the model follows more precisely the observations of the algae concentrations (middle in green) than those of the rotifers (bottom in red). One of the possible reasons given by the authors to explain these results is that the growth of the algal population depends on the unmeasured nitrogen concentration, so the model has the flexibility to adjust this concentration to reproduce the algal dynamics, which is not the case for rotifers whose growth depends on the algal population.</p>
</div>
</div>
<div id="application-seed-and-seedling-dispersal" class="section level1">
<h1>Application: seed and seedling dispersal</h1>
<p>This example presents a study I conducted with collaborators to estimate seed and seedling dispersal curves as a function of distance for different species in the forest of Barro Colorado Island in Panama (Marchand et al. 2020).</p>
<div id="model-and-data" class="section level2">
<h2>Model and data</h2>
<p>The study site is a 50 ha (1 km x 0.5 km) plot of the Smithsonian Tropical Research Institute (STRI) where all trees with a DBH &gt;1 cm have been mapped and measured every 5 years since 1985. STRI researchers have been conducting annual seed collection and seed identification in 500 mesh traps scattered throughout this plot, as well as counting new seedlings in plots around these traps.</p>
<p>Seed dispersal is modelled separately for each of the main tree species. The number of seeds found in trap <span class="math inline">\(j\)</span> during year <span class="math inline">\(t\)</span> follows a negative binomial distribution with a mean <span class="math inline">\(\mu_{jt}\)</span> given by the following equation:</p>
<p><span class="math display">\[\mu_{jt} = a \sum_i Q(b_{it}) F(r_{ij})\]</span></p>
<p>For each tree <span class="math inline">\(i\)</span> of that species, the seed production <span class="math inline">\(Q\)</span> is multiplied by the dispersal kernel <span class="math inline">\(F\)</span>, which gives the probability that a seed will fall within an area of one square meter located at a distance <span class="math inline">\(r_{ij}\)</span>. We then sum up the contributions of all the trees and multiply by the trap area <span class="math inline">\(a\)</span>.</p>
<p>The seed production of a tree is assumed to be proportional to its basal area <span class="math inline">\(b\)</span>.</p>
<p><span class="math display">\[Q(b_{it}) = e^{\beta_t} b_{it}\]</span></p>
<p>The logarithm of the proportionality constant, <span class="math inline">\(\beta_t\)</span>, varies from year to year according to a normal distribution with two parameters to be estimated.</p>
<p><span class="math display">\[\beta_t \sim \text{N}(\mu_{\beta}, \sigma_{\beta})\]</span></p>
<p>The same model can be used to estimate the dispersal of seedlings in relation to potential parents.</p>
<p>Our main objective was to determine the shape of the dispersal kernel <span class="math inline">\(F\)</span> for seeds and seedlings of different species, as well as the relative germination success (ratio between the number of seedlings and the number of seeds) as a function of the distance from the parent.</p>
<p>The figure below shows the different dispersal kernels we considered. Each one contains two parameters to be fit from the data.</p>
<p><img src="../images/Marchand2020_disp_EN.png" /></p>
<p>The problem of estimating dispersal kernels poses two challenges for which the Bayesian approach offers interesting solutions:</p>
<ul>
<li><p>Seed dispersal follows a leptokurtic pattern, i.e. most seeds fall close to the parent, but a small proportion can reach extreme distances (several kilometers) under the action of wind and animals. However, our data do not allow us to observe dispersal over long or very short distances. In this situation, an unconstrained fit to the data may lead to an unrealistic dispersal function (e.g. median dispersal distance &lt; 10 cm and mean &gt; 10 km).</p></li>
<li><p>It is likely that none of the dispersal kernels presented above is entirely appropriate. In this case, we would like to estimate dispersal characteristics (mean dispersal distance of seeds and seedlings, relative seed survival as a function of distance) taking into account the uncertainty on the shape of the dispersal kernel.</p></li>
</ul>
<p>To solve the second problem, we estimated the parameters of each of the five dispersal kernels for each species and used model stacking to obtain multi-model predictions of the dispersal characteristics of interest. Before the model stacking step, we used posterior predictive checks to eliminate models that were very poorly fitted to the data: for example, those that did not give plausible values for the total number of seeds observed in the traps, or for the number of zeros observed.</p>
<p>To solve the first problem, we chose prior distributions for the parameters of each dispersal kernel, so that the plausible values of the median and mean dispersal distances are of the order of 1 m to 1 km; this does not mean that the posterior distribution is confined to this range, but that these more plausible values are favored unless the data strongly supports more extreme values. We also chose prior distributions based on biological plausibility for seed production parameters.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>The figure below shows an example of the dispersal kernels and relative seed survival rates that we estimated from the model stacking. In both cases, the seedlings are on average further away from the parent than the seeds. For the species on the left, the germination rate increases continuously between 1 and 100 m from the parent, while for the species on the right, it seems to reach a plateau between 10 and 100 m.</p>
<p><img src="../images/Marchand2020_res_EN.png" /></p>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Marchand, P., Comita, L. S., Wright, S. J., Condit, R., Hubbell, S. P., &amp; Beckman, N. G. (2020). Seed-to-seedling transitions exhibit distance-dependent mortality but no strong spacing effects in a Neotropical forest. <em>Ecology</em>, 101(2), e02926. <a href="doi:10.1002/ecy.2926" class="uri">doi:10.1002/ecy.2926</a>.</p>
<p>Rosenbaum, B., Raatz, M., Weithoff, G., Fussmann, G.F. and Gaedke, U. (2019) Estimating parameters from multiple time series of population dynamics using bayesian inference. <em>Frontiers in Ecology and Evolution</em> 6, 234. doi: 10.3389/fevo.2018.00234.</p>
<p>Vehtari, A., Gelman, A. and Gabry, J. (2017) Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. <em>Statistics and Computing</em> 27(5), 1413–1432. <a href="doi:10.1007/s11222-016-9696-4" class="uri">doi:10.1007/s11222-016-9696-4</a>.</p>
<p>Yao, Y., Vehtari, A., Simpson, D. and Gelman, A. (2018) Using stacking to average Bayesian predictive distributions. Bayesian Analysis 13(3), 917–1007. <a href="doi:10.1214/17-BA1091" class="uri">doi:10.1214/17-BA1091</a>.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
