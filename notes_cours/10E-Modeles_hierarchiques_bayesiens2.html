<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Hierarchical Bayesian models, part 2</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Hierarchical Bayesian models, part 2</h1>

</div>


<div id="contents" class="section level1">
<h1>Contents</h1>
<ul>
<li><p>Review: Model comparison and selection</p></li>
<li><p>Bayesian approach to model comparison</p></li>
<li><p>Comparing models with <em>loo</em> and <em>brms</em></p></li>
</ul>
</div>
<div id="model-comparison-and-selection" class="section level1">
<h1>Model comparison and selection</h1>
<p>Suppose we have different statistical models that aim to explain the same data. Models could include different predictors, different distributions of the response, etc. How to determine which model best represents the phenomenon studied?</p>
<p>Most model comparison methods seek to optimize the model’s ability to <strong>predict new observations</strong> of the phenomenon. In other words, it is not enough to assess whether the model approaches the data used to fit it. A more complex model, with more adjustable parameters, will always be closer to this data.</p>
<p>In general, an overly simple model has a large systematic error (bias or underfitting), because it omits significant effects on the response variable; an overly complex model has a large random error (variance or overfitting), because it tends to represent “accidental” associations of a particular sample that do not generalize to the population. The ideal compromise between these two types of error, which minimizes the total error, depends on the amount of data, since a large sample decreases the variance associated with the estimation of many parameters in a complex model.</p>
<p><img src="../images/biais_variance_en.png" /></p>
<p>With large data sets, it is possible to set aside part of the data (often ~20 to 30%) to create a validation set, while the rest of the data form the training set. In this case, each of the candidate models is fit on the training data and the predictive performance of the fitted models is evaluated on the validation set.</p>
<div id="cross-validation" class="section level2">
<h2>Cross-validation</h2>
<p>Setting aside part of the data for validation is not practical if the sample size is small. With relatively little data, each point is important for accurately estimating the parameters of the model; also, the smaller the validation set, the more likely it is to be non-representative of the population.</p>
<p>Cross-validation provides a way to assess predictive performance on new observations without having to set aside a validation set. This method consists in randomly dividing the observations into groups and measuring the quality of the prediction of the observations of a group according to a model fitted to the rest of the observations.</p>
<p>For example, if each group has only one observation (leave-one-out cross-validation), we can evaluate the prediction of each value of the response <span class="math inline">\(y_i\)</span> for a model fitted without observation <span class="math inline">\(i\)</span>. However, this method requires refitting the model <span class="math inline">\(n\)</span> times, where <span class="math inline">\(n\)</span> is the number of observations.</p>
<p>If the number of observations is large, it may be more practical to divide the observations into <span class="math inline">\(k\)</span> groups (k-fold cross-validation), for example <span class="math inline">\(k\)</span> = 10, and to fit each of the models to be compared <span class="math inline">\(k\)</span> times, leaving a different <span class="math inline">\(1/k\)</span> fraction of the observations aside each time.</p>
</div>
<div id="akaike-information-criterion" class="section level2">
<h2>Akaike information criterion</h2>
<p>Since cross-validation methods are costly in terms of calculation, it is useful to be able to approximate the prediction error that would be obtained in cross-validation without having to refit the model several times.</p>
<p>For models fitted by the maximum likelihood method, the Akaike information criterion (AIC) offers a measure of fit based on information theory, which tends to produce the same result as leave-one-out cross-validation if the sample size is large enough. The AIC is calculated as follows:</p>
<p><span class="math display">\[ AIC = -2 \log L + 2 K \]</span></p>
<p>where <span class="math inline">\(L\)</span> is the likelihood function at its maximum and <span class="math inline">\(K\)</span> is the number of parameters estimated by the model. A small AIC value represents a better predictive power of the model. The first term in the equation represents the fit to the observed data, while the second term penalizes more complex models.</p>
<p>The AIC is defined to the nearest additive constant, so its absolute value gives no information. Rather, it is the difference of AIC between the candidate models which is interpretable. This difference is defined with respect to the minimum value of the AIC among the compared models: <span class="math inline">\(\Delta AIC = AIC - \min AIC\)</span>. The best model has a <span class="math inline">\(\Delta AIC = 0\)</span>.</p>
<p>The expression:</p>
<p><span class="math display">\[ e^{-\frac{\Delta AIC}{2} } \]</span></p>
<p>corresponds to the evidence ratio of each model vs. the one with the minimum AIC. For example, <span class="math inline">\(\Delta AIC = 2\)</span> corresponds to a ratio of ~0.37 (~3 times less likely), while <span class="math inline">\(\Delta AIC = 10\)</span> corresponds to a ratio of ~0.0067 (~150 times less likely).</p>
</div>
<div id="multi-model-predictions" class="section level2">
<h2>Multi-model predictions</h2>
<p>With <span class="math inline">\(m\)</span> candidate models, we can use the evidence ratios described above to define the Akaike weight <span class="math inline">\(w\)</span> for each model:</p>
<p><span class="math display">\[w_i = \frac{e^{\frac{-\Delta AIC_i}{2}}}{\sum_{j=1}^{m} e^{\frac{-\Delta AIC_j}{2}}}\]</span></p>
<p>The denominator normalizes each ratio by their sum, so that the sum of the weights <span class="math inline">\(w_i\)</span> equals 1.</p>
<p>If several models are plausible and have a significant Akaike weight, then it is possible to average their predictions for a new observation of the response (this prediction is noted <span class="math inline">\(\tilde{y}\)</span>), by weighting the prediction <span class="math inline">\(\tilde{y_j}\)</span> of each candidate model by its weight <span class="math inline">\(w_j\)</span>.</p>
<p><span class="math display">\[\tilde{y} = \sum_{j = 1}^m w_j \tilde{y_j}\]</span></p>
<p>Multi-model predictions are often more accurate than those obtained by considering only the best model, because they take into account the uncertainty about the form of the model.</p>
</div>
</div>
<div id="bayesian-approach-for-model-comparison" class="section level1">
<h1>Bayesian approach for model comparison</h1>
<div id="predictive-density" class="section level2">
<h2>Predictive density</h2>
<p>For a model estimated by maximum likelihood, the predictions of new observations are obtained by fixing the parameters of the model at their estimated value. The likelihood of this new observation is therefore <span class="math inline">\(p(\tilde {y} | \hat{\ theta})\)</span>, where <span class="math inline">\(\hat{\theta}\)</span> are the maximum likelihood parameter estimates.</p>
<p>In a Bayesian approach, the predictions of new observations are obtained by averaging the predictions over the posterior distribution of the parameters. The predictive density of <span class="math inline">\(\tilde{y}\)</span> conditional on the model fitted to observations <span class="math inline">\(y\)</span>, denoted <span class="math inline">\(p(\tilde{y} | y)\)</span>, is equal to the mean of the likelihood <span class="math inline">\(p(\tilde{y} | \theta)\)</span> for the joint posterior distribution of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math inline">\(p(\tilde{y} | y) = \int p(\tilde{y} | \theta) p(\theta | y) \text{d}\theta\)</span> .</p>
<p>In practice, if a Monte-Carlo method generates <span class="math inline">\(S\)</span> parameter vectors <span class="math inline">\(\theta_{(1)}, ..., \theta_{(S)}\)</span> that approximate the joint posterior distribution, we calculate <span class="math inline">\(p(\tilde{y} | y)\)</span> by averaging the predictions of each vector:</p>
<p><span class="math inline">\(p(\tilde{y} | y) = \frac{1}{S} \sum_{j = 1}^S p(\tilde{y} | \theta_{(j)})\)</span> .</p>
<p>As with likelihood, it is easier to work with the logarithm of the predictive density.</p>
</div>
<div id="cross-validation-1" class="section level2">
<h2>Cross-validation</h2>
<p>To determine the model that maximizes the predictive density of new observations, as defined above, we can use cross-validation. However, since fitting hierarchical Bayesian models sometimes requires considerable computing time, it is not practical in these cases to repeat the estimation of the model a large number of times, leaving aside some of the data. We therefore most often use criteria which approximate the predictive performance of a cross-validation.</p>
<p>If the AIC approximates the cross-validation error for the models fit by maximum likelihood with a fairly large number of observations, this criterion does not apply well to Bayesian models. On the one hand, the parameter values maximizing the likelihood are not directly found when fitting Bayesian models. In addition, it is difficult to define a number of parameters <span class="math inline">\(K\)</span> because of the hierarchical structure and the constraints imposed by the prior distributions of the parameters, which result in those parameters not being completely “free”.</p>
</div>
<div id="selection-criteria-for-hierarchical-bayesian-models" class="section level2">
<h2>Selection criteria for hierarchical Bayesian models</h2>
<div id="dic" class="section level3">
<h3>DIC</h3>
<p>The Deviance Information Criterion (DIC), based on AIC, was one of the first criteria developed for the comparison of Bayesian models:</p>
<p><span class="math display">\[DIC = -2 \log p(y | \bar{\theta}) + 2 p_D\]</span></p>
<p>where <span class="math inline">\(\bar{\theta}\)</span> is the mean of the posterior distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p_D\)</span> is the effective number of parameters, which can be calculated in several ways.</p>
<p>Like the AIC, the DIC represents well the relative predictive performance of models on new data, if the sample size is large enough. However, this is not a Bayesian prediction because it is based on a single estimate of each parameter (its mean value) rather than on the entire posterior distribution.</p>
</div>
<div id="waic" class="section level3">
<h3>WAIC</h3>
<p>The Watanabe-Akaike information criterion (WAIC) is similar to the DIC, but the first term is based on the joint predictive density of the observations <span class="math inline">\(y_1, ..., y_n\)</span>.</p>
<p><span class="math display">\[WAIC = -2 \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{j = 1}^S p(y_i | \theta_{(j)}) \right) + 2 p_W\]</span> ,</p>
<p>where the penalty <span class="math inline">\(p_W\)</span> is the sum of variances of the log predictive density at each point:</p>
<p><span class="math display">\[p_W = \sum_{i=1}^n \text{Var}_j \left(\log p(y_i | \theta_{(j)}) \right)\]</span></p>
<p>Here, Var<span class="math inline">\(_j\)</span> denotes the variance of the expression in parentheses over the set of iterations <span class="math inline">\(j\)</span>.</p>
<p>The WAIC of a <em>brms</em> model can be calculated with the <code>waic</code> function.</p>
</div>
<div id="psis-loo" class="section level3">
<h3>PSIS-LOO</h3>
<p>A method recently developed by Vehtari et al. (2017) consists in estimating the predictive density at each point which would be obtained by leave-one-out cross-validation, that is to say by predicting <span class="math inline">\(y_i\)</span> from the model fitted to the data excluding <span class="math inline">\(i\)</span>, <span class="math inline">\(y_{-i}\)</span>.</p>
<p><span class="math inline">\(p(y_i | y_{-i}) = \int p(y_i | \theta) p(\theta | y_{-i}) \text{d}\theta\)</span> .</p>
<p>The PSIS-LOO method (PSIS = Patero smoothed importance sampling, LOO = leave-one-out) aims to estimate this quantity without performing cross-validation. Summarily, this approximation is obtained by averaging the <span class="math inline">\(p(y | \theta_{(j)})\)</span>, but with a special weighting of the <span class="math inline">\(\theta_{(j)}\)</span> (importance sampling). This weighting is then adjusted so that the extreme weights follow a theoretical model (Pareto distribution).</p>
<p>This method is implemented in the R package <em>loo</em> and can be called from the <code>loo</code> function applied to the result of a model in <em>brms</em>.</p>
<p>As we will see in the example below, the PSIS-LOO method produces its own diagnosis. The choice of the weights for each value <span class="math inline">\(y_i\)</span> is based on a parameter of the Pareto distribution <span class="math inline">\(k\)</span> and when <span class="math inline">\(k &gt; 0.7\)</span>, the approximation of <span class="math inline">\(p(y_i | y_{- i})\)</span> is potentially unstable. If this problem occurs for some observations, it is possible to refit the model by excluding these observations only in order to directly calculate <span class="math inline">\(p(y_i | y_{-i})\)</span>.</p>
<p>The result of this method is the estimated logarithm of the predictive density <span class="math inline">\(elpd_{loo}\)</span>, in other words the sum of <span class="math inline">\(\log p(y_i | y_{- i})\)</span>. An information criterion (LOOIC) similar to DIC and WAIC can be obtained by multiplying <span class="math inline">\(elpd_{loo}\)</span> by -2.</p>
</div>
<div id="comparison-of-methods" class="section level3">
<h3>Comparison of methods</h3>
<p>The PSIS-LOO method is a little more precise than the WAIC, especially for small samples, but the WAIC is generally faster to calculate.</p>
<p>Since they are based on Bayesian predictive density rather than on a single mean estimate of each parameter, these two methods (WAIC and PSIS-LOO) are currently preferred over DIC. However, both assume that the individual observations <span class="math inline">\(y_i\)</span> are independent of each other, conditional on the value of the parameters. Typically, this assumption is not respected if the model directly includes a correlation between different values of the response (e.g. temporal or spatial correlation).</p>
</div>
</div>
<div id="multi-model-predictions-1" class="section level2">
<h2>Multi-model predictions</h2>
<p>In the previous section, we saw that a multi-model prediction for a new observation <span class="math inline">\(\tilde{y}\)</span> is calculated by the weighted average of the predictions of the different models.</p>
<p><span class="math display">\[\tilde{y} = \sum_{j = 1}^m w_j \tilde{y_j}\]</span></p>
<p>As for the AIC, we can define weights according to the differences in IC between two models and this for different Bayesian criteria (e.g. WAIC, LOOIC).</p>
<p>However, it is not always optimal to combine the models in proportion to the evidence ratios. For example, the two best models can produce redundant predictions, while the third and fourth best models can help correct some of the worse models’ poor predictions.</p>
<p><strong>Model stacking</strong> consists in searching for the weights <span class="math inline">\(w_j\)</span> which minimize the multi-model prediction error given by the weighted average (Yao et al. 2018). This calculation can be done directly from the results of the PSIS-LOO method, as we will see in the example in the next section.</p>
</div>
</div>
<div id="comparison-of-models-with-loo-and-brms" class="section level1">
<h1>Comparison of models with <em>loo</em> and <em>brms</em></h1>
<p>The <a href="../donnees/rikz.csv">rikz.csv</a> dataset contains data on the richness of the benthic microfauna (<em>Richness</em>) for 45 sites spread over 5 beaches (<em>Beach</em>) in the Netherlands, depending on the vertical position of the site (<em>NAP</em>) and an exposure index measured at the beach level (<em>Exposure</em>).</p>
<pre class="r"><code>rikz &lt;- read.csv(&quot;../donnees/rikz.csv&quot;)
rikz$Exposure &lt;- as.factor(rikz$Exposure)
head(rikz)</code></pre>
<pre><code>##   Sample Richness Exposure    NAP Beach
## 1      1       11       10  0.045     1
## 2      2       10       10 -1.036     1
## 3      3       13       10 -1.336     1
## 4      4       11       10  0.616     1
## 5      5       10       10 -0.684     1
## 6      6        8        8  1.190     2</code></pre>
<p>Last week, we fit with <em>brms</em> a Poisson regression model for specific richness as a function of <em>NAP</em> and <em>Exposure</em>, with a random effect of the <em>Beach</em> on the intercept.</p>
<pre class="r"><code>library(brms)

rikz_prior &lt;- c(set_prior(&quot;normal(0, 1)&quot;, class = &quot;b&quot;),
                set_prior(&quot;normal(2, 1)&quot;, class = &quot;Intercept&quot;),
                set_prior(&quot;normal(0, 0.5)&quot;, class = &quot;sd&quot;))

mod1 &lt;- brm(Richness ~ NAP + Exposure + (1 | Beach), data = rikz, 
            family = poisson, prior = rikz_prior,
            control = list(adapt_delta = 0.99))</code></pre>
<p>We now consider a different version of the model where the effect of <em>NAP</em> also varies randomly between beaches.</p>
<pre class="r"><code>mod2 &lt;- brm(Richness ~ NAP + Exposure + (1 + NAP | Beach), data = rikz, 
            family = poisson, prior = rikz_prior,
            control = list(adapt_delta = 0.99))</code></pre>
<p>Here are the fixed effects and the standard deviation of the random effects estimated for the two models. In model 2, the uncertainty on <code>b_NAP</code> has increased and the random effect of the range on this coefficient has a standard deviation of 0.34 with a credibility interval of 0.06 to 0.70, comparable to the random effect of the beach on the intercept.</p>
<pre class="r"><code>posterior_summary(mod1, pars = &quot;b|sd&quot;)</code></pre>
<pre><code>##                       Estimate  Est.Error        Q2.5      Q97.5
## b_Intercept          2.3946098 0.27786240  1.76758655  2.8940389
## b_NAP               -0.5019450 0.07335376 -0.64929008 -0.3582454
## b_Exposure10        -0.4700039 0.31063290 -1.02921969  0.2472092
## b_Exposure11        -1.1769722 0.32001645 -1.75949837 -0.4851790
## sd_Beach__Intercept  0.2331968 0.13449523  0.02419773  0.5318149</code></pre>
<pre class="r"><code>posterior_summary(mod2, pars = &quot;b|sd&quot;)</code></pre>
<pre><code>##                       Estimate Est.Error        Q2.5      Q97.5
## b_Intercept          2.3675129 0.3186264  1.65983871  2.9289606
## b_NAP               -0.5803622 0.1561123 -0.90593248 -0.2913867
## b_Exposure10        -0.3845888 0.3643043 -1.06348562  0.4073055
## b_Exposure11        -1.1553513 0.3677035 -1.83032283 -0.3422517
## sd_Beach__Intercept  0.3022290 0.1494390  0.05319863  0.6571213
## sd_Beach__NAP        0.3439203 0.1557883  0.06782613  0.6868742</code></pre>
<div id="looic-computation" class="section level2">
<h2>LOOIC computation</h2>
<p>The <code>loo</code> function in <em>brms</em> compares different models according to the PSIS-LOO criterion.</p>
<pre class="r"><code>loo1 &lt;- loo(mod1, mod2, compare = TRUE)</code></pre>
<pre><code>## Warning: Found 2 observations with a pareto_k &gt; 0.7 in model &#39;mod1&#39;. It is
## recommended to set &#39;reloo = TRUE&#39; in order to calculate the ELPD without the
## assumption that these observations are negligible. This will refit the model 2
## times to compute the ELPDs for the problematic observations directly.</code></pre>
<pre><code>## Warning: Found 4 observations with a pareto_k &gt; 0.7 in model &#39;mod2&#39;. It is
## recommended to set &#39;reloo = TRUE&#39; in order to calculate the ELPD without the
## assumption that these observations are negligible. This will refit the model 4
## times to compute the ELPDs for the problematic observations directly.</code></pre>
<pre class="r"><code>loo1</code></pre>
<pre><code>## Output of model &#39;mod1&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -106.3  9.8
## p_loo        10.9  3.9
## looic       212.5 19.5
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     41    91.1%   1256      
##  (0.5, 0.7]   (ok)        2     4.4%   309       
##    (0.7, 1]   (bad)       2     4.4%   67        
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## See help(&#39;pareto-k-diagnostic&#39;) for details.
## 
## Output of model &#39;mod2&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -102.4  7.5
## p_loo        13.9  3.9
## looic       204.8 14.9
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     34    75.6%   801       
##  (0.5, 0.7]   (ok)        7    15.6%   366       
##    (0.7, 1]   (bad)       3     6.7%   25        
##    (1, Inf)   (very bad)  1     2.2%   28        
## See help(&#39;pareto-k-diagnostic&#39;) for details.
## 
## Model comparisons:
##      elpd_diff se_diff
## mod2  0.0       0.0   
## mod1 -3.8       3.4</code></pre>
<p>The result indicates that model 1 has a higher LOOIC of 6.42 compared to model 2, which seems a big difference, except that the standard error of this difference (column 2) is 5.92. So we cannot be certain that model 2 is the best.</p>
<p>In addition, R warns us that for 3 observations (1 of model 1, 2 of model 2), the PSIS-LOO estimate is unstable with a $ k&gt; 0.7 $ in the Pareto distribution. This warning means that for these observations, the weights used for the approximation of the predictive density of cross validation have too many extreme values to estimate their variance. As suggested by the message, we re-evaluate the LOOIC with the argument <code>reloo = TRUE</code>, which will refit the model by omitting each of the problematic observations, to calculate the predictive density of cross validation directly.</p>
<pre class="r"><code>loo_corr &lt;- loo(mod1, mod2, compare = TRUE, reloo = TRUE)</code></pre>
<pre><code>## 2 problematic observation(s) found.
## The model will be refit 2 times.</code></pre>
<pre><code>## 
## Fitting model 1 out of 2 (leaving out observation 10)</code></pre>
<pre><code>## 
## Fitting model 2 out of 2 (leaving out observation 22)</code></pre>
<pre><code>## Start sampling
## Start sampling</code></pre>
<pre><code>## 4 problematic observation(s) found.
## The model will be refit 4 times.</code></pre>
<pre><code>## 
## Fitting model 1 out of 4 (leaving out observation 10)</code></pre>
<pre><code>## 
## Fitting model 2 out of 4 (leaving out observation 22)</code></pre>
<pre><code>## 
## Fitting model 3 out of 4 (leaving out observation 38)</code></pre>
<pre><code>## 
## Fitting model 4 out of 4 (leaving out observation 42)</code></pre>
<pre><code>## Start sampling
## Start sampling
## Start sampling
## Start sampling</code></pre>
<pre class="r"><code>loo_corr</code></pre>
<pre><code>## Output of model &#39;mod1&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -106.6  9.9
## p_loo        11.3  4.1
## looic       213.1 19.9
## ------
## Monte Carlo SE of elpd_loo is 0.2.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     43    95.6%   67        
##  (0.5, 0.7]   (ok)        2     4.4%   309       
##    (0.7, 1]   (bad)       0     0.0%   &lt;NA&gt;      
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## 
## All Pareto k estimates are ok (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.
## 
## Output of model &#39;mod2&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -103.0  7.7
## p_loo        14.5  4.2
## looic       206.0 15.3
## ------
## Monte Carlo SE of elpd_loo is 0.3.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     38    84.4%   25        
##  (0.5, 0.7]   (ok)        7    15.6%   366       
##    (0.7, 1]   (bad)       0     0.0%   &lt;NA&gt;      
##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;      
## 
## All Pareto k estimates are ok (k &lt; 0.7).
## See help(&#39;pareto-k-diagnostic&#39;) for details.
## 
## Model comparisons:
##      elpd_diff se_diff
## mod2  0.0       0.0   
## mod1 -3.6       3.4</code></pre>
<p>Here, the LOOIC value changed lightly compared with the previous case. As a comparison, WAIC produces a larger difference between the two models.</p>
<pre class="r"><code>waic(mod1, mod2, compare = TRUE)</code></pre>
<pre><code>## Output of model &#39;mod1&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -105.9  9.8
## p_waic        10.6  3.9
## waic         211.7 19.6
## 
## 6 (13.3%) p_waic estimates greater than 0.4. We recommend trying loo instead. 
## 
## Output of model &#39;mod2&#39;:
## 
## Computed from 4000 by 45 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -100.7  7.0
## p_waic        12.2  3.4
## waic         201.5 14.0
## 
## 9 (20.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. 
## 
## Model comparisons:
##      elpd_diff se_diff
## mod2  0.0       0.0   
## mod1 -5.1       3.8</code></pre>
</div>
<div id="comparison-with-glmm" class="section level2">
<h2>Comparison with GLMM</h2>
<p>When we fitted these models with GLMMs in course 5, the AIC was lower for model 1, with a random effect on the intercept only. Why does the Bayesian method give a different result?</p>
<ul>
<li><p>First, the use of prior distributions constrains the values of the parameters so that a more complex model shows less overfitting.</p></li>
<li><p>Then, the AIC and the Bayesian criteria are based on different predictions. Suppose that two models differ by the inclusion or omission of a parameter <span class="math inline">\(\theta\)</span>. The AIC compares the predictions when this parameter is omitted, which implies for example <span class="math inline">\(\theta = 0\)</span>, with the predictions at the estimated maximum likelihood value $ $. In contrast, the Bayesian predictions of the model including <span class="math inline">\(\theta\)</span> are an average on the posterior distribution of <span class="math inline">\(\theta\)</span>, which will include values close to 0 if the posterior probability of that case is not negligible.</p></li>
</ul>
<p>For these two reasons, the maximum likelihood and Bayesian approach predictions differ except in very specific cases, e.g. a linear model where the prior distribution is of little importance and the posterior distribution for all parameters is symmetrical.</p>
</div>
<div id="model-stacking" class="section level2">
<h2>Model stacking</h2>
<p>The result of <code>loo</code> contains an element for each of the compared models. Each of these elements contains a <code>pointwise</code> matrix which presents among other statistics the estimated value of the log predictive density <span class="math inline">\(\log p(y_i | y_{- i})\)</span> for each point <span class="math inline">\(i\)</span> (<code>elpd_loo</code>) and the standard error of this estimate (<code>mcse_elpd_loo</code>).</p>
<pre class="r"><code>head (loo1$loos$mod1$pointwise)</code></pre>
<pre><code>##       elpd_loo mcse_elpd_loo      p_loo    looic
## [1,] -3.081013   0.011313030 0.24225057 6.162027
## [2,] -2.635261   0.010944615 0.18551945 5.270522
## [3,] -2.568526   0.009998346 0.13673731 5.137052
## [4,] -4.631419   0.021687926 0.69405906 9.262838
## [5,] -2.210039   0.003636875 0.02775377 4.420077
## [6,] -2.225742   0.006137787 0.06883449 4.451484</code></pre>
<p>If we wanted to combine the predictions of these two models, the <code>stacking_weights</code> function of the <em>loo</em> package allows us to determine the weights for the optimal superposition of the two models. This function requires a matrix with one column per model, corresponding to the <code>elpd_loo</code> column of the<code>pointwise</code> matrix mentioned above.</p>
<pre class="r"><code>library(loo)
stacking_weights(cbind(loo1$loos$mod1$pointwise[,1], loo1$loos$mod2$pointwise[,1]))</code></pre>
<pre><code>## Method: stacking
## ------
##        weight
## model1 0.000 
## model2 1.000</code></pre>
<pre class="r"><code>stacking_weights(cbind(loo_corr$loos$mod1$pointwise[,1], loo_corr$loos$mod2$pointwise[,1]))</code></pre>
<pre><code>## Method: stacking
## ------
##        weight
## model1 0.034 
## model2 0.966</code></pre>
<p>For the PSIS-LOO estimate with correction of problematic values, we see that almost all the weight is given to model 2, so it is probably enough to make predictions with the more complex model. As mentioned above, since the predictions are based on the whole posterior distribution of <code>sd_Beach__NAP</code>, this includes cases where the standard deviation of this random effect approaches 0 and we therefore approach model 1.</p>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Vehtari, A., Gelman, A. et Gabry, J. (2017) Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. <em>Statistics and Computing</em> 27(5), 1413–1432. <a href="doi:10.1007/s11222-016-9696-4" class="uri">doi:10.1007/s11222-016-9696-4</a>.</p>
<p>Yao, Y., Vehtari, A., Simpson, D. et Gelman, A. (2018) Using stacking to average Bayesian predictive distributions. Bayesian Analysis 13(3), 917–1007. <a href="doi:10.1214/17-BA1091" class="uri">doi:10.1214/17-BA1091</a>.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
