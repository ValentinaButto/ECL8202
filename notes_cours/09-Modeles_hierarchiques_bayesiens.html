<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Modèles hiérarchiques bayésiens</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Modèles hiérarchiques bayésiens</h1>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Le cours d’aujourd’hui couvre d’abord les méthodes de Monte-Carlo par chaînes de Markov, une famille d’algorithmes permettant d’appliquer l’inférence bayésienne à des modèles complexes. Nous parlerons spécifiquement de la plateforme Stan, qui possède certains avantages uniques par rapport à d’autres logiciels en raison de son implémentation de l’algorithme de Monte-Carlo hamiltonien. Nous présenterons ensuite un protocole pour le développement de modèle hiérarchiques bayésiens; ce protocole sera ensuite appliqué à un exemple déjà vu dans le cours sur les modèles linéaires généralisés à effets mixtes (GLMM).</p>
</div>
<div id="contenu-du-cours" class="section level1">
<h1>Contenu du cours</h1>
<ul>
<li><p>Méthodes de Monte-Carlo par chaînes de Markov</p></li>
<li><p>Plateforme Stan pour l’inférence bayésienne</p></li>
<li><p>Étapes de développement d’un modèle hiérarchique bayésien</p></li>
<li><p>GLMM bayésien avec brms</p></li>
</ul>
</div>
<div id="methodes-de-monte-carlo-par-chaines-de-markov" class="section level1">
<h1>Méthodes de Monte-Carlo par chaînes de Markov</h1>
<p>Lors du dernier cours, nous avons vu l’application du théorème de Bayes pour estimer la distribution <em>a posteriori</em> des paramètres <span class="math inline">\(\theta\)</span> d’un modèle en fonction d’observations <span class="math inline">\(y\)</span>.</p>
<p><span class="math display">\[p(\theta | y) = \frac{p(y | \theta) p(\theta)}{p(y)}\]</span></p>
<p>Dans cette équation, <span class="math inline">\(p(\theta)\)</span> est la distribution de probabilité <em>a priori</em> de <span class="math inline">\(\theta\)</span> (représentant leur incertitude avant d’avoir observé les données), tandis que <span class="math inline">\(p(y | \theta)\)</span> est la probabilité des observations <span class="math inline">\(y\)</span> conditionnelle à une valeur de <span class="math inline">\(\theta\)</span>, autrement dit la fonction de vraisemblance.</p>
<p>Avec plusieurs paramètres, <span class="math inline">\(\theta\)</span> est un vecteur, donc la distribution <em>a posteriori</em> résultant est la distribution conjointe des <span class="math inline">\(\theta\)</span> en fonction des données. Il est important de considérer cette distribution conjointe, car les valeurs les plus probables pour un paramètre peuvent dépendre de la valeur des autres paramètres.</p>
<p>Dans l’équation ci-dessous, le dénominateur <span class="math inline">\(p(y)\)</span> est la probabilité marginale des données. Puisqu’elle ne dépend pas de <span class="math inline">\(\theta\)</span>, cette probabilité peut être vue comme une constante de normalisation nécessaire pour que l’intégrale de la distribution de probabilité <em>a posteriori</em> soit égale à 1.</p>
<p>Nous avons aussi vu que <span class="math inline">\(p(y)\)</span> correspond à l’intégrale du numérateur <span class="math inline">\(p(y | \theta) p(\theta)\)</span> pour toutes les valeurs possibles de <span class="math inline">\(\theta\)</span>. Excepté dans des cas simples, on ne peut pas calculer exactement cette intégrale pour obtenir une formule mathématique de <span class="math inline">\(p(\theta | y)\)</span>.</p>
<p>Pour résoudre ce problème, nous ferons appel aux méthodes de Monte-Carlo. Comme nous avons vu dans le premier cours de la session, il s’agit de méthodes pour approximer une distribution en simulant un échantillon de cette distribution.</p>
<p>Il ne semble pas possible de simuler des échantillons de la distribution <span class="math inline">\(p(\theta | y)\)</span> si nous ne connaissons pas <span class="math inline">\(p(y)\)</span>. Toutefois, puisque <span class="math inline">\(p(y)\)</span> ne dépend pas de <span class="math inline">\(\theta\)</span>, il est possible de calculer le <em>rapport</em> des probabilités <em>a posteriori</em> de deux vecteurs <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[\frac{p(\theta_{(2)} | y)}{p(\theta_{(1)} | y)} = \frac{p(y | \theta_{(2)}) p(\theta_{(2)})}{p(y | \theta_{(1)}) p(\theta_{(1)})}\]</span></p>
<p><em>Note</em>: Ici, nous utilisons des indices entre parenthèses pour représenter différents vecteurs <span class="math inline">\(\theta\)</span>, afin d’éviter de confondre avec les différents éléments d’un seul vecteur, ex.: si <span class="math inline">\(\theta\)</span> est un vecteur de <span class="math inline">\(m\)</span> paramètres, <span class="math inline">\(\theta_{(1)} = (\theta_{1(1)}, \theta_{2(1)}, ... \theta_{m(1)})\)</span>.</p>
<div id="algorithme-de-metropolis-hastings" class="section level2">
<h2>Algorithme de Metropolis-Hastings</h2>
<p>L’algorithme de Metropolis-Hastings permet de générer un échantillon de la distribution <span class="math inline">\(p(\theta | y)\)</span> à partir de ces rapports de probabilité. Voici un résumé du fonctionnement de cette méthode.</p>
<ol style="list-style-type: decimal">
<li><p>Tout d’abord, on choisit aléatoirement un premier vecteur de paramètres <span class="math inline">\(\theta_{(1)}\)</span>.</p></li>
<li><p>Ensuite, on choisit un deuxième vecteur <span class="math inline">\(\theta_{(2)}\)</span>, qui dépend de <span class="math inline">\(\theta_{(1)}\)</span> selon une certaine probabilité de transition. Par exemple, chacun des paramètres de <span class="math inline">\(\theta_{(1)}\)</span> est déplacé d’une quantité tirée d’une distribution alaétoire normale.</p></li>
<li><p>On calcule le rapport des probabilités <em>a posteriori</em> <span class="math inline">\(\frac{p(\theta_{(2)} | y)}{p(\theta_{(1)} | y)}\)</span>.</p>
<ul>
<li><p>Si le rapport est plus grand ou égal à 1 (<span class="math inline">\(\theta_{(2)}\)</span> est plus probable que <span class="math inline">\(\theta_{(1)}\)</span>), on accepte <span class="math inline">\(\theta_{(2)}\)</span>.</p></li>
<li><p>Si le rapport est plus petit que 1, on accepte <span class="math inline">\(\theta_{(2)}\)</span> avec une probabilité égale à ce rapport; sinon, on reste au même point donc <span class="math inline">\(\theta_{(2)} = \theta_{(1)}\)</span>.</p></li>
</ul></li>
</ol>
<p>Les étapes 2 et 3 sont répétées pour le nombre d’itérations voulues.</p>
<p>Il a été démontré qu’avec suffisamment d’itérations, la distribution des <span class="math inline">\(\theta_{(i)}\)</span> s’approche aussi près que voulu de la distribution recherchée: <span class="math inline">\(p(\theta | y)\)</span>. Ce résultat théorique dépend en fait de certaines conditions; cependant, nous ne discuterons pas de ces détails ici, car nous sommes intéressés non pas à savoir si l’algorithme converge éventuellement, mais s’il converge assez rapidement pour être utile en pratique. Cela dépend du problème et il faudra déterminer empiriquement la convergence en inspectant les résultats de l’inférence, comme nosu verrons plus loin.</p>
</div>
<div id="chaines-de-markov" class="section level2">
<h2>Chaînes de Markov</h2>
<p>Dans l’algorithme de Metropolis-Hastings, chaque vecteur <span class="math inline">\(\theta_{(i+1)}\)</span> est un vecteur aléatoire qui dépend de <span class="math inline">\(\theta_{(i)}\)</span>. En théorie des probabilités, ce type de séquence est nommé chaîne de Markov. Cet algorithme est donc à la base des méthodes de Monte-Carlo par chaînes de Markov (<em>Markov chain Monte Carlo</em> en anglais, souvent abbrévié MCMC).</p>
<p>Pour illustrer la progression d’une chaîne de Markov, prenons l’exemple ci-dessous qui représente la distribution conjointe de deux paramètres <span class="math inline">\(\theta_1\)</span> et <span class="math inline">\(\theta_2\)</span>; les régions plus foncées représentent une plus grande densité de probabilité. Notez que les deux distributions sont corrélées: plus <span class="math inline">\(\theta_1\)</span> est grand, plus il est probable que <span class="math inline">\(\theta_2\)</span> soit petit et vice versa.</p>
<p><img src="../images/mcmc_dens_ex1.png" /></p>
<p>Dans le graphique suivant, les flèches vertes et mauves représentent deux chaînes de Markov initialisées à des positions aléatoires différentes. Bien que les transitions soient aléatoires, la probabilité d’accepter une transition est plus grande lorsque la densité de probabilité <em>a posteriori</em> est plus élevée, donc les chaînes s’approchent graduellement de la partie principale de la distribution.</p>
<p><img src="../images/mcmc_dens_ex3.png" /></p>
<p>Après cette période initiale, les deux chaînes explorent la distribution et la probabilité que chaque point <span class="math inline">\((\theta_1, \theta_2)\)</span> soit visité par une chaîne est proportionnelle à la densité de probabilité <em>a posteriori</em>.</p>
<p><img src="../images/mcmc_dens_ex4.png" /></p>
<p>Considérons maintenant les séquences de valeurs d’un paramètre <span class="math inline">\(\theta\)</span> visitées par trois chaînes de Markov, telles qu’illustrées dans le graphique ci-dessous.</p>
<p><img src="../images/mcmc_trace_ex.png" /></p>
<p>Au départ, les chaînes doivent partir de leur points initiaux respectifs et s’approcher de la partie principale de la distribution. Il s’agit de la période de rodage (appelée <em>burn-in</em> ou <em>warmup</em> en anglais). Les valeurs du paramètre durant la période de rodage ne sont pas retenues pour l’inférence. Après la ligne pointillée, on voit que les chaînes ont convergé et se sont mélangées. Il s’agit de la période d’échantillonnage qui sera utilisée pour approximer la distribution <em>a posteriori</em> du paramètre.</p>
</div>
<div id="verification-de-la-convergence-des-chaines" class="section level2">
<h2>Vérification de la convergence des chaînes</h2>
<p>Comme nous avons vu ci-dessus, l’inspection du tracé des chaînes (<em>trace plot</em> en anglais) peut nous indiquer si différentes chaînes de Markov ont convergé, ce qui signifie que leurs valeurs peuvent être utilisées pour estimer la distribution <em>a posteriori</em>.</p>
<p>Pour déterminer de façon plus quantitative la convergence, nous pouvons utiliser la statistique de Gelman-Rubin, dénotée <span class="math inline">\(\hat{R}\)</span>. Cette statistique représente la variance d’un paramètre entre les chaînes relative à la variance du paramètre dans chaque chaîne. Cette statistique est conceptuellement semblable à une ANOVA: si les chaînes explorent la même distribution, alors le niveau de variation entre valeurs d’une même chaîne est semblable à la variation entre valeurs de chaînes différentes.</p>
<p>À la convergence, <span class="math inline">\(\hat{R}\)</span> doit être d’environ 1. Il n’y a pas de seuil définitif pour cette valeur, mais la plupart des auteurs s’entendent pour dire que <span class="math inline">\(\hat{R}\)</span> ne devrait pas dépasser 1.1. Néanmoins, <span class="math inline">\(\hat{R} \leq 1.1\)</span> ne garantit pas la convergence vers la bonne distribution; nous verrons plus tard d’autres diagnostics visant à confirmer que l’algorithme explore bien la distribution <em>a posteriori</em>.</p>
<p>En cas de problème de convergence, nous pouvons allonger la période de rodage. Si la convergence est beaucoup trop lente ou que chaque chaîne reste “prise” dans une partie de la distribution plutôt que de se mélanger aux autres chaînes, cela pourrait indiquer une difficulté d’estimation des paramètres du modèle avec les données fournies. Dans ce cas, il serait utile de reparamétriser ou modifier le modèle.</p>
</div>
<div id="efficacite-de-lechantillonnage" class="section level2">
<h2>Efficacité de l’échantillonnage</h2>
<p>Si l’algorithme converge, nous pouvons quantifier l’efficacité avec laquelle les chaînes de Markov approximent notre distribution <em>a posteriori</em>.</p>
<p>Considérons une fonction <span class="math inline">\(f\)</span> calculée à partir des paramètres du modèle. Il peut s’agir de la moyenne du paramètre, d’un quantile, ou de toute statistique d’intérêt qui dépend d’un ou plusieurs paramètres du modèle. Si on avait un échantillon de <span class="math inline">\(N\)</span> tirages aléatoires <strong>indépendants</strong> de la distribution conjointe <em>a posteriori</em> des paramètres, alors la valeur de <span class="math inline">\(f\)</span> calculée à partir de cet échantillon s’approcherait de sa valeur pour la distribution exacte, avec une erreur d’approximation (erreur-type de Monte-Carlo) proportionnelle à <span class="math inline">\(1/\sqrt{N}\)</span>.</p>
<p><em>Note</em>: Il ne faut pas confondre l’erreur-type de Monte-Carlo avec l’écart-type de la distribution <em>a posteriori</em> du paramètre. L’écart-type de la distribution <em>a posteriori</em> (semblable à l’erreur-type pour un estimateur fréquentiste) représente lincertitude sur la valeur du paramètre et dépend (notamment) de la quantité d’observations. L’erreur-type de Monte-Carlo est l’erreur d’approximation numérique de l’algorithme. En augmentant le nombre d’itérations, nous pouvons estimer plus précisément toutes les propriétés de la distribution <em>a posteriori</em>, incluant son écart-type, mais nous ne pouvons pas réduire cet écart-type sans avoir plus de données. Nous avions la même situation dans le cas du bootstrap: nous pouvions augmenter le nombre d’échantillons bootstrap pour réduire l’erreur d’approximation numérique, mais pas l’incertitude due aux données limitées.</p>
<p>Toutefois, la chaîne de Markov ne produit <em>pas</em> des tirages indépendants, puisque la valeur de <span class="math inline">\(\theta_{(i+1)}\)</span> est conditionnelle à <span class="math inline">\(\theta_{i}\)</span>. Dans ce cas, les valeurs successives de la chaîne sont corrélées, donc un nombre <span class="math inline">\(N\)</span> d’itérations n’est pas équivalent à un échantillon de <span class="math inline">\(N\)</span> valeurs indépendantes.</p>
<p>Les logiciels d’inférence bayésienne calculent l’erreur-type de Monte-Carlo et la taille effective de l’échantillon, <span class="math inline">\(N_{eff}\)</span>, soit le nombre de tirages indépendants nécessaires pour avoir la même précision que les <span class="math inline">\(N\)</span> itérations corrélées. En général, <span class="math inline">\(N_{eff}\)</span> est inférieur au nombre d’itérations, mais ce n’est pas toujours le cas, en particulier pour certains algorithmes plus performants comme l’algorithme hamiltonien vu dans la section suivante.</p>
</div>
</div>
<div id="plateforme-stan-pour-linference-bayesienne" class="section level1">
<h1>Plateforme Stan pour l’inférence bayésienne</h1>
<p>Stan (<a href="https://mc-stan.org" class="uri">https://mc-stan.org</a>) désigne à la fois un langage pour spécifier des modèles statistiques (comme nous avons vu au dernier labo) et un logiciel qui implémente différents algorithmes d’inférence pour ces modèles. Il s’agit d’un des plus récents logiciels d’inférence bayésienne, lancé en 2015.</p>
<blockquote>
<p>Carpenter, B. et al. (2017) Stan: A Probabilistic Programming Language. <em>Journal of Statistical Software</em> 76(1). 10.18637/jss.v076.i01.</p>
</blockquote>
<p>Les modèles codés en langage Stan sont compilés en code C++ afin d’obtenir une bonne rapidité d’exécution.</p>
<p>Si Stan est un logiciel séparé, il existe des packages en R (<em>rstan</em>) et Python permettant de faire l’interface avec Stan. Aussi, il existe plusieurs packages R qui offrent davantage d’options pour utiliser Stan:</p>
<ul>
<li><p><em>brms</em> et <em>rstanarm</em> traduisent automatiquement des modèles spécifiés en R en langage Stan;</p></li>
<li><p><em>bayesplot</em> et <em>shinystan</em> produisent des visualisations des résultats des modèles, comme nous verrons plus tard;</p></li>
<li><p><em>loo</em> implémente une méthode de comparaison de modèles et de prédiction multi-modèles basées sur l’approximation de l’erreur de validation croisée;</p></li>
<li><p><em>tidybayes</em> offre d’autres options de visualisations, en particulier pour les distributions <em>a posteriori</em> de paramètres.</p></li>
</ul>
<div id="methode-de-monte-carlo-hamiltonienne" class="section level2">
<h2>Méthode de Monte-Carlo hamiltonienne</h2>
<p>L’algorithme MCMC implémenté par Stan est la méthode de Monte-Carlo hamiltonienne (HMC). Une des particularités de cette méthode est qu’elle évalue non seulement la valeur de <span class="math inline">\(p(y | \theta) p(\theta)\)</span> à chaque itération, mais aussi son gradient, qui est l’équivalent de la “pente” d’une surface en plusieurs dimensions. Ainsi, l’algorithme sait dans quelle direction la probabilité <em>a posteriori</em> augmente, ce qui permet de converger plus rapidement vers la partie de la distribution contenant la plus grande part de la probabilité.</p>
<p>De plus, chaque itération de cet algorithme est composée de plusieurs pas et suit une “courbe” dans l’espace des paramètres qui est guidée par la forme de la distribution de probabilité. Cela permet aux points de la chaîne d’être moins rapprochés que dans le cas des méthodes MCMC traditionnelles, ce qui signifie que ces points sont davantage indépendants et que la taille effective de l’échantillon <span class="math inline">\(N_{eff}\)</span> est plus grande pour un même nombre d’itérations.</p>
<p>En plus de ces avantages au niveau de la performance, l’algorithme hamiltonien offre des diagnostics uniques, donc la présence de transitions divergentes, qui permettent de vérifier sa validité.</p>
<p>L’article suivant présente plus de détails sur la méthode de Monte-Carlo hamiltonienne dans un contexte de modélisation écologique.</p>
<blockquote>
<p>Monnahan, C.C., Thorson, J.T. et Branch, T.A. (2017) Faster estimation of Bayesian models in ecology using Hamiltonian Monte Carlo. <em>Methods in Ecology and Evolution</em> 8: 339-348.</p>
</blockquote>
</div>
<div id="diagnostics-dans-stan" class="section level2">
<h2>Diagnostics dans Stan</h2>
<div id="transitions-divergentes" class="section level3">
<h3>Transitions divergentes</h3>
<p>Les transition divergentes indiquent que l’algorithme a de la difficulté à explorer une région de la distribution de probabilité <em>a posteriori</em>, généralement en raison d’un changement trop abrupt de la forme de cette distribution. Il s’agit du diagnostic le plus sérieux, car même un petit nombre de divergences compromet la validité des résultats de l’algorithme.</p>
<p>Une des façons d’éliminer les divergences est de forcer l’algorithme à faire des plus petits pas, en augmentant le paramètre <em>adapt_delta</em> réglable dans Stan. Cependant, dans un cas où les divergences persistent, il peut être nécessaire de reparamétriser le modèle.</p>
</div>
<div id="profondeur-maximale-de-larbre-maximum-tree-depth" class="section level3">
<h3>Profondeur maximale de l’arbre (<em>maximum tree depth</em>)</h3>
<p>L’algorithme hamiltonien évalue différentes trajectoires possibles (représentées par un arbre) pour choisir la valeur des paramètres à la prochaine itération. Lorsque la profondeur maximale de l’arbre est atteinte, cela signifie que l’algorithme a essayé le nombre maximal de trajectoires, mais qu’une trajectoire plus longue demeure possible. Contrairement aux transitions divergentes, cet avertissement n’invalide pas les résultats, mais il peut indiquer une paramétrisation sous-optimale.</p>
<p>On peut augmenter la profondeur maximale avec l’argument <em>max_treedepth</em>, mais cela augmente le temps pris pour chaque itération.</p>
</div>
<div id="energie-bfmi-low" class="section level3">
<h3>Énergie (<em>BFMI low</em>)</h3>
<p>Comme pour les divergences, cet avertissement indique que l’algorithme ne parcourt pas la distribution <em>a posteriori</em> de façon efficace. Ce problème peut parfois être réglé en allongeant la période de rodage. Toutefois, s’il survient en même temps qu’un des précédents, la formulation du modèle doit probablement être revue.</p>
</div>
</div>
<div id="options-pour-utiliser-stan-a-partir-de-r" class="section level2">
<h2>Options pour utiliser Stan à partir de R</h2>
<p>Pour conclure cette section, nous allons voir différentes façons d’utiliser Stan à partir de R. Tout d’abord, nous pouvons écrire un programme Stan, tel que vu au dernier laboratoire. Voici le début du code Stan pour un modèle simple, où il y a <span class="math inline">\(N\)</span> observations d’une variable réponse <span class="math inline">\(y\)</span> et d’un prédicteur <span class="math inline">\(x\)</span>.</p>
<pre><code>data {
  int N;
  vector[N] y;
  vector[N] x;
}

[...]
</code></pre>
<p>Pour estimer les paramètres de ce modèle à partir de données présentes dans un tableau de données <code>df</code> contenant les colonnes <code>x</code> et <code>y</code>, nous devons d’abord créer une liste associant les données à chaque variable du bloc <code>data</code> du programme Stan.</p>
<pre class="r"><code>dat &lt;- list(N = nrow(df), y = df$y, x = df$x)</code></pre>
<p>Ensuite, nous appelons la fonction <code>stan_model</code> pour compiler le modèle, puis <code>sampling</code> pour estimer la distribution <em>a posteriori</em> des paramètres à partir des données.</p>
<pre class="r"><code>library(rstan)
mod &lt;- stan_model(&quot;model.stan&quot;)
result &lt;- sampling(mod, data = dat)</code></pre>
<p>Lors du dernier cours, nous avons brièvement présenté le package <em>brms</em>, qui permet de représenter des modèles avec une formule semblable aux fonctions déjà vues dans de cours (<code>lm</code>, <code>glm</code>, <code>lmer</code>, etc.), puis les traduit automatiquement en langage Stan pour estimer les paramètres de façon bayésienne. La fonction <code>brm</code> est utilisée pour tous les types de modèles supportés par le package (modèles linéaires généralisés, modèles à effets mixtes, dépendance temporelle et spatiale, etc.).</p>
<pre class="r"><code>library(brms)
res_brms &lt;- brm(y ~ x, data = df)</code></pre>
<p>Le package <em>rstanarm</em> est une alternative à <em>brms</em>. Plutôt que d’utiliser une seule fonction, ce package contient des fonctions spécialisées à chaque type de modèle (ex.: <code>stan_lm</code>, <code>stan_glm</code>, <code>stan_lmer</code>).</p>
<pre class="r"><code>library(rstanarm)
res_arm &lt;- stan_lm(y ~ x, data = df)</code></pre>
<p>Ce package a un peu moins d’options que <em>brms</em>, mais son principal avantage est que les programmes Stan utilisés sont pré-compilés. Le temps de compilation n’est généralement que de quelques minutes pour un nouveau modèle, mais cette économie de temps peut être utile lorsqu’il faut évaluer successivement de nombreux modèles différents.</p>
<p>Les deux packages <em>rstanarm</em> et <em>brms</em> permettent d’estimer les paramètres de types de modèles courants sans se soucier de programmer en langage Stan et particulièrement d’optimiser la formulation du modèle pour faciliter le travail de l’algorithme. Leur usage est donc recommandé, sauf lorsqu’on a besoin d’un modèle personnalisé qui doit être codé en Stan.</p>
</div>
</div>
<div id="etapes-de-developpement-dun-modele-hierarchique-bayesien" class="section level1">
<h1>Étapes de développement d’un modèle hiérarchique bayésien</h1>
<p>Cette partie présente les étapes d’un plan suggéré pour le développement et la validation d’un modèle hiérarchique bayésien. La démonstration de ces étapes sera présentée dans la partie suivante.</p>
<p>Ce plan est basé sur l’article:</p>
<blockquote>
<p>Betancourt, M. (2018) Towards A Principled Bayesian Workflow (RStan). <a href="https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html" class="uri">https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html</a>.</p>
</blockquote>
<p>Michael Betancourt est un des développeurs de Stan et son site web contient plusieurs articles sur la théorie reliée aux modèles hiérarchiques bayésiens, ainsi que des études de cas sur leur application à différents problèmes.</p>
<p>Voici les principales étapes à suivre lorsqu’on développe un nouveau modèle hiérarchique bayésien pour représenter un système donné.</p>
<ol style="list-style-type: decimal">
<li><p>Formuler le modèle.</p></li>
<li><p>Vérifier les prédictions <em>a priori</em>.</p></li>
<li><p>Tester l’ajustement du modèle à des données simulées.</p></li>
<li><p>Ajuster le modèle aux données réelles et vérifier les diagnostics.</p></li>
<li><p>Vérifier les prédictions <em>a posteriori</em>.</p></li>
</ol>
<p>Notez qu’avant l’étape 4, nous vérifions la cohérence interne du modèle, puis aux étapes 4 et 5 nous vérifions s’il représente adéquatement les données observées.</p>
<div id="formulation-du-modele" class="section level2">
<h2>Formulation du modèle</h2>
<p>D’abord, nous devons décrire les variables du modèle et leurs relations mathématiques, ainsi que la distribution statistique assignée aux variables réponses. Il est notamment important de considérer la structure de l’échantillonnage ou de l’expérience afin de spécifier la hiérarchie des effets aléatoires.</p>
<p>C’est aussi à cette étape que nous choisissons les distributions <em>a priori</em> des paramètres.</p>
</div>
<div id="predictions-a-priori" class="section level2">
<h2>Prédictions <em>a priori</em></h2>
<p>Pour vérifier si les distributions <em>a priori</em> des prédicteurs génèrent des valeurs réalistes des observations, nous commençons par générer des vecteurs de paramètres à partir de leur distribution <em>a priori</em>, puis nous simulons un jeu de données semblable à celui observé à partir du modèle basé sur chaque vecteur de paramètres. Cette simulation utilise sur la valeur réelle des prédicteurs pour chaque observation.</p>
<p>À partir des résultats des simulations, nous vérifions si les propriétés des observations simulées correspondent à des valeurs réalistes pour le problème. À cette étape, nous ne comparons pas directement les simulations aux observations réelles, seulement à notre connaissance préalable de ce qui constitue une valeur raisonnable de la réponse.</p>
</div>
<div id="ajustement-du-modele-aux-donnees-simulees" class="section level2">
<h2>Ajustement du modèle aux données simulées</h2>
<p>Pour cette étape, nous ajustons le modèle à chaque jeu de données simulé à l’étape précédente. Ces jeux de données contiennent les valeurs réelles des paramètres, mais la réponse est simulée à partir de paramètres connus tirés de la distribution <em>a priori</em>.</p>
<p>Ensuite, nous vérifions les diagnostics de l’ajustement pour chaque simulation, puis nous vérifions l’exactitude des inférences du modèle en comparant les distributions <em>a posteriori</em> aux valeurs des paramètres utilisées pour chaque simulation. Puisque les données ont été obtenues par simulation, nous nous attendons à ce que l’inférence produise des estimés compatibles avec les valeurs connues des paramètres. Deux tests sont utiles à ce point-ci:</p>
<ul>
<li><p>Test de calibration: les intervalles de probabilité <em>a posteriori</em> sont-ils justes?</p></li>
<li><p>Test de sensibilité: Les données permettent-elles de cerner la valeur du paramètre?</p></li>
</ul>
<div id="calibration-par-simulation" class="section level3">
<h3>Calibration par simulation</h3>
<p>Supposons que nous avons des observations <span class="math inline">\(y\)</span> simulées à partir du modèle avec un paramètre <span class="math inline">\(\theta\)</span> tiré de la distribution <em>a priori</em>. En ajustant le modèle à ces <span class="math inline">\(y\)</span>, nous obtenons un échantillon de la distribution <em>a posteriori</em> de <span class="math inline">\(\theta\)</span>, soit <span class="math inline">\(\theta_{(i)}\)</span> pour <span class="math inline">\(i\)</span> de 1 jusqu’à <span class="math inline">\(N\)</span> itérations.</p>
<p>Si l’inférence est correcte, le rang de <span class="math inline">\(\theta\)</span> parmi les <span class="math inline">\(\theta_{(i)}\)</span> est distribué uniformément entre 1 et <span class="math inline">\(N + 1\)</span>. Ceci équivaut à dire que si un intervalle contenant une certaine fraction (disons 90%) de la probabilité <em>a posteriori</em> de <span class="math inline">\(\theta\)</span>, la vraie valeur du paramètre s’y trouve cette même fraction du temps. (Cette propriété de couverture est analogue à celle des intervalles de confiance fréquentistes.) En particulier, si <span class="math inline">\(\theta\)</span> se trouve plus souvent aux rangs extrêmes que prévu, cela signifierait que la distribution <em>a posteriori</em> sous-estime l’incertitude sur <span class="math inline">\(\theta\)</span>. Au contraire, si <span class="math inline">\(\theta\)</span> se trouve toujours aux rangs centraux, cela signifierait que son incertitude est surestimée.</p>
<p>Le test de calibration par simulation vise donc à vérifier que sur un grand nombre de simulations, le rang de la vraie valeur de <span class="math inline">\(\theta\)</span> parmi les <span class="math inline">\(\theta_{(i)}\)</span> est distribué uniformément.</p>
<blockquote>
<p>Talts, S. et al. (2018) Validating Bayesian inference algorithms with simulation-based calibration. arXiv:1804.06788.</p>
</blockquote>
</div>
<div id="sensibilite" class="section level3">
<h3>Sensibilité</h3>
<p>Si le modèle est bien calibré, le test de sensibilité vise à déterminer si, en fonction de la quantité de données disponible, il est possible d’estimer précisément la valeur de chaque paramètre.</p>
<p>Le <span class="math inline">\(z\)</span>-score (ou cote <span class="math inline">\(z\)</span>) est la différence centrée réduite entre la valeur postérieure estimée et la valeur réelle du paramètre.</p>
<p><span class="math inline">\(\frac{\bar{\theta}_{post} - \theta}{\sigma_{post}}\)</span></p>
<p>Autrement dit, cette statistique donne l’écart entre la valeur estimée et la valeur réelle du paramètre, en unités d’écarts-types de la distribution <em>a posteriori</em>. Cette valeur devrait être près de zéro en général; par exemple, si la distribution <em>a posteriori</em> du paramètre est normale, pour 95% des simulations la valeur de cette cote sera entre -2 et 2.</p>
<p>Le facteur de contraction (<em>shrinkage</em>) représente la réduction de la variance par rapport à la distribution <em>a priori</em>:</p>
<p><span class="math inline">\(1 - \frac{\sigma^2_{post}}{\sigma^2_{prior}}\)</span></p>
<p>En général, on s’attent à ce que la variance <em>a posteriori</em> soit plus petite que celle <em>a priori</em>. Par exemple, si cette variance <em>a posteriori</em> est 10 fois plus faible que la variance <em>a priori</em>, la contraction est de 90%.</p>
<p>Notez que ce terme n’a pas le même sens ici que celui vu plus tôt dans le contexte des modèles mixtes, où il désigne la contraction des effets aléatoires vers la moyenne générale.</p>
</div>
</div>
<div id="ajustement-aux-donnees-reelles" class="section level2">
<h2>Ajustement aux données réelles</h2>
<p>Une fois la cohérence interne du modèle vérifiée, nous pouvons maintenant ajuster le modèle aux données réelles, vérifier les diagnostics (divergences, profondeur de l’arbre, énergie), puis réajuster le modèle au besoin en modifiant les paramètres de l’algorithme.</p>
<p>Si aucun problème n’est détecté, nous pouvons consulter le sommaire des estimés et visualiser les distributions <em>a posteriori</em> des paramètres.</p>
</div>
<div id="predictions-a-posteriori" class="section level2">
<h2>Prédictions <em>a posteriori</em></h2>
<p>À cette dernière étape, nous voulons vérifier que les prédictions obtenues en simulant des observations à partir de la distribution <em>a posteriori</em> des paramètres se rapprochent suffisamment des observations.</p>
<p>Tel que vu à la fin du dernier cours, nous pouvons vérifier si suffisamment d’observations se trouvent dans leur intervalle de prédiction selon le modèle ajusté. Aussi, nous pouvons comparer les prédictions et observations au moyen de statistiques sommaires décrivant des caractéristiques importantes du jeu de données qui ne sont pas directement ajustées par le modèle.</p>
</div>
</div>
<div id="exemple-detaille-glmm-bayesien-avec-brms" class="section level1">
<h1>Exemple détaillé: GLMM bayésien avec <em>brms</em></h1>
<p>Pour cet exemple, nous utilisons le jeu de données <code>rikz</code> tiré du manuel de Zuur et al., <em>Mixed Effects Models and Extensions in Ecology with R</em>. Nous avions utilisé ces mêmes données dans le cours 5 sur les modèles linéaires généralisés à effets mixtes.</p>
<p>Le jeu de données <code>rikz</code> présente des mesures de la richesse de la faune benthique (<em>Richness</em>) pour 45 sites répartis sur 9 plages (<em>Beach</em>) aux Pays-Bas, ainsi que deux prédicteurs: la position verticale du site (<em>NAP</em>) ainsi que l’indice d’exposition de la plage (<em>Exposure</em>). Puisque ce dernier ne prend que trois valeurs différentes (8, 10 et 11), nous le traiterons comme un facteur.</p>
<pre class="r"><code>rikz &lt;- read.csv(&quot;../donnees/rikz.csv&quot;)
# Exprimer Beach et Exposure comme des variables catégorielle (facteurs)
rikz &lt;- mutate(rikz, Beach = as.factor(Beach), 
               Exposure = as.factor(Exposure))
head(rikz)</code></pre>
<pre><code>##   Sample Richness Exposure    NAP Beach
## 1      1       11       10  0.045     1
## 2      2       10       10 -1.036     1
## 3      3       13       10 -1.336     1
## 4      4       11       10  0.616     1
## 5      5       10       10 -0.684     1
## 6      6        8        8  1.190     2</code></pre>
<p>Voici les packages dont nous aurons besoin pour cet exemple.</p>
<pre class="r"><code>library(brms)
library(dplyr)
library(tidyr)
library(ggplot2)
library(cowplot)
theme_set(theme_cowplot())</code></pre>
<div id="formulation-du-modele-1" class="section level2">
<h2>Formulation du modèle</h2>
<p>Comme au cours 5, nous modélisons ces données par une régression de Poisson, avec un effet aléatoire de la plage sur l’ordonnée à l’origine. Voici la représentation mathématique de ce modèle, où nous avons choisi des noms des paramètres qui se rapprochent de ceux donnés par <em>brms</em>:</p>
<pre><code>Richness ~ poisson(lambda)
log(lambda) = b_Intercept + r_Beach + b_NAP * NAP + b_Exposure10 * Exposure10 + b_Exposure * Exposure11
r_Beach ~ normal(0, sd_Beach)</code></pre>
<ul>
<li>la richesse spécifique suit une distribution de Poisson de moyenne <code>lambda</code>;</li>
<li><code>log(lambda)</code> suit une fonction linéaire dépendant du NAP et de l’indice d’exposition, avec une ordonnée à l’origine moyenne (<code>b_Intercept</code>) et un effet aléatoire pour chaque plage autour de cette moyenne (<code>r_Beach</code>);</li>
<li>les effets aléatoires de plage sont distribués normalement avec un écart-type <code>sd_Beach</code>.</li>
</ul>
<p>La formule ci-dessus est basée sur le codage par défaut des facteurs dans R, avec <code>Exposure = 8</code> comme niveau de référence. <code>Exposure10</code> et <code>Exposure11</code> prennent une valeur de 1 lorsque la variable <code>Exposure</code> est de 10 ou 11, respectivement; ainsi, <code>b_Exposure10</code> représente la différence de <code>log(lambda)</code> entre les niveaux 8 et 10 et <code>b_Exposure11</code> la différence entre les niveaux 8 et 11.</p>
<p>Il nous reste à choisir la distribution <em>a priori</em> pour chaque paramètre. Supposons que nous savons que la richesse spécifique sur ce type de site peut atteindre des dizaines d’espèces, mais pas des centaines. Une distribution <code>normal(2, 1)</code> pour l’ordonnée à l’origine signifie que nous accordons 95% de la probabilité à des valeurs entre 0 et 4 sur l’échelle logarithmique, ou de 1 à 55 en prenant l’exponentielle.</p>
<pre class="r"><code>exp(c(0, 4))</code></pre>
<pre><code>## [1]  1.00000 54.59815</code></pre>
<p>Pour les coefficients des prédicteurs, la distribution <code>normal(0, 1)</code> est un choix raisonnable: nous supposons déjà que le logarithme du nombre d’espèces varie sur une échelle de quelques unités, ce qui est aussi le cas pour le seul prédicteur numérique (NAP).</p>
<pre class="r"><code>summary(rikz$NAP)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -1.3360 -0.3750  0.1670  0.3477  1.1170  2.2550</code></pre>
<p>Finalement, il faut donner une distribution à l’écart-type des effets aléatoires de plage. La distribution <code>normal(0, 1)</code>, qui est en fait demi-normale car tronquée à zéro, est peut-être trop permissive ici. Un écart-type de 2 signifierait que la richesse spécifique varierait d’un facteur de <span class="math inline">\(e^2 \approx 7\)</span> d’une plage à l’autre. Nous utilisons donc plutôt <code>normal(0, 0.5)</code>.</p>
<p>Voici comment spécifier ces distributions <em>a priori</em> avec <em>brms</em>:</p>
<pre class="r"><code>rikz_prior &lt;- c(set_prior(&quot;normal(0, 1)&quot;, class = &quot;b&quot;),
                set_prior(&quot;normal(2, 1)&quot;, class = &quot;Intercept&quot;),
                set_prior(&quot;normal(0, 0.5)&quot;, class = &quot;sd&quot;))</code></pre>
<p>La classe de coefficients “b” représente une distribution qui s’applique à tous les coefficients fixes, sauf l’ordonnée à l’origine qui correspond à la classe “Intercept”. Pour spécifier la distribution d’un seul coefficient dans une classe, il faut préciser l’argument <code>coef</code> en plus de <code>class</code>. La classe “sd” représente la distribution <em>a priori</em> pour les écarts-types des effets aléatoires. Pusique <em>brms</em> sait que les écarts-types sont toujours supérieurs à zéro, il n’est pas nécessaire de spécifier la borne inférieure avec <code>lb</code> ici.</p>
</div>
<div id="predictions-a-priori-1" class="section level2">
<h2>Prédictions <em>a priori</em></h2>
<p>Nous appelons d’abord la fonction <code>brm</code> avec l’argument <code>sample_prior = "only"</code> pour obtenir un échantillon de la distribution <em>a priori</em> des paramètres d’un modèle.</p>
<pre class="r"><code>res_prior &lt;- brm(Richness ~ NAP + Exposure + (1 | Beach), data = rikz, 
                 family = poisson, sample_prior = &quot;only&quot;, 
                 chains = 1, iter = 400, prior = rikz_prior)</code></pre>
<p>Par défaut, <code>brm</code> estime la distribution <em>a posteriori</em> avec 4 chaînes et 2000 itérations par chaîne, avec 50% de ces itérations constituant la période de rodage. Ici, nous spécifions une seule chaîne et 400 itérations, donc il y a 200 itérations de rodage et 200 pour l’échantillonnage.</p>
<p>La fonction <code>posterior_samples</code> retourne en général un tableau des valeurs tirées de la distribution <em>a posteriori</em> des paramètres, mais il s’agit ici de la distribution <em>a priori</em> en raison de l’argument <code>sample_prior</code>.</p>
<pre class="r"><code>prior_params &lt;- posterior_samples(res_prior)
str(prior_params)</code></pre>
<pre><code>## &#39;data.frame&#39;:    200 obs. of  15 variables:
##  $ b_Intercept         : num  1.113 3.499 2.153 0.136 3.622 ...
##  $ b_NAP               : num  -0.0686 -0.6141 -1.234 -0.2929 0.4807 ...
##  $ b_Exposure10        : num  -0.441 -0.406 -0.473 0.321 -0.639 ...
##  $ b_Exposure11        : num  0.265 -0.257 0.138 2.156 -1.629 ...
##  $ sd_Beach__Intercept : num  0.3018 0.0763 0.0478 0.3042 0.7093 ...
##  $ r_Beach[1,Intercept]: num  0.3419 -0.0968 -0.0536 0.0275 -0.0424 ...
##  $ r_Beach[2,Intercept]: num  -0.32867 -0.00337 0.05592 -0.01466 0.04736 ...
##  $ r_Beach[3,Intercept]: num  -0.1363 0.0619 0.024 0.1615 -0.7123 ...
##  $ r_Beach[4,Intercept]: num  0.6445 -0.1244 -0.0595 -0.3911 0.8905 ...
##  $ r_Beach[5,Intercept]: num  -0.31044 0.00526 0.0078 0.23129 -0.6946 ...
##  $ r_Beach[6,Intercept]: num  -0.1426 0.0228 0.0352 0.5641 0.0794 ...
##  $ r_Beach[7,Intercept]: num  0.0378 -0.0767 -0.0131 0.3138 -0.7143 ...
##  $ r_Beach[8,Intercept]: num  -0.27338 -0.02351 0.00226 0.15147 -0.14758 ...
##  $ r_Beach[9,Intercept]: num  -0.1703 -0.1018 -0.0316 -0.3655 1.0734 ...
##  $ lp__                : num  -18.3 -18.8 -18.2 -20 -18.2 ...</code></pre>
<p>Chaque rangée représente une itération de la période d’échantillonnage. Dans ce cas-ci, Stan ne fait que tirer des valeurs de chaque paramètre à partir de leur distribution <em>a priori</em>. Les paramètres dont le nom commence par <code>b</code> représentent les effets fixes, <code>sd_Beach__Intercept</code> est l’écart-type des effets aléatoires de la plage sur l’ordonnée à l’origine, puis les paramètres avec un nom commençant par <code>r</code> sont les effets aléatoires; ces derniers sont tirés de la distribution définie par <code>sd_Beach__Intercept</code>. Le dernier paramètre, <code>lp__</code>, représente le log de la probabilité conjointe des paramètres.</p>
<p>Nous ajoutons à ce jeu de données une colonne identifiant la simulation (de 1 à 200):</p>
<pre class="r"><code>prior_params$sim_id &lt;- as.character(1:200)</code></pre>
<p>La fonction <code>posterior_predict</code> génère une simulation de la variable réponse en fonction de la valeur des paramètres à chaque itération et des prédicteurs du jeu de données. (Malgré le nom, cette fonction se rapproche plus de la fonction <code>simulate</code> pour les modèles classiques dans R, plutôt que de <code>predict</code>.) Le résultat est une matrice avec une rangée pour chacune des 200 itérations de la distribution <em>a priori</em> et une colonne pour chacune des 45 observations du jeu de données original (dans le même ordre que les 45 rangées de ce jeu de données original).</p>
<pre class="r"><code>prior_pred &lt;- posterior_predict(res_prior)
str(prior_pred)</code></pre>
<pre><code>##  int [1:200, 1:45] 6 21 4 0 19 1 99 6 25 95 ...</code></pre>
<p>Afin de pouvoir visualiser ces prédictions, nous ajoutons une colonne pour identifier la simulation et nous faisons un pivot pour obtenir 3 colonnes: <code>sim_id</code>, <code>obs_id</code> et <code>Richness</code>.</p>
<pre class="r"><code>prior_df &lt;- data.frame(prior_pred)
prior_df$sim_id &lt;- 1:200
prior_df &lt;- pivot_longer(prior_df, cols = -sim_id, 
                         names_to = &quot;obs_id&quot;, values_to = &quot;Richness&quot;)
summary(prior_df$Richness)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00    2.00    8.00   25.15   20.00 3375.00</code></pre>
<p>La valeur maximale simulée est très élevée (3458 espèces), ce qui se produit fréquemment lorsque la réponse suit une fonction exponentielle des prédicteurs. Cependant, la grande majorité des valeurs simulées sont inférieures à 100.</p>
<pre class="r"><code>mean(prior_df$Richness &lt; 100)</code></pre>
<pre><code>## [1] 0.9581111</code></pre>
<p>Voici une façon de visualiser la distribution de la richesse spécifique pour chaque simulation <em>a priori</em> du modèle. Nous créons une courbe de densité de probabilité (avec <code>stat_density</code>) pour chaque simulation (<code>group = sim_id</code>), avec un niveau de transparence <code>alpha = 0.3</code> pour voir les courbes superposées. Nous appliquons une transformation racine carrée à l’axe des <code>x</code> pour mieux voir toutes les données (une transformation log est impossible en raison de la présence de zéros), puis nous limitons cet axe aux valeurs entre 0 et 200.</p>
<pre class="r"><code>ggplot(prior_df, aes(x = Richness)) +
    stat_density(aes(group = sim_id), position = &quot;identity&quot;, geom = &quot;line&quot;, alpha = 0.3) +
    scale_x_sqrt(breaks = c(0, 1, 10, 25, 50, 75, 100)) +
    coord_cartesian(xlim = c(0, 200))</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>La plupart des simulations donnent une faible probabilité à des valeurs de richesse spécifique &gt; 100, tel que prévu par les connaissances <em>a priori</em> du système.</p>
</div>
<div id="ajustement-aux-donnees-simulees" class="section level2">
<h2>Ajustement aux données simulées</h2>
<p>Nous avons plus haut obtenu des simulations de la variable réponse pour chacune des 200 valeurs tirées de la distribution <em>a priori</em> de notre vecteur de paramètres. Nous souhaitons maintenant ajuster le modèle à chacune de ces simulations pour effectuer les tests de calibration et de sensibilité.</p>
<p>Le package <em>brms</em> inclut une fonction <code>brm_multiple</code> qui permet d’ajuster le même modèle à plusieurs jeux de données contenus dans une liste. Nous créons donc une liste de 200 réplicats du jeu de données original avec <code>replicate</code> (<code>simplify = FALSE</code> est nécessaire pour éviter que R tente de combiner les 200 jeux de données en un seul), puis nous utlisons une boucle pour remplacer la colonne réponse de chaque jeu de données avec une des rangées de la matrice de prédictions <em>a priori</em>.</p>
<pre class="r"><code>rikz_repl &lt;- replicate(200, rikz, simplify = FALSE)

for (i in 1:200) {
    rikz_repl[[i]]$Richness &lt;- prior_pred[i, ] 
}</code></pre>
<p>Ensuite, nous appelons la fonction <code>brm_multiple</code> avec notre modèle et cette liste de jeux de données. Nous réduisons le nombre de chaînes de Markov à 2 et spécifions le paramètre de contrôle <code>adapt_delpth = 0.99</code>, car nous avions observé au préalable que la valeur par défaut (<code>adapt_delta = 0.8</code>) génère plusieurs divergences. Finalement, <code>combine = FALSE</code> assure que <code>brm_multiple</code> produise une liste de 200 objets pour les résultats de chaque modèle, plutôt que de les combiner en un seul objet.</p>
<p>Nous n’avons pas spécifié le nombre d’itérations par chaîne, donc <em>brms</em> utilise par défaut 2000 itérations, dont 1000 dans la période de rodage. Avec deux chaînes, l’échantillon de la distribution <em>a posteriori</em> sera donc composé de 2000 valeurs de chaque paramètre par simulation.</p>
<pre class="r"><code>res_test &lt;- brm_multiple(Richness ~ NAP + Exposure + (1 | Beach), 
                         data = rikz_repl, family = poisson,
                         chains = 2, control = list(adapt_delta = 0.99),
                         prior = rikz_prior, combine = FALSE)</code></pre>
<p>L’ajustement des 200 modèles prend un certain temps et il est possible que Stan donne des avertissements en lien avec les diagnostics pour chaque modèle. Nous allons maintenant vérifier combien de modèles contiennent des transitions divergentes ou qui ont atteint la profondeur maximale de l’arbre. La fonction <code>nuts_params</code> produit les valeurs des diagnostics pour chaque itération d’un modèle ajusté. Ici, puisque nous avons une liste de 200 résultats, nous utilisons <code>lapply</code> pour appliquer cette fonction à chaque élément de la liste: <code>diags</code> contient donc une liste de 200 jeux de données.</p>
<pre class="r"><code>diags &lt;- lapply(res_test, nuts_params)</code></pre>
<p>Nous pouvons combiner ces jeux de données en les plaçant bout à bout avec la fonction <code>bind_rows</code> de <em>dplyr</em>. L’argument <code>.id</code> de cette fonction crée une colonne (ici <code>sim_id</code>) qui identifie l’élément de la liste originale dont provient chaque rangée. Puisque les éléments de la liste n’ont pas de noms, cet identifiant est ici le numéro entre 1 et 200.</p>
<pre class="r"><code>diags &lt;- bind_rows(diags, .id = &quot;sim_id&quot;)
head(diags)</code></pre>
<pre><code>##   sim_id Iteration     Parameter     Value Chain
## 1      1         1 accept_stat__ 0.9996518     1
## 2      1         2 accept_stat__ 0.9945361     1
## 3      1         3 accept_stat__ 0.9878070     1
## 4      1         4 accept_stat__ 0.9948456     1
## 5      1         5 accept_stat__ 0.9973272     1
## 6      1         6 accept_stat__ 0.9981608     1</code></pre>
<p>Plutôt que deux colonnes indiquant le paramètre et sa valeur, nous voudrions une colonne par paramètre, donc il faut faire pivoter les données avec <code>pivot_wider</code>.</p>
<pre class="r"><code>diags &lt;- pivot_wider(diags, names_from = Parameter, values_from = Value)
head(diags)</code></pre>
<pre><code>## # A tibble: 6 x 9
##   sim_id Iteration Chain accept_stat__ stepsize__ treedepth__ n_leapfrog__
##   &lt;chr&gt;      &lt;int&gt; &lt;int&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;
## 1 1              1     1         1.000     0.0746           5           63
## 2 1              2     1         0.995     0.0746           5           31
## 3 1              3     1         0.988     0.0746           5           31
## 4 1              4     1         0.995     0.0746           6           63
## 5 1              5     1         0.997     0.0746           6           63
## 6 1              6     1         0.998     0.0746           6           63
## # ... with 2 more variables: divergent__ &lt;dbl&gt;, energy__ &lt;dbl&gt;</code></pre>
<p>Chaque rangée indique le numéro de la simulation, de la chaîne et de l’itération dans cette chaîne, ainsi que les valeurs de six paramètres de l’algorithme. Nous nous intéressons surtout à <code>divergent__</code>, qui indique si la transition était divergente (1) ou non (0), puis à la profondeur de l’arbre <code>treedepth__</code>. Avec <code>summarize</code>, nous comptons le nombre de transitions divergentes par simulation et le nombre d’itérations ayant atteint la profondeur maximale de l’arbre (qui est de 10 par défaut). Nous joignons aussi à ce résultat le tableau <code>prior_params</code> contenant les paramètres de chaque simulation afin de vérifier si certaines valeurs des paramètres sont associées aux problèmes identifiés par les diagnostics.</p>
<pre class="r"><code>diags &lt;- group_by(diags, sim_id) %&gt;%
    summarize(div = sum(divergent__), maxtree = sum(treedepth__ == 10)) %&gt;%
    inner_join(prior_params)</code></pre>
<pre><code>## Joining, by = &quot;sim_id&quot;</code></pre>
<p>Puisqu’il ne s’agit pas de nos vraies données, mais de simulations, nous ne cherchons pas à éliminer tous les diagnostics problématiques, mais plutôt à voir généralement si l’estimation est difficile pour certaines plages de valeurs des paramètres <em>a priori</em>.</p>
<p>En inspectant ce jeu de données, nous notons que 2 simulations contiennent 1 ou 2 transitions divergentes (respectivement), tandis qu’une dizaine de simulations ont parfois atteint la profondeur maximale de l’arbre. Les simulations où de nombreuses itérations atteignent la profondeur maximale semblent être caractérisées par une valeur élevée de l’ordonnée à l’origine ou de l’écart-type des effets aléatoires, mais sinon il n’y a pas de patron particulier à ces résultats.</p>
<pre class="r"><code>filter(diags, maxtree &gt; 0 | div &gt; 0) %&gt;%
  arrange(desc(div), desc(maxtree)) %&gt;%
  select(div, maxtree, b_Intercept, sd_Beach__Intercept)</code></pre>
<pre><code>## # A tibble: 13 x 4
##      div maxtree b_Intercept sd_Beach__Intercept
##    &lt;dbl&gt;   &lt;int&gt;       &lt;dbl&gt;               &lt;dbl&gt;
##  1     2       0       2.98               0.0807
##  2     1       0       2.90               0.0831
##  3     0     459       4.73               0.884 
##  4     0     305       2.37               1.20  
##  5     0      43       1.50               0.656 
##  6     0      43       2.48               0.851 
##  7     0       7       1.97               1.37  
##  8     0       7       2.85               0.635 
##  9     0       4       1.36               0.859 
## 10     0       3       2.37               1.20  
## 11     0       2       3.13               0.730 
## 12     0       2       0.869              0.515 
## 13     0       2       2.86               1.02</code></pre>
</div>
<div id="test-de-calibration" class="section level2">
<h2>Test de calibration</h2>
<p>Pour ce test, nous voulons vérifier si la position de la vraie valeur du paramètre utilisée pour une simulation est distribuée uniformément parmi les valeurs obtenues pour la distribution <em>a posteriori</em> estimée à partir de cette simulation. Pour chaque résultat dans notre liste, nous pourrions extraire ces distributions <em>a posteriori</em> avec <code>posterior_samples</code>, qui produit un tableau de 2000 itérations x 15 paramètres (14 paramètres plus le log de la probabilité conjointe).</p>
<pre class="r"><code>test_params &lt;- lapply(res_test, posterior_samples)</code></pre>
<p>Cependant, le test de calibration est basé sur un échantillon indépendant, donc nous sous-échantillonnons les résultats en prenant une valeur à chaque 5 itérations, pour un total de 399 valeurs entre les itérations 5 et 1995. Comme auparavant, nous combinons les résultats des 200 simulations avec <code>bind_rows</code> en ajoutant une colonne identifiant la simulation d’origine.</p>
<pre class="r"><code>test_params &lt;- lapply(res_test, function(x) posterior_samples(x)[seq(5, 1995, 5),])
test_params &lt;- bind_rows(test_params, .id = &quot;sim_id&quot;)</code></pre>
<p>Nous utilisons <code>bind_rows</code> pour combiner ces distributions <em>a posteriori</em> avec les valeurs des paramètres <em>a priori</em> utilisées pour les simulations, contenues dans le tableau <code>prior_params</code>. Avec <code>id = "type"</code>, nous créons une colonne qui indique s’il s’agit de la vraie valeur du paramètre (<code>prior</code>) ou d’une valeur tirée de la distribution <em>a posteriori</em> (<code>posterior</code>). Finalement, nous appliquons un pivot pour obtenir 4 colonnes: le type de valeur, le numéro de simulation, le paramètre et sa valeur.</p>
<pre class="r"><code>test_params &lt;- bind_rows(prior = prior_params, posterior = test_params, .id = &quot;type&quot;)
test_params &lt;- pivot_longer(test_params, cols = -c(sim_id, type), 
                            names_to = &quot;param&quot;, values_to = &quot;value&quot;)
head(test_params)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   type  sim_id param                  value
##   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt;
## 1 prior 1      b_Intercept           1.11  
## 2 prior 1      b_NAP                -0.0686
## 3 prior 1      b_Exposure10         -0.441 
## 4 prior 1      b_Exposure11          0.265 
## 5 prior 1      sd_Beach__Intercept   0.302 
## 6 prior 1      r_Beach[1,Intercept]  0.342</code></pre>
<p>Pour chaque simulation et chaque paramètre, <code>test_params</code> contient 400 valeurs: la valeur <em>a priori</em> du paramètre qui a été utilisée pour simuler les données, puis 399 valeurs de la distribution <em>a posteriori</em>.</p>
<p>Il nous reste donc à grouper les valeurs par simulation et paramètre, déterminer le rang des 400 valeurs (avec <code>mutate</code>), puis conserver seulement les rangs des valeurs originales des paramètres (<code>type == "prior"</code>), qui devraient être distribués uniformément entre 1 et 400 si le modèle est bien calibré.</p>
<p><em>Note</em>: Nous éliminons la colonne <code>lp__</code> car la log-probabilité n’est pas un paramètre du modèle et n’est surtout pas comparable entre les distributions <em>a priori</em> et <em>a posteriori</em>.</p>
<pre class="r"><code>calib &lt;- group_by(test_params, sim_id, param) %&gt;%
  mutate(rank = rank(value)) %&gt;%
  filter(type == &quot;prior&quot;, param != &quot;lp__&quot;) %&gt;%
  ungroup()
head(calib)</code></pre>
<pre><code>## # A tibble: 6 x 5
##   type  sim_id param                  value  rank
##   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;
## 1 prior 1      b_Intercept           1.11     332
## 2 prior 1      b_NAP                -0.0686   145
## 3 prior 1      b_Exposure10         -0.441     64
## 4 prior 1      b_Exposure11          0.265     42
## 5 prior 1      sd_Beach__Intercept   0.302    183
## 6 prior 1      r_Beach[1,Intercept]  0.342    366</code></pre>
<p>Pour tester l’uniformité des rangs pour chaque paramètre, nous groupons les rangs en 10 classes avec un histogramme : 1 à 40, 41 à 80, etc. (Il est important de spécifier les limites des classes manuellement avec <code>breaks</code>.) En théorie, le nombre d’observations par classe est donné par une distribution binomiale <span class="math inline">\(\text{Bin}(N = 200, p = 0.1)\)</span>, dont la moyenne est 20 (ligne pointillée) et pour laquelle 99% de la probabilité se retrouve dans la zone grise.</p>
<pre class="r"><code>ggplot(calib, aes(x = rank)) +
    geom_rect(ymin = qbinom(0.005, 200, 0.1), ymax = qbinom(0.995, 200, 0.1),
              xmin = -40, xmax = 440, color = &quot;white&quot;, fill = &quot;grey80&quot;) +
    geom_hline(yintercept = 20, linetype = &quot;dashed&quot;) +
    geom_histogram(breaks = seq(0.5, 400.5, 40), fill = &quot;orange&quot;, color = &quot;white&quot;) +
    facet_wrap(~ param)</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>La distribution semble uniforme et sur 140 barres (14 paramètres x 10 classes), seules 2 ou 3 sont hors de l’intervalle à 99%.</p>
</div>
<div id="test-de-sensibilite" class="section level2">
<h2>Test de sensibilité</h2>
<p>Vérifions maintenant si 45 observations suffisent à réaliser une estimation précise des paramètres pour chaque simulation.</p>
<p>La fonction <code>posterior_summary</code> de <em>brms</em> produit le sommaire des distributions <em>a posteriori</em> de chaque paramètre, incluant la moyenne <code>Estimate</code> et l’écart-type <code>Est.Error</code>.</p>
<pre class="r"><code>posterior_summary(res_test[[1]])</code></pre>
<pre><code>##                           Estimate Est.Error          Q2.5        Q97.5
## b_Intercept             0.73804309 0.3812837   -0.01103903    1.5237741
## b_NAP                  -0.03361925 0.0889227   -0.20693122    0.1288139
## b_Exposure10            0.01350677 0.4361814   -0.85850562    0.8661791
## b_Exposure11            0.74411153 0.4199943   -0.14942175    1.5425583
## sd_Beach__Intercept     0.33599318 0.1520600    0.09363625    0.6933657
## r_Beach[1,Intercept]   -0.01540107 0.2678292   -0.57347713    0.5171688
## r_Beach[2,Intercept]   -0.09866021 0.3071263   -0.80956364    0.4577313
## r_Beach[3,Intercept]   -0.27225569 0.2568916   -0.81579188    0.1746308
## r_Beach[4,Intercept]    0.40803088 0.2527743   -0.01824350    0.9602471
## r_Beach[5,Intercept]    0.09984275 0.2611922   -0.41250694    0.6686724
## r_Beach[6,Intercept]   -0.12874140 0.2371614   -0.62286951    0.3433624
## r_Beach[7,Intercept]    0.03781016 0.2324461   -0.39633465    0.5317407
## r_Beach[8,Intercept]    0.05143366 0.2660621   -0.47575466    0.6039337
## r_Beach[9,Intercept]   -0.18864904 0.2779928   -0.80830224    0.2940953
## lp__                 -105.25568957 3.1047764 -112.14681150 -100.1294588</code></pre>
<p>Le format de ce résultat est une matrice où les noms des rangées indiquent le paramètre. Nous créons une fonction qui convertit ce résultat en tableau de données, ajoute les noms de rangées comme colonne du tableau et renomme cette colonne <code>param</code>. Nous pouvons ensuite appliquer cette fonction à chaque simulation et combiner les résultats avec <code>bind_rows</code> (vous pouvez ignorer les avertissements selon lesquels la fonction <code>add_rownames</code> est désuète).</p>
<pre class="r"><code>get_post_sum &lt;- function(x) {
    posterior_summary(x) %&gt;%
        as.data.frame() %&gt;%
        add_rownames() %&gt;%
        rename(param = rowname)
}

post_sum &lt;- bind_rows(lapply(res_test, get_post_sum), .id = &quot;sim_id&quot;)
head(post_sum)</code></pre>
<pre><code>## # A tibble: 6 x 6
##   sim_id param                Estimate Est.Error    Q2.5 Q97.5
##   &lt;chr&gt;  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
## 1 1      b_Intercept            0.738     0.381  -0.0110 1.52 
## 2 1      b_NAP                 -0.0336    0.0889 -0.207  0.129
## 3 1      b_Exposure10           0.0135    0.436  -0.859  0.866
## 4 1      b_Exposure11           0.744     0.420  -0.149  1.54 
## 5 1      sd_Beach__Intercept    0.336     0.152   0.0936 0.693
## 6 1      r_Beach[1,Intercept]  -0.0154    0.268  -0.573  0.517</code></pre>
<p>Il reste à ajouter à ce tableau les vraies valeurs des paramètres utilisées pour simuler les données. Pour ce faire, nous faisons un pivot du jeu de données <code>prior_params</code>, puis nous joignons les deux jeux de données.</p>
<pre class="r"><code>prior_params &lt;- pivot_longer(prior_params, cols = -sim_id, 
                             names_to = &quot;param&quot;, values_to = &quot;true_val&quot;)
head(prior_params)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   sim_id param                true_val
##   &lt;chr&gt;  &lt;chr&gt;                   &lt;dbl&gt;
## 1 1      b_Intercept            1.11  
## 2 1      b_NAP                 -0.0686
## 3 1      b_Exposure10          -0.441 
## 4 1      b_Exposure11           0.265 
## 5 1      sd_Beach__Intercept    0.302 
## 6 1      r_Beach[1,Intercept]   0.342</code></pre>
<pre class="r"><code>post_sum &lt;- inner_join(post_sum, prior_params)</code></pre>
<pre><code>## Joining, by = c(&quot;sim_id&quot;, &quot;param&quot;)</code></pre>
<pre class="r"><code>head(post_sum)</code></pre>
<pre><code>## # A tibble: 6 x 7
##   sim_id param                Estimate Est.Error    Q2.5 Q97.5 true_val
##   &lt;chr&gt;  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1 1      b_Intercept            0.738     0.381  -0.0110 1.52    1.11  
## 2 1      b_NAP                 -0.0336    0.0889 -0.207  0.129  -0.0686
## 3 1      b_Exposure10           0.0135    0.436  -0.859  0.866  -0.441 
## 4 1      b_Exposure11           0.744     0.420  -0.149  1.54    0.265 
## 5 1      sd_Beach__Intercept    0.336     0.152   0.0936 0.693   0.302 
## 6 1      r_Beach[1,Intercept]  -0.0154    0.268  -0.573  0.517   0.342</code></pre>
<p>Finalement, nous calculons la cote <span class="math inline">\(z\)</span> pour chaque estimé, soit la différence entre la moyenne et la vraie valeur du paramètre, divisée par l’écart-type; puis le facteur de contraction, soit 1 - le rapport entre les variances <em>a posteriori</em> et <em>a priori</em> des paramètres (la variance <em>a priori</em> est de 1 pour les effets fixes et de 0.09 pour <code>sd_Beach__Intercept</code>, cette dernière calculée à partir de la distribution normale tronquée).</p>
<pre class="r"><code>post_sum &lt;- filter(post_sum, param != &quot;lp__&quot;) %&gt;%
    mutate(zscore = (Estimate - true_val) / Est.Error,
           prior_var = ifelse(param == &quot;sd_Beach__Intercept&quot;, 0.09, 1),
           contr = 1 - Est.Error^2/prior_var)</code></pre>
<p>Le graphique suivant présente la distribution des cotes <span class="math inline">\(z\)</span> pour chaque paramètre. Celles-ci sont centrées sur 0 et la plupart des estimés sont à deux écarts-types de part et d’autre de la vraie valeur du paramètre (<span class="math inline">\(-2 &lt; z &lt; 2\)</span>), ce qui montre que l’écart-type de la distribution <em>a posteriori</em> représente bien l’incertitude sur la valeur de chaque paramètre.</p>
<pre class="r"><code>ggplot(post_sum, aes(x = zscore)) +
    geom_density() +
    facet_wrap(~ param)</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>Ensuite, nous regardons le facteur de contraction en fonction de la vraie valeur de chaque paramètre. Nous ne nous intéressons pas aux effets aléatoires de chaque plage pour cette comparaison, puisque leur écart-type <em>a priori</em> dépend de <code>sd_Beach__Intercept</code>.</p>
<pre class="r"><code>post_sum2 &lt;- filter(post_sum, 
                    param %in% c(&quot;b_Intercept&quot;, &quot;b_NAP&quot;, &quot;b_Exposure10&quot;,
                                 &quot;b_Exposure11&quot;, &quot;sd_Beach__Intercept&quot;))
ggplot(post_sum2, aes(x = true_val, y = contr)) +
    geom_point() +
    facet_wrap(~ param, scales = &quot;free_x&quot;)</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>Sauf pour <code>b_NAP</code>, nous remarquons que le facteur de contraction est souvent faible. Par exemple, un facteur de 0.5 signifie que les données ne permettent de réduire que de moitié l’incertitude par rapport à la distribution <em>a priori</em>. Selon le graphique pour <code>sd_Beach__Intercept</code>, plus l’écart-type des effets aléatoires est élevé, moins nous pouvons l’estimer précisément. Pour vérifier si c’est le cas pour les autres paramètres, nous ajoutons une autre colonne représentant la vraie valeur de <code>sd_Beach__Intercept</code> pour chaque simulation.</p>
<pre class="r"><code>sd_true &lt;- filter(post_sum, param == &quot;sd_Beach__Intercept&quot;) %&gt;%
    select(sim_id, sd_true = true_val)
post_sum &lt;- inner_join(post_sum, sd_true)</code></pre>
<pre><code>## Joining, by = &quot;sim_id&quot;</code></pre>
<pre class="r"><code>post_sum2 &lt;- filter(post_sum, 
                    param %in% c(&quot;b_Intercept&quot;, &quot;b_NAP&quot;, &quot;b_Exposure10&quot;,
                                 &quot;b_Exposure11&quot;, &quot;sd_Beach__Intercept&quot;))
ggplot(post_sum2, aes(x = sd_true, y = contr)) +
    geom_point() +
    facet_wrap(~ param, scales = &quot;free_x&quot;)</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>En effet, nous constatons que plus la variance entre les plages est grande, plus nos estimés sont imprécis.</p>
</div>
<div id="ajustement-du-modele-aux-observations" class="section level2">
<h2>Ajustement du modèle aux observations</h2>
<p>Nous sommes maintenant prêts à ajuster le modèle aux observations réelles.</p>
<pre class="r"><code>res_br &lt;- brm(Richness ~ NAP + Exposure + (1 | Beach), data = rikz, 
              family = poisson, control = list(adapt_delta = 0.99), 
              chains = 2, prior = rikz_prior)</code></pre>
<pre class="r"><code>summary(res_br, mc_se = TRUE)</code></pre>
<pre><code>##  Family: poisson 
##   Links: mu = log 
## Formula: Richness ~ NAP + Exposure + (1 | Beach) 
##    Data: rikz (Number of observations: 45) 
## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 2000
## 
## Group-Level Effects: 
## ~Beach (Number of levels: 9) 
##               Estimate MC.Error Est.Error l-95% CI u-95% CI Eff.Sample
## sd(Intercept)     0.23     0.01      0.14     0.02     0.55        487
##               Rhat
## sd(Intercept) 1.00
## 
## Population-Level Effects: 
##            Estimate MC.Error Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept      2.38     0.01      0.29     1.69     2.90        764 1.00
## NAP           -0.50     0.00      0.07    -0.65    -0.36       2032 1.00
## Exposure10    -0.47     0.01      0.33    -1.06     0.26        715 1.00
## Exposure11    -1.17     0.01      0.34    -1.76    -0.38        728 1.01
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Le sommaire des résultats indique d’abord la formule du modèle, puis les paramètres de l’algorithme (nombre de chaînes et d’itérations, nombre d’itérations de rodage). La section <em>Group-Level Effects</em> présente l’écart-type des effets aléatoires, tandis que la section <em>Population-Level Effects</em> présente les effets fixes. Chaque estimé indique la moyenne, l’écart-type et un intervalle de crédibilité à 95% pour la distribution <em>a posteriori</em>, ainsi que la statistique de Gelman-Rubin (<em>Rhat</em>) et la taille effective de l’échantillon Monte-Carlo (<code>Eff.Sample</code>).</p>
<p>En spécifiant <code>mc_se = TRUE</code>, nous obtenons aussi une colonne pour l’erreur-type de Monte-Carlo (<code>MC.Error</code>). Cette erreur d’approximation numérique devrait être négligeable par rapport à l’écart-type de la distribution <em>a posteriori</em> (<code>Est.Error</code>), ce qui est le cas ici.</p>
<p>En comparant ces résultats à ceux du GLMM classique, nous obtenons des différences entre les estimés moyens, mais ces différences sont raisonnables en considérant les marges d’erreurs de chaque paramètre.</p>
<pre class="r"><code>library(lme4)
res_glmm &lt;- glmer(Richness ~ NAP +Exposure + (1 | Beach), data = rikz, family = poisson)
summary(res_glmm)</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: poisson  ( log )
## Formula: Richness ~ NAP + Exposure + (1 | Beach)
##    Data: rikz
## 
##      AIC      BIC   logLik deviance df.resid 
##      210      219     -100      200       40 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.8080 -0.4947 -0.2078  0.2789  3.9801 
## 
## Random effects:
##  Groups Name        Variance Std.Dev.
##  Beach  (Intercept) 0.01138  0.1067  
## Number of obs: 45, groups:  Beach, 9
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  2.52472    0.16720  15.100  &lt; 2e-16 ***
## NAP         -0.50781    0.07128  -7.125 1.04e-12 ***
## Exposure10  -0.59863    0.19615  -3.052  0.00227 ** 
## Exposure11  -1.33491    0.21817  -6.119 9.43e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##            (Intr) NAP    Exps10
## NAP         0.078              
## Exposure10 -0.847 -0.027       
## Exposure11 -0.766 -0.063  0.653</code></pre>
<p>Il existe quelques raisons pour lesquelles les estimés de l’approche bayésienne diffèrent de ceux obtenus par le maximum de vraisemblance, notamment:</p>
<ul>
<li><p>la distribution <em>a priori</em> peut influencer l’inférence si le nombre d’observations est petit;</p></li>
<li><p>même dans le cas ou la distribution <em>a priori</em> a peu d’influence et que la distribution <em>a posteriori</em> prend la même forme de la fonction de vraisemblance, l’estimé bayésien est la moyenne de la distribution <em>a posteriori</em>, qui n’est pas nécessairement égal au maximum de la probabilité <em>a posteriori</em>, en particulier si la distribution est asymétrique.</p></li>
</ul>
<p>La fonction <code>stanplot</code> permet de visualiser différents résultats du modèle, notamment le tracé des chaînes de Markov, un histogramme des distributions <em>a posteriori</em>, ou une représentation des différents coefficients avec leurs intervalles de crédibilité</p>
<pre class="r"><code>stanplot(res_br, type = &quot;trace&quot;)</code></pre>
<pre><code>## No divergences to plot.</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<pre class="r"><code>stanplot(res_br, type = &quot;hist&quot;)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-44-2.png" width="672" /></p>
<pre class="r"><code>stanplot(res_br, type = &quot;intervals&quot;)</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-44-3.png" width="672" /></p>
<p>Notez que par défaut, les intervalles en traits gras représentent 50% de la probabilité <em>a posteriori</em> tandis que ceux en traits fins représentent 90% de cette probabilité.</p>
<p>Un outil interactif de visualisation des résultats et diagnostics du modèle peut être lancée avec la fonction <code>launch_shinystan</code>.</p>
<pre class="r"><code>launch_shinystan(res_br)</code></pre>
</div>
<div id="verification-des-predictions-a-posteriori" class="section level2">
<h2>Vérification des prédictions <em>a posteriori</em></h2>
<p>Comme au dernier cours, nous utilisons <code>pp_check</code> avec <code>type = "intervals"</code> pour comparer les observations aux intervalles de prédiction du modèle. En moyenne, nous nous attendons à ce que 50% des observations soient dans les intervalles en gras et 90% dans les intervalles en traits pâles.</p>
<pre class="r"><code>pp_check(res_br, type = &quot;intervals&quot;)</code></pre>
<pre><code>## Using all posterior samples for ppc type &#39;intervals&#39; by default.</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Pour une régression de Poisson, nous pouvons aussi vérifier que l’écart-type des observations et le nombre de zéros obtenus sont comparables aux valeurs prévues par le modèle ajusté.</p>
<pre class="r"><code>pp_check(res_br, type = &quot;stat&quot;, stat = sd)</code></pre>
<pre><code>## Using all posterior samples for ppc type &#39;stat&#39; by default.</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<pre class="r"><code>pp_check(res_br, type = &quot;stat&quot;, stat = function(x) sum(x == 0))</code></pre>
<pre><code>## Using all posterior samples for ppc type &#39;stat&#39; by default.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="09-Modeles_hierarchiques_bayesiens_files/figure-html/unnamed-chunk-47-2.png" width="672" /></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
