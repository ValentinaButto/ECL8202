<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Generalized linear mixed models</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Generalized linear mixed models</h1>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Generalized linear mixed models combine the features of generalized linear models (modelling non-normally distributed variables, especially binary and count data) and linear mixed models (modelling grouped data). Here, we will first review concepts seen in the prerequisite course, before discussing the particularities of GLMM in terms of parameter estimation, model evaluation and comparison.</p>
<div id="contents" class="section level2">
<h2>Contents</h2>
<ul>
<li><p>Revision: generalized linear models and linear mixed models</p></li>
<li><p>Generalized linear mixed models (GLMM): mathematical form and estimation techniques</p></li>
<li><p>Evaluate the fit of a GLMM</p></li>
<li><p>Compare different versions of a GLMM</p></li>
<li><p>Predictions and simulations from a GLMM</p></li>
</ul>
</div>
</div>
<div id="generalized-linear-models" class="section level1">
<h1>Generalized linear models</h1>
<p>When using a linear regression model to explain a random variable <span class="math inline">\(y\)</span> as a function of predictors <span class="math inline">\(x_1, ..., x_m\)</span>, we assume both a specific relationship between the mean response and the predictors, as well as a specific distribution of the variation of <span class="math inline">\(y\)</span> around its mean. More precisely:</p>
<ul>
<li><p>the mean of <span class="math inline">\(y\)</span> is a linear function of the <span class="math inline">\(x_i\)</span>: <span class="math inline">\(\mu = \beta_0 + \sum_{i = 1}^m \beta_i x_i\)</span>; and</p></li>
<li><p><span class="math inline">\(y\)</span> follows a normal distribution of constant standard deviation around this mean: <span class="math inline">\(y \sim N(\mu, \sigma)\)</span>.</p></li>
</ul>
<p>Several variables measured in environmental science are poorly represented by this model, notably binary data (e.g. presence/absence, mortality/survival) or count data (e.g. number of individuals, number of species). On the one hand, a linear model of the mean does not include the constraints of these data: the mean probability of presence must be between 0 and 1; the mean number of individuals cannot be negative. On the other hand, the variance of these data is not constant: the presence of a species is more variable if the mean presence is 50% than if it is close to 0 or 1; the variance of count data tends to increase with the mean. Nor is it always possible to transform the data to approach normality and variance homogeneity sufficiently.</p>
<p>Generalized linear models (GLM) help to solve these problems. In a GLM, the linear predictor <span class="math inline">\(\eta\)</span> (linear combination of predictors) is related to the mean of the response by a link function <span class="math inline">\(g\)</span>:</p>
<p><span class="math display">\[g(\mu) = \eta = \beta_0 + \sum_{i = 1}^m \beta_i x_i\]</span></p>
<p>and different distributions can be used to represent the variation of <span class="math inline">\(y\)</span> relative to <span class="math inline">\(\mu\)</span>.</p>
<p>Linear regression is therefore an example of GLM where <span class="math inline">\(\mu = \eta\)</span> (identity link) and <span class="math inline">\(y\)</span> follows a normal distribution. Logistic regression, with a logit link and a binomial distribution of the response, is suitable for binary data, while Poisson regression, with a log link and a Poisson distribution, is suitable for count data. The following is a comparative table of those three models:</p>
<table>
<colgroup>
<col width="12%" />
<col width="25%" />
<col width="30%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Distribution</th>
<th>Default link</th>
<th>Inverse of link</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear regression</td>
<td>Normal: <span class="math inline">\(y \sim N(\mu, \sigma)\)</span></td>
<td>Identity: <span class="math inline">\(\mu = \eta\)</span></td>
<td><span class="math inline">\(\mu = \eta\)</span></td>
</tr>
<tr class="even">
<td>Logistic regression</td>
<td>Binomial: <span class="math inline">\(y \sim B(n, p)\)</span></td>
<td>Logit: <span class="math inline">\(\log(p/(1-p)) = \eta\)</span></td>
<td><span class="math inline">\(p = 1/(1+e^{-\eta})\)</span></td>
</tr>
<tr class="odd">
<td>Poisson regression</td>
<td>Poisson: <span class="math inline">\(y \sim Pois(\lambda)\)</span></td>
<td>Log: <span class="math inline">\(\log(\lambda) = \eta\)</span></td>
<td><span class="math inline">\(\lambda = e^{\eta}\)</span></td>
</tr>
</tbody>
</table>
<div id="poisson-regression" class="section level2">
<h2>Poisson regression</h2>
<p>The Poisson distribution can be used to represent a response <span class="math inline">\(y\)</span> that takes integer values greater than or equal to 0. Theoretically, this distribution represents the number of events observed in a given interval (temporal or spatial), when the events are independent of each other.</p>
<p>For example, if <span class="math inline">\(y\)</span> is the number of customers entering a store during a given one-hour period each day, assuming that each person acts independently, then <span class="math inline">\(y\)</span> could follow a Poisson distribution.</p>
<p>This distribution contains a single adjustable parameter, <span class="math inline">\(\lambda\)</span>, which is both the mean and variance of <span class="math inline">\(y\)</span>.</p>
<p><span class="math display">\[P(y | \lambda) = \frac{\lambda^y}{y!} e^{-\lambda}\]</span></p>
<p>As we can see in the graph below, for a small <span class="math inline">\(\lambda\)</span>, the distribution is more skewed (since <span class="math inline">\(y\)</span> cannot be less than zero); as <span class="math inline">\(\lambda\)</span> increases, the distribution approaches symmetry and a normal shape.</p>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Poisson regression most often uses a logarithmic link:</p>
<p><span class="math display">\[\log{\lambda} = \beta_0 + \sum_{i = 1}^m \beta_i x_i\]</span></p>
<p>Inverting this link, we find that <span class="math inline">\(\lambda\)</span> is the exponential of the linear predictor. This ensures that <span class="math inline">\(\lambda\)</span> is always positive. Since <span class="math inline">\(e^0 = 1\)</span>, a negative value of the linear predictor corresponds to <span class="math inline">\(\lambda &lt; 1\)</span> and a positive value to <span class="math inline">\(\lambda &gt; 1\)</span>.</p>
<p><span class="math display">\[\lambda = e^{\beta_0 + \sum_{i = 1}^m \beta_i x_i}\]</span></p>
<p>Also, since the exponential transforms additive effects into multiplicative effects:</p>
<p><span class="math display">\[\lambda = e^{\beta_0} e^{\beta_1 x_1} e^{\beta_2 x_2} \ldots\]</span></p>
<p>we can interpret the effect of each predictor separately. For example, if <span class="math inline">\(x_1\)</span> increases by 1, then the mean of the response is multiplied by <span class="math inline">\(e^{\beta_1}\)</span>.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
<p>Suppose a binary response is coded 0/1 (e.g. absence/presence, failure/success). If <span class="math inline">\(y\)</span> is the number of positive responses (1) among <span class="math inline">\(n\)</span> independent replicates that share the same probability <span class="math inline">\(p\)</span> of obtaining a positive response, then <span class="math inline">\(y\)</span> follows a binomial distribution <span class="math inline">\(Bin(n, p)\)</span>.</p>
<p><span class="math display">\[P(y \vert n, p) = \binom{n}{y} p^y(1-p)^{n-y}\]</span></p>
<p>The mean of <span class="math inline">\(y\)</span> equals <span class="math inline">\(np\)</span> and the variance equals <span class="math inline">\(np(1-p)\)</span>. In practice, this means that the variance is maximal for $p = $0.5 and decreases as <span class="math inline">\(p\)</span> approaches 0 or 1.</p>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>In a regression context, <span class="math inline">\(n\)</span> is known and we try to estimate how <span class="math inline">\(p\)</span> varies according to the predictors.</p>
<p>Often, <span class="math inline">\(n = 1\)</span>, that is, we model individual observations of the binary outcome as a function of environmental conditions. Cases where <span class="math inline">\(n &gt; 1\)</span> are often controlled experiments. For example, if we want to determine the probability of seed germination as a function of soil moisture, we could plant a group of <span class="math inline">\(n = 20\)</span> seeds for each moisture value; the response <span class="math inline">\(y\)</span> would be the number of germinations observed out of 20.</p>
<p>Logistic regression gets its name from the fact that a logistic function is used to transform the linear predictor <span class="math inline">\(\eta\)</span> into a probability <span class="math inline">\(p\)</span> between 0 and 1.</p>
<p><span class="math display">\[p = \frac{1}{1 + e^{-\eta}}\]</span></p>
<p>This function takes a value of 0.5 if <span class="math inline">\(\eta = 0\)</span> and approaches 0 and 1 (without ever reaching them) for very negative and positive values of <span class="math inline">\(\eta\)</span>, respectively.</p>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-3-1.png" width="384" /></p>
<p>The inverse of the logistic function is the logit link:</p>
<p><span class="math display">\[\eta = \text{logit}(p) = \log \left( \frac{p}{1-p} \right)\]</span></p>
<p>Due to the non-linear form of the logistic function, the effect of each predictor on the probability <span class="math inline">\(p\)</span> is not constant. This effect is maximal around <span class="math inline">\(p = 0.5\)</span>. In other words, the closer we are to conditions where the probabilities of positive and negative responses are equal, the more sensitive this probability is to a variation in the predictors.</p>
<p><span class="math display">\[p = \frac{1}{1 + e^{-(\beta_0 + \sum_{i = 1}^m \beta_i x_i)}}\]</span></p>
<p>It can be shown that the maximum slope of <span class="math inline">\(p\)</span> as a function of a predictor <span class="math inline">\(x_i\)</span>, when <span class="math inline">\(p = 0.5\)</span>, is equal to <span class="math inline">\(\beta_i / 4\)</span>.</p>
<p>For example, the graph below shows <span class="math inline">\(p\)</span> vs. <span class="math inline">\(x\)</span> for a logistic model where <span class="math inline">\(\text{logit}(p) = -1 + 0.4x\)</span>.</p>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-4-1.png" width="384" /></p>
<p>The value of <span class="math inline">\(x\)</span> for which <span class="math inline">\(p = 0.5\)</span> is the solution to the equation <span class="math inline">\(-1 + 0.4x = 0\)</span>, so <span class="math inline">\(x = 2.5\)</span>. The slope of <span class="math inline">\(p\)</span> vs. <span class="math inline">\(x\)</span> around this point (shown in blue) is <span class="math inline">\(0.4/4 = 0.1\)</span>.</p>
</div>
<div id="generalized-linear-models-in-r" class="section level2">
<h2>Generalized linear models in R</h2>
<p>In R, we use the <code>glm</code> function to fit a generalized linear model. As with <code>lm</code>, we specify a formula of the form <code>response ~ predictors</code> and a dataset <code>data</code> where the variables are found; in addition, <code>glm</code> requires us to specify the family of distributions used (e.g. <code>binomial</code> or <code>poisson</code>).</p>
<pre class="r"><code>glm(y ~ x1 + x2 + ..., data = ..., family = binomial)</code></pre>
<p>We could also specify the link function: <code>family = binomial(link = "logit")</code>, but this is not necessary if we use the default link (logit for binomial, log for Poisson).</p>
<p>The above code applies to logistic regression if the response variable <code>y</code> contains binary values (0 or 1). If each row summarizes several binary outcomes, then the variables counting the number of positive and negative outcomes, e.g. <code>pos</code> and <code>neg</code>, must be specified as follows:</p>
<pre class="r"><code>glm(cbind(pos, neg) ~ x1 + x2 + ..., data = ..., family = binomial)</code></pre>
</div>
<div id="overdispersion" class="section level2">
<h2>Overdispersion</h2>
<p>In a linear regression, the residual variance <span class="math inline">\(\sigma^2\)</span> is the same for all observations and is estimated independently of the mean trend. For generalized linear models with Poisson or binomial distribution, the variance depends on the mean value (i.e. the predictors for each observation) and this relationship is fixed by the distribution. Thus, the variance is always equal to <span class="math inline">\(\lambda\)</span> (Poisson) or <span class="math inline">\(np(1-p)\)</span> (binomial).</p>
<p>By fitting a generalized linear model, it is therefore possible that the mean trend is well represented by the model, but that the residual variance exceeds that predicted by the theoretical distribution. In the graph below, the green histograms represent a Poisson distribution with <span class="math inline">\(\lambda = 5\)</span> (left) and a binomial distribution with <span class="math inline">\(n = 15\)</span> and <span class="math inline">\(p = 0.3\)</span> (right). The histograms in orange represent distributions with the same mean, but with overdispersion.</p>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><em>Note</em>: In the case of a logistic regression where the response is binary (i.e. binomial with <span class="math inline">\(n = 1\)</span>), there can be no overdispersion.</p>
<p>Later in the course, we will discuss methods to identify overdispersion and alternative models for overdispersed data.</p>
</div>
</div>
<div id="linear-mixed-models" class="section level1">
<h1>Linear mixed models</h1>
<p>Consider a simple linear regression for <span class="math inline">\(n\)</span> observations of a response variable <span class="math inline">\(y\)</span> and a predictor <span class="math inline">\(x\)</span>. According to this model, the observation <span class="math inline">\(y_k\)</span> (for <span class="math inline">\(k = 1, 2, ..., n\)</span>) follows a normal distribution <span class="math inline">\(N(\mu_k, \sigma_y)\)</span> with a mean <span class="math inline">\(\mu_k = \beta_0 + \beta_1 x_k\)</span>.</p>
<p>Now suppose that the <span class="math inline">\(n\)</span> observations are grouped together. For example, they could be sampling points spread over a few separate sites; a survey of members of different communities; or repeated measurements of the same individuals at different times. In all of these cases, we expect that the residual variation in response (unexplained by the predictors) will not be independent from one observation to the next. In particular, observations from the same group tend to be more similar than observations from different groups, due to unmeasured factors that vary at the group level rather than at the individual observation level.</p>
<p>A linear mixed model represents this situation by allowing the coefficients of the linear model to vary from one group to another, according to a normal distribution. In the previous model, if <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> vary from group to group and <span class="math inline">\(j[k]\)</span> denotes the group <span class="math inline">\(j\)</span> containing the observation <span class="math inline">\(k\)</span>, then the mean value of this observation in the mixed model is equal to:</p>
<p><span class="math display">\[\mu_k = \beta_{0j[k]} + \beta_{1j[k]} x_k\]</span></p>
<p>In this model, <span class="math inline">\(y_k\)</span> follows a normal distribution:</p>
<p><span class="math display">\[y_k \sim N(\mu_k, \sigma_y)\]</span></p>
<p>and so do the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. For example, for the intercept <span class="math inline">\(\beta_0\)</span>:</p>
<p><span class="math display">\[\beta_{0j} \sim N(\mu_{\beta_0}, \sigma_{\beta_0})\]</span></p>
<p>Mixed models get their name from the fact that they combine fixed effects specified by predictors such as <span class="math inline">\(x\)</span> with random effects representing the variation between groups. Fitting a linear mixed model would allow us to estimate the mean of the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the standard deviation of these coefficients across groups, as well as <span class="math inline">\(\sigma_y\)</span>, the standard deviation of individual observations from their group means.</p>
<p>In addition, the mixed model produces estimates of the coefficients for each group, here <span class="math inline">\(\beta_{0j}\)</span> and <span class="math inline">\(\beta_{1j}\)</span>. A model with a group fixed effect that interacts with <span class="math inline">\(x\)</span> also produces estimates of the intercept and slope of <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x\)</span> for each group. However, such <em>fixed</em> effects are estimated independently based on the data in each group, whereas the <em>random</em> effects of the mixed model are derived from a distribution centred on the mean value of all groups.</p>
<p>Specifically, the mixed model “shrinks” the effects of each group towards the mean effect, as can be seen in the graph below, where each color represents a different group and the regression lines are estimated for random (solid lines) or fixed (dashes) effects at the group level. The slopes of the solid lines are more similar to each other than the slopes of the dashed lines because they are assumed to come from a common distribution.</p>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The shrinkage effect is based on the idea that some of the observed differences between groups are due to random sampling rather than actual differences between populations. In particular, more shrinkage occurs when there are few observations in the group, consistent with the fact that a larger portion of the difference is due to chance in the case of a small sample.</p>
<p>As will be discussed later, modelling group random effects also allow us to predict the mean response and its uncertainty for a new group that was absent from the data used to fit the model.</p>
<p>Finally, another advantage of mixed models is that we can include both a random group effect and the effect of a predictor that varies at the group level. For example, the variation in the intercept <span class="math inline">\(\beta_0\)</span> between groups may depend on the value of a predictor <span class="math inline">\(u\)</span>:</p>
<p><span class="math display">\[\beta_{0j} \sim N(\gamma_0 + \gamma_1 u_j, \sigma_{\beta_0})\]</span></p>
<p>Since variation in the response is modelled at several levels (group and individual observation), mixed models are also called “hierarchical models”.</p>
<p>For example, suppose we measure plant biodiversity in quadrats located on different disturbed sites. Here, the quadrats are thus grouped by site. In this case, an example of a predictor <span class="math inline">\(u\)</span> defined at the group level would be the intensity of disturbance at a site, while the predictors <span class="math inline">\(x_1, x_2, ...\)</span> at the level of individual observations would represent measurements taken in each quadrat.</p>
<p>In summary, mixed models are particularly useful if one or more of the following conditions apply:</p>
<ul>
<li><p>the data are grouped or have a hierarchical structure with two or more levels (e.g. plots grouped by sites grouped by region);</p></li>
<li><p>the explanatory variables are also defined at multiple levels;</p></li>
<li><p>the number of groups is too large, or the number of observations in some groups is too small, to estimate a separate effect for each group;</p></li>
<li><p>there is more interest in the variation between groups than in the effect of particular groups;</p></li>
<li><p>there is a desire to apply the model to groups where no measurement has been taken.</p></li>
</ul>
<div id="mixed-linear-models-in-r" class="section level2">
<h2>Mixed linear models in R</h2>
<p>In this course, we will use the <em>lme4</em> package to fit mixed models. The <code>lmer</code> function of this package estimates the parameters of a linear mixed model. The formulas used by <code>lmer</code> follow the form <code>response ~ predictors</code>, with a specific syntax for random effects.</p>
<p>In the following example, <code>g</code> is the variable containing the group identifiers in the data frame <code>df</code>. The term <code>(1 + x | g)</code> tells the function to model a random effect of the group <code>g</code> for the intercept (noted by “1”) and the coefficient of <code>x</code>. If only the intercept varied per group, that is, if the slope of <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x\)</span> was set to a single value for all groups, we could write <code>(1 | g)</code>.</p>
<pre class="r"><code>library(lme4)

lmer(y ~ x + u + (1 + x | g), data = df)</code></pre>
<p>Note that predictors defined at the group level (such as <code>u</code>) appear in the formula like any other predictor.</p>
</div>
</div>
<div id="generalized-linear-mixed-models" class="section level1">
<h1>Generalized linear mixed models</h1>
<p>Generalized linear mixed models (abbreviated GLMM) combine the features of both types of models seen above.</p>
<ul>
<li>As in generalized linear models, different distributions are possible for the response <span class="math inline">\(y\)</span> and the mean of <span class="math inline">\(y\)</span> is related to the linear predictor by a link function:</li>
</ul>
<p><span class="math display">\[g(\mu) = \eta = \beta_0 + \sum_{i = 1}^m \beta_i x_i\]</span></p>
<ul>
<li>As with linear mixed models, the coefficients of the linear predictor vary randomly between groups. Note that this variation always follows a normal distribution.</li>
</ul>
<div id="example" class="section level2">
<h2>Example</h2>
<p>The <a href="../donnees/rikz.csv">rikz.csv</a> dataset, from the textbook of Zuur et al. (see references at the end), presents data on benthic communities for 9 beaches in the Netherlands. Species richness was measured at 5 sites on each of the 9 beaches for a total of 45 observations. The variable <code>NAP</code> measures the vertical position of each site with respect to mean sea level, while the wave exposure index (<code>Exposure</code>) is measured at the scale of a beach.</p>
<pre class="r"><code>rikz &lt;- read.csv(&quot;../donnees/rikz.csv&quot;)
# Convert Beach and Exposure to categorical variables (factors)
rikz &lt;- mutate(rikz, Beach = as.factor(Beach), 
               Exposure = as.factor(Exposure))
head(rikz)</code></pre>
<pre><code>##   Sample Richness Exposure    NAP Beach
## 1      1       11       10  0.045     1
## 2      2       10       10 -1.036     1
## 3      3       13       10 -1.336     1
## 4      4       11       10  0.616     1
## 5      5       10       10 -0.684     1
## 6      6        8        8  1.190     2</code></pre>
<p>Since species richness represents the species count at a site, we can model this response by a Poisson regression, with a fixed effect of the <code>NAP</code> and a random effect of the beach on both coefficients.</p>
<p>The <em>lme4</em> package contains a <code>glmer</code> function to estimate the parameters of a GLMM. This is similar to <code>lmer</code>, except that we specify the non-normal distribution of the response through the <code>family</code> parameter.</p>
<pre class="r"><code>glmm_res &lt;- glmer(Richness ~ NAP + (1 + NAP | Beach), data = rikz, family = poisson)
summary(glmm_res)</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: poisson  ( log )
## Formula: Richness ~ NAP + (1 + NAP | Beach)
##    Data: rikz
## 
##      AIC      BIC   logLik deviance df.resid 
##    218.7    227.8   -104.4    208.7       40 
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -1.35846 -0.51129 -0.21846  0.09802  2.45384 
## 
## Random effects:
##  Groups Name        Variance Std.Dev. Corr
##  Beach  (Intercept) 0.2630   0.5128       
##         NAP         0.0891   0.2985   0.18
## Number of obs: 45, groups:  Beach, 9
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   1.6942     0.1868   9.071  &lt; 2e-16 ***
## NAP          -0.6074     0.1374  -4.421 9.81e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##     (Intr)
## NAP 0.121</code></pre>
<p>According to the <em>Fixed effects</em> section of the summary, the mean intercept is 1.69 and the mean effect of the NAP is -0.61. Since the Poisson regression uses a log link by default, these coefficients mean that the mean richness is <span class="math inline">\(e^{1.69} = 5.42\)</span> species if NAP = 0 and is multiplied by <span class="math inline">\(e^{-0.61} = 0.54\)</span> (i.e. decreases by 46%) for every increase of one unit of the NAP. According to the <em>Random effects</em> section, the standard deviation of the intercept between the beaches is 0.51 and the standard deviation of the NAP coefficient is 0.30. If this were a linear mixed model, we would also get an estimate of the residual (within-group) standard deviation, but this is not the case here because the residual variance is fixed by the mean in the Poisson distribution.</p>
<p>The <code>ranef</code> function produces estimates of the difference between the value of a coefficient for each group and its mean value, while <code>coef</code> returns the values of the coefficients per group, thus the sum of <code>ranef</code> and the fixed effects.</p>
<pre class="r"><code>ranef(glmm_res)</code></pre>
<pre><code>## $Beach
##   (Intercept)         NAP
## 1   0.5579965  0.39325120
## 2   0.8038562  0.26321427
## 3  -0.4823311 -0.01681456
## 4  -0.4922817 -0.00227238
## 5   0.5590590 -0.40091320
## 6  -0.2740162  0.09140229
## 7  -0.3072758 -0.09381168
## 8  -0.1895568  0.03540481
## 9   0.0541533 -0.18368180
## 
## with conditional variances for &quot;Beach&quot;</code></pre>
<pre class="r"><code>coef(glmm_res)</code></pre>
<pre><code>## $Beach
##   (Intercept)        NAP
## 1    2.252151 -0.2141373
## 2    2.498011 -0.3441742
## 3    1.211824 -0.6242030
## 4    1.201873 -0.6096609
## 5    2.253214 -1.0083017
## 6    1.420139 -0.5159862
## 7    1.386879 -0.7012001
## 8    1.504598 -0.5719837
## 9    1.748308 -0.7910703
## 
## attr(,&quot;class&quot;)
## [1] &quot;coef.mer&quot;</code></pre>
<p>As with generalized linear models, it is useful to plot the non-linear relationship between the response and the predictors estimated by the model. The graph below superimposes the observed data (points) and the fitted model values (lines) for each beach.</p>
<pre class="r"><code>ggplot(rikz, aes(x = NAP, y = Richness, color = Beach)) +
    geom_point() +
    geom_line(aes(y = fitted(glmm_res)))</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="estimating-the-coefficients-of-a-glmm" class="section level2">
<h2>Estimating the coefficients of a GLMM</h2>
<p>For a mixed model, the probability of having observed a given value of the response depends not only on the parameters (fixed but unknown), but also on the value of the random effects for the group containing that observation. Thus, in order to calculate the likelihood function for the parameters to be estimated, the probability of the observed data for all possible values of the group random effects must be averaged (mathematically, this is an integral).</p>
<p>In the case of a mixed linear model, the equation becomes simpler and it is possible to estimate separately the fixed effects on the one hand, and the variances associated with the group effects and the residual variation between individuals on the other. The method that applies in this case is a modified version of the maximum likelihood called the restricted maximum likelihood (REML). Without going into detail, the REML estimates the variance parameters based on the model-independent residuals after estimating the fixed effects. In practice, this ensures that the variances are based on the correct number of residual degrees of freedom and corrects for the bias associated with variance estimation by the maximum likelihood method.</p>
<p>For a GLMM, there is no corresponding simplification and several methods have been proposed to numerically approximate the integral contained in the likelihood function. The method that <code>glmer</code> uses by default is the Laplace approximation, which is based on a quadratic approximation of the likelihood function. For models with a single random effect (e.g. the effect of a group variable on the intercept only), <code>glmer</code> offers a more accurate approximation method, the Gauss-Hermite quadrature. To apply this method, a value greater than 1 must be specified for the <code>nAGQ</code> argument of <code>glmer</code>. This argument corresponds to the number of points used to approximate the integral. A higher value is more precise, but requires more calculations; the authors of the package suggest a maximum value of 25.</p>
</div>
<div id="confidence-intervals" class="section level2">
<h2>Confidence intervals</h2>
<p>The <code>confint</code> function calculates confidence intervals for each of the parameters of a mixed model, including the coefficients of fixed effects, standard deviations and correlations of random effects.</p>
<pre class="r"><code>confint(glmm_res, oldNames = FALSE)</code></pre>
<pre><code>## Computing profile confidence intervals ...</code></pre>
<pre><code>##                                 2.5 %     97.5 %
## sd_(Intercept)|Beach       0.30813882  0.9344068
## cor_NAP.(Intercept)|Beach -0.63136889  0.9423103
## sd_NAP|Beach               0.08444686  0.6394023
## (Intercept)                1.27203026  2.0884038
## NAP                       -0.93296597 -0.3318997</code></pre>
<p>Note that it is important to specify <code>oldNames = FALSE</code> to get the correct identifiers for each interval. Those beginning with <code>sd</code> are the standard deviations of the random effects, the one beginning with <code>cor</code> corresponds to the correlation between two random effects, while the last two rows correspond to the fixed effects.</p>
<p>As indicated in the message, <code>confint</code> calculates the intervals from the profiled likelihood. It is also possible to calculate the intervals using the boostrap method by specifying the argument <code>method = "boot"</code> in <code>confint</code>. Note however that these are percentile intervals of the bootstrap and that the more precise methods (studentized intervals and BCa) are not available because of their computational cost.</p>
</div>
</div>
<div id="model-evaluation-and-comparison" class="section level1">
<h1>Model evaluation and comparison</h1>
<p>In this section, we will see how to evaluate the goodness of fit of a GLMM and compare the fit of different versions of a model.</p>
<div id="distribution-of-the-residuals" class="section level2">
<h2>Distribution of the residuals</h2>
<p>For a linear regression, the diagnostic graphs allowed us to check whether the residuals were normally distributed with a homogeneous variance. These properties of the residuals do not apply to a GLMM with a binomial or Poisson distribution. However, we can test if there is overdispersion of the residuals, which would be indicative of a poor fit of the theoretical model to the data.</p>
<p>If <span class="math inline">\(\hat{y_k}\)</span> represents the expected value of the observation <span class="math inline">\(k\)</span> according to the model, the <em>Pearson residual</em> for this observation is obtained by dividing the raw residual by the expected standard deviation of this observation.</p>
<p><span class="math display">\[r_{P(k)} = \frac{y_k - \hat{y_k}}{\hat{\sigma}_{k}}\]</span></p>
<p>The expected standard deviation is equal to <span class="math inline">\(\sqrt{\lambda}\)</span> in a Poisson model and <span class="math inline">\(\sqrt{np(1-p)}\)</span> for a binomial model. If the data follow the assumed model, the sum of the squares of these residuals follows a <span class="math inline">\(\chi^2\)</span> distribution with a number of degrees of freedom equal to the residual degrees of freedom of the model. This allows us to evaluate the fit of the model with a <span class="math inline">\(\chi^2\)</span> test.</p>
<pre class="r"><code>chi2 &lt;- sum(residuals(glmm_res, type = &quot;pearson&quot;)^2)
chi2</code></pre>
<pre><code>## [1] 26.40239</code></pre>
<pre class="r"><code>1 - pchisq(chi2, df = df.residual(glmm_res))</code></pre>
<pre><code>## [1] 0.9516085</code></pre>
<p>A small <span class="math inline">\(p\)</span>-value for this test would indicate overdispersion of the residuals relative to the model.</p>
<p>We can also define a coefficient of dispersion by dividing the <span class="math inline">\(\chi^2\)</span> value by the number of residual degrees of freedom.</p>
<pre class="r"><code>chi2 / df.residual(glmm_res)</code></pre>
<pre><code>## [1] 0.6600598</code></pre>
<p>The <span class="math inline">\(\chi^2\)</span> test is one-sided, because we do not generally care about underdispersion (coefficient of dispersion less than 1). However, an extreme case of underdispersion (<span class="math inline">\(p\)</span>-value very close to 1) could indicate that the model overfits the data.</p>
<p>The <em>DHARMa</em> package provides a general method for checking whether the residuals of a GLMM are distributed according to the specified model and whether there is any residual trend. The package works by simulating replicates of each observation according to the fitted model and then determining a “standardized residual”, which is the relative position of the observed value with respect to the simulated values, e.g. 0 if the observation is smaller than all the simulations, 0.5 if it is in the middle, etc. If the model represents the data well, each value of the standardized residual between 0 and 1 should be equally likely, so the standardized residuals should produce a uniform distribution between 0 and 1.</p>
<p>The <code>simulateResiduals</code> function performs the calculation of the standardized residuals, then the <code>plot</code> function plots the diagnostic graphs with the results of certain tests.</p>
<pre class="r"><code>library(DHARMa)
resid_sim &lt;- simulateResiduals(glmm_res)
plot(resid_sim)</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The graph on the left is a quantile-quantile plot of standardized residuals. The results of three statistical tests also also shown: a Kolmogorov-Smirnov (<em>KS</em>) test which checks whether there is a deviation from the theoretical distribution, a dispersion test that checks whether there is underdispersion or overdispersion, and an outlier test based on the number of residuals that are more extreme than all the simulations. In our case, all three results are insignificant, so there is no problem to signal.</p>
<p>On the right, we see a graph of the standardized residuals (in <em>y</em>) as a function of the rank of the predicted values (in <em>x</em>). The plots represent a non-parametric quantile regression for the 1st quartile, the median and the 3rd quartile. In theory, these three curves should be horizontal straight lines (no leftover trend in the residuals vs. predictions). The curve for the 1st quartile (in red) is significantly different from a horizontal line and the presence of a trend (even non-linear) could indicate that an important effect is missing from the model.</p>
<p>For more information on DHARMa, you can read the [package vignette](<a href="https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html" class="uri">https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html</a>.</p>
</div>
<div id="distribution-of-the-random-effects" class="section level2">
<h2>Distribution of the random effects</h2>
<p>It is also useful to verify that the random effects follow an approximately normal distribution. The <code>ranef</code> function produces a list of random effects for each grouping variable. Here, we choose the only grouping variable, <em>Beach</em>.</p>
<pre class="r"><code>re &lt;- ranef(glmm_res)$Beach</code></pre>
<p>The variable <code>re</code> is a data frame with two columns representing the random effects of the beaches on the intercept and the coefficient of the NAP. We use a quantile-quantile plot to check whether the values in each column are normally distributed.</p>
<pre class="r"><code>qqnorm(re$`(Intercept)`)
qqline(re$`(Intercept)`)</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>qqnorm(re$NAP)
qqline(re$NAP)</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-21-2.png" width="672" /></p>
<p>It is difficult to assess normality with only 9 group effects, but the extreme values for the NAP coefficient appear to exceed those of a normal distribution.</p>
</div>
<div id="coefficient-of-determination" class="section level2">
<h2>Coefficient of determination</h2>
<p>In a linear model, the coefficient of determination <span class="math inline">\(R^2\)</span> indicates the fraction of the variance of the data explained by the model:</p>
<p><span class="math display">\[R^2 = 1 - \frac{\sigma_{\epsilon}^2}{\sigma_t^2}\]</span></p>
<p>where <span class="math inline">\(\sigma_{\epsilon}^2\)</span> is the variance of the residuals and <span class="math inline">\(\sigma_t^2\)</span> is the total variance of the response.</p>
<p>The generalization of <span class="math inline">\(R^2\)</span> to a GLMM poses two problems:</p>
<ul>
<li>the variance of the data in a GLMM depends on the mean;</li>
<li>for a mixed model, the response varies at several levels (group and individual).</li>
</ul>
<p>The <code>r.squaredGLMM</code> function of the <em>MuMIn</em> package calculates a version of the coefficient of determination appropriate for GLMMs.</p>
<pre class="r"><code>library(MuMIn)
r.squaredGLMM(glmm_res)</code></pre>
<pre><code>##                 R2m       R2c
## delta     0.4206307 0.8577819
## lognormal 0.4240694 0.8647945
## trigamma  0.4168256 0.8500224</code></pre>
<p>The <code>R2m</code> value represents the marginal <span class="math inline">\(R^2\)</span>, i.e. the variance explained by taking into account only the fixed effects, while <code>R2c</code> represents the conditional <span class="math inline">\(R^2\)</span>, i.e. the variance explained by the fixed and random effects. For a linear mixed model, these <span class="math inline">\(R^2\)</span> are interpreted directly as a fraction of the variance of the response. For a GLMM, they are based on the variance on the scale of the linear predictor, in other words, the variance of the response transformed by the link function.</p>
<p>The result of the function <code>r.squaredGLMM</code> gives several estimates that are quite similar. According to the authors, the <code>trigamma</code> method is the most accurate, but it is only available for a GLMM with log link.</p>
</div>
<div id="model-comparison" class="section level2">
<h2>Model comparison</h2>
<p>The comparison of models with the AIC, or AICc for small samples, also applies to GLMMs. For mixed models, the textbook of Zuur et al. (2009) suggests the following method:</p>
<ul>
<li><p>First, include all the fixed effects of interest and choose, if necessary, between different versions of the random effects.</p></li>
<li><p>Retain the random effects chosen in the previous step and compare different versions of the fixed effects.</p></li>
</ul>
<p>This order is motivated by a desire to keep as many fixed effects as possible based on the data, thus reducing the complexity of the random effects before that of the fixed effects.</p>
<p>For linear mixed models, the first step is based on fitting the models by REML, while the second step requires a maximum likelihood fit, because REML can only compare models with the same fixed effects. In the case of GLMM, REML does not apply.</p>
<p><em>Note</em>: As we will show below, the first step can be used to choose which coefficients to apply random effects to: only the intercept, or the intercept and the predictor coefficients? However, the choice of groups should be based on the structure of the data and not on the selection of models; in other words, if the data are clustered, at a minimum, a random effect on the intercept should be included to account for the non-independence of observations from the same group.</p>
<p>For the <em>rikz</em> dataset, we first define a complete model (<code>glmm1</code>) that includes the effect of a beach-level predictor (<code>Exposure</code>) and the effect of the NAP, in addition to random effects of the beach on the intercept and the coefficient of the NAP. We compare this model to one that includes only a random effect on the intercept.</p>
<p>The <code>aictab</code> function of the package <em>AICcmodavg</em> calculates the AICc for each model in a list and gives their relative weights determined by the differences in AICc.</p>
<pre class="r"><code>library(AICcmodavg)

glmm1 &lt;- glmer(Richness ~ Exposure + NAP + (1 + NAP | Beach), data = rikz, 
               family = poisson)
glmm2 &lt;- glmer(Richness ~ Exposure + NAP + (1 | Beach), data = rikz, 
               family = poisson)
aictab(list(glmm1, glmm2))</code></pre>
<pre><code>## 
## Model selection based on AICc:
## 
##      K   AICc Delta_AICc AICcWt Cum.Wt      LL
## Mod2 5 211.55       0.00   0.69   0.69 -100.00
## Mod1 7 213.15       1.61   0.31   1.00  -98.06</code></pre>
<p>In this case, the simplest model gets the best AICc, so it will be chosen for parsimony, even if the complete model has a very close AICc.</p>
<p>Then we compare the <code>glmm2</code> model with a model without effect of the <code>Exposure</code> variable.</p>
<pre class="r"><code>glmm3 &lt;- glmer(Richness ~ NAP + (1 | Beach), data = rikz, 
               family = poisson)
aictab(list(glmm2, glmm3))</code></pre>
<pre><code>## 
## Model selection based on AICc:
## 
##      K   AICc Delta_AICc AICcWt Cum.Wt      LL
## Mod1 5 211.55       0.00   0.99   0.99 -100.00
## Mod2 3 221.37       9.82   0.01   1.00 -107.39</code></pre>
<p>The model including <code>Exposure</code> produces a much better AICc fit.</p>
<p>Even if a model fits better than other candidate models, this does not mean that the model produces a good fit. To answer this question, we must verify the fit of the selected model with the methods discussed above.</p>
<ul>
<li>The graphs of standardized residuals produced by <em>DHARMa</em> show no problems; compared to those of the previous model, the quantile-quantile plot (left) is closer to a straight line and the residuals vs. predicted values graph (right) shows a more random pattern. Note that the red star in the figure on the right represents an extreme value.</li>
</ul>
<pre class="r"><code>plot(simulateResiduals(glmm2))</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<ul>
<li>The random effects of the beach on the intercept are close to a normal distribution, considering the small number of groups.</li>
</ul>
<pre class="r"><code>qqnorm(ranef(glmm2)$Beach$`(Intercept)`)
qqline(ranef(glmm2)$Beach$`(Intercept)`)</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<ul>
<li>Finally, the addition of the <code>Exposure</code> variable explains much of the differences between the beaches, as the marginal <span class="math inline">\(R^2\)</span> (fixed effects only) now approaches the <span class="math inline">\(R^2\)</span> including random effects.</li>
</ul>
<pre class="r"><code>r.squaredGLMM(glmm2)</code></pre>
<pre><code>## Warning: The null model is correct only if all variables used by the original
## model remain unchanged.</code></pre>
<pre><code>##                 R2m       R2c
## delta     0.7270454 0.7435881
## lognormal 0.7420813 0.7589661
## trigamma  0.7100514 0.7262074</code></pre>
</div>
</div>
<div id="predictions-and-simulations-from-a-glmm" class="section level1">
<h1>Predictions and simulations from a GLMM</h1>
<div id="creating-a-prediction-data-frame" class="section level2">
<h2>Creating a prediction data frame</h2>
<p>The <code>predict</code> function, which is available for several types of models in R, returns the value of the response variable predicted by a model for given combinations of the predictor variables.</p>
<p>In the context of GLMM, this function is particularly useful to illustrate the non-linear effect of different combinations of predictors on the response.</p>
<p>As an example, consider the best model chosen in the previous section to explain the variation in species richness in the <code>rikz</code> dataset.</p>
<pre class="r"><code>glmm2 &lt;- glmer(Richness ~ Exposure + NAP + (1 | Beach), data = rikz, 
               family = poisson)</code></pre>
<p>To illustrate the effect of predictors, we create a new data frame that contains regularly spaced values of the NAP (from -1.5 to 2.5, in steps of 0.2) for each of the beaches. The function <code>expand.grid</code> is useful in this case because it produces a table with each combination of the variables shown. Note that the function <code>unique(rikz$Beach)</code> produces a vector of the unique values in the <code>Beach</code> column of <code>rikz</code>.</p>
<pre class="r"><code>pred_df &lt;- expand.grid(Beach = unique(rikz$Beach), 
                       NAP = seq(-1.5, 2.5, 0.2))</code></pre>
<p>We still have to provide the right value of <code>Exposure</code> for each of the beaches. To do this, we use two functions of the <em>dplyr</em> package: <code>distinct</code> chooses the unique combinations of <code>Beach</code> and <code>Exposure</code> present in the <code>rikz</code> data frame (so each of the 9 beaches associated with its exposure index), then <code>inner_join</code> joins these data to <code>pred_df</code> by matching the beach numbers in each row.</p>
<pre class="r"><code>library(dplyr)
plages &lt;- distinct(rikz, Beach, Exposure)
pred_df &lt;- inner_join(pred_df, plages)</code></pre>
<p>The <code>pred_df</code> data frame now contains all the predictors of the model, which will allow us to predict the specific richness for each case.</p>
</div>
<div id="choice-of-prediction-scale" class="section level2">
<h2>Choice of prediction scale</h2>
<p>Here is the mathematical form of our Poisson GLMM, with a logarithmic link and a random group effect on the intercept:</p>
<p><span class="math display">\[y \sim \text{Pois}(\lambda) \]</span> <span class="math display">\[\log(\lambda) = \beta_0 + \beta_1 x\]</span> <span class="math display">\[\beta_{0} \sim N(\gamma_0 + \gamma_{1} u, \sigma_{\beta_0})\]</span></p>
<p>In this particular case, <span class="math inline">\(y\)</span> is the site species richness, <span class="math inline">\(x\)</span> is the NAP and <span class="math inline">\(\beta_0\)</span> varies between beaches, with a mean depending on the exposure index <span class="math inline">\(u\)</span> and a standard deviation equal to <span class="math inline">\(\sigma_{\beta_0}\)</span>.</p>
<p>For a GLM or GLMM, the <code>predict</code> function can give a prediction either on the scale of the link function, so here <span class="math inline">\(\log(\lambda)\)</span>, or on the scale of the response, so <span class="math inline">\(\lambda\)</span>. This choice is given by the argument <code>type</code>; by default, <code>type = "link"</code>, so if we want the mean richness rather than its logarithm, we must specify <code>type = "response"</code>.</p>
<pre class="r"><code>pred_df$rich_pred &lt;- predict(glmm2, newdata = pred_df, type = &quot;response&quot;)</code></pre>
<p>In the example below, we represent these predictions by lines on a graph and then superimpose the points of the original observations. Note that the arguments <code>data</code> and <code>aes(...)</code> are specified in <code>geom_point</code> to get data from another source than the one specified at the beginning of the <code>ggplot</code> statement.</p>
<pre class="r"><code>ggplot(pred_df, aes(x = NAP, y = rich_pred, color = Exposure)) +
    geom_point(data = rikz, aes(y = Richness)) +
    geom_line() +
    facet_wrap(~ Beach) +
    scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>On the graph, we see that the predictions vary from beach to beach, but are more similar for beaches with the same exposure index.</p>
</div>
<div id="predictions-and-random-effects" class="section level2">
<h2>Predictions and random effects</h2>
<p>We saw earlier that for a mixed model, we obtain not only an estimate of the variance of the random effects (<span class="math inline">\(\sigma_{\beta_0}\)</span> in the model above), but also an estimate of the coefficient <span class="math inline">\(\beta_0\)</span> for each group, which we can consult with <code>coef(glmm2)</code>.</p>
<p>By default, the <code>predict</code> function uses the estimated coefficients for each group to produce the predictions. However, that method cannot predict the response for a new group that was not part of the original sample.</p>
<p>In the following example, we add rows to <code>pred_df</code> with <code>rbind</code> that correspond to a new unknown beach, so <code>Beach = NA</code>, but with known values of the NAP and the exposure index. We specify <code>allow.new.levels = TRUE</code> in the <code>predict</code> function. In this case, for an unknown range of the model, the function returns the average of <span class="math inline">\(\beta_0\)</span> given by the fixed effects (<span class="math inline">\(\gamma_0 + \gamma_1 u\)</span>).</p>
<pre class="r"><code>pred_df &lt;- rbind(pred_df, 
                 data.frame(Beach = NA, NAP = seq(-1.5, 2.5, 0.2),
                            Exposure = &quot;10&quot;, rich_pred = NA))

pred_df$rich_pred2 &lt;- predict(glmm2, newdata = pred_df, type = &quot;response&quot;,
                              allow.new.levels = TRUE)

ggplot(pred_df, aes(x = NAP, y = rich_pred2, color = Exposure)) +
    geom_point(data = rikz, aes(y = Richness)) +
    geom_line() +
    facet_wrap(~ Beach) +
    scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Finally, another argument of <code>predict</code>, <code>re.form</code>, allows us to ignore some random effects. In this case, by specifying <code>re.form = ~0</code> (no random effects), predictions would be made only with fixed effects even for known beaches: thus, these predictions would be identical for all beaches sharing the same exposure index.</p>
<p>For a model with several random effects, we can ignore some of the effects. For example, suppose that we have ecological monitoring sites where the same measurements are taken every year and some response is modeled based on random effects of site and year, i.e., <code>(1 | site) + (1 | year)</code>. If we want to make predictions for the following year at a known site, we can include the site effect only in the predictions with <code>re.form = ~(1|site)</code>.</p>
</div>
<div id="simulations-from-the-model" class="section level2">
<h2>Simulations from the model</h2>
<p>If the <code>predict</code> function returns for each row of a data frame the mean value of the response predicted by the model, <code>simulate</code> produces several randomly generated datasets from the fitted model (in the model above, these would be values of <span class="math inline">\(y\)</span> rather than <span class="math inline">\(\lambda\)</span>).</p>
<p>The arguments in <code>simulate</code> are similar to those in <code>predict</code>, except that the number of simulations with must also be specified with the <code>nsim</code> argument. The two functions also treat random effects differently. By default, <code>predict</code> takes into account the estimated coefficients for each group, while <code>simulate</code> ignores the random effects of the groups, as if <code>re.form = ~0</code> had been specified. Thus, even for a known group, <code>simulate</code> will simulate a value of <span class="math inline">\(\beta_0\)</span> from the random effects distribution <span class="math inline">\(\beta_{0} \sim N(\gamma_0 + \gamma_{1} u, \sigma_{\beta_0})\)</span>, rather than using the <span class="math inline">\(\beta_0\)</span> estimate given by the model for that group. If we want to keep the <span class="math inline">\(\beta_0\)</span> of the known groups and only simulate the individual random response from the Poisson distribution, then we need to specify <code>re.form = NULL</code>.</p>
<pre class="r"><code>rich_sims &lt;- simulate(glmm2, nsim = 1000, newdata = pred_df, re.form = NULL,
                      allow.new.levels = TRUE)</code></pre>
<p>The result of <code>simulate</code> is a data set with one row for each row of <code>newdata</code> and one column for each of the <code>nsim</code> simulations. This result is used, among other things, to produce a prediction interval, i.e. an interval that should contain a certain fraction of the individual observations if the model is correct. In the example below, we extract the 2.5% and 97.5% quantiles from each row of <code>rich_sims</code> and add them to <code>pred_df</code> as the bounds of a 95% prediction interval. This interval is visualized with the <code>geom_ribbon</code> function of <em>ggplot2</em>.</p>
<pre class="r"><code>pred_df$q025 &lt;- apply(rich_sims, 1, quantile, probs = 0.025)
pred_df$q975 &lt;- apply(rich_sims, 1, quantile, probs = 0.975)

ggplot(pred_df, aes(x = NAP, y = rich_pred2, color = Exposure, fill = Exposure)) +
    geom_point(data = rikz, aes(y = Richness)) +
    geom_ribbon(aes(ymin = q025, ymax = q975), alpha = 0.3, color = &quot;white&quot;) +
    geom_line() +
    facet_wrap(~ Beach) +
    scale_color_brewer(palette = &quot;Dark2&quot;) +
    scale_fill_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="05E-Modeles_generalises_mixtes_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Note that in this example, the simulations for the known beaches use the estimated <span class="math inline">\(\beta_0\)</span>, while those for the unknown <code>NA</code> beach generate a value of <span class="math inline">\(\beta_0\)</span> from its distribution. We would therefore expect the interval to be wider for the unknown beach. This difference is imperceptible here because the random effect of the beach, after taking into account the exposure index, is very small. Thus the uncertainty represented is almost exclusively due to the variation of individual observations according to the Poisson distribution.</p>
</div>
<div id="parameter-uncertainty" class="section level2">
<h2>Parameter uncertainty</h2>
<p>The <code>simulate</code> function accounts for the variation in individual observations around their mean and (optionally) the variation in random effects, but assumes that the model parameters (fixed effects and random effect variances) are accurate. The parametric bootstrap, implemented by the <code>bootMer</code> function of <em>lme4</em>, is a way of including uncertainty in the parameter estimates:</p>
<ul>
<li><p>First, new values of the response for the original dataset are simulated from the fitted model.</p></li>
<li><p>Then, the model is refit with these simulated data.</p></li>
<li><p>Finally, we call <code>predict</code> or <code>simulate</code> from the refit model.</p></li>
</ul>
<p>By repeating this process a large number of times, we obtain either a confidence interval for the mean predictions (with <code>predict</code>) or prediction intervals that include the uncertainty of the parameters (with <code>simulate</code>).</p>
<p>We will not demonstrate this method in the course. However, note that a bootstrap with <span class="math inline">\(N\)</span> replicates requires <span class="math inline">\(N\)</span> replicates of the GLMM fit, which may require a long computation time for a complex model.</p>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<ul>
<li><p>Bolker, B. et al. (2009) Generalized linear mixed models: a practical guide for ecology and evolution. <em>Trends in Ecology and Evolution</em> 24: 127-135.</p></li>
<li><p>Harrison, X.A. et al. (2018) A brief introduction to mixed effects modelling and multi-model inference in ecology. <em>PeerJ</em> 6: e4794.</p></li>
<li><p>Zuur, A.F., Ieno, E.N., Walker, N.J., Saveliev, A.A., Smith, G.M. (2009) <em>Mixed Effects Models and Extensions in Ecology with R</em>. New York, Springer-Verlag.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
